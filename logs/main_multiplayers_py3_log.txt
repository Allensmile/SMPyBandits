Loaded experiments configuration from 'configuration.py' :
configuration = {'horizon': 10000, 'players': [Softmax(decreasing), Softmax(decreasing), Softmax(decreasing), Softmax(decreasing), Softmax(decreasing), Softmax(decreasing)], 'collisionModel': <function onlyUniqUserGetsReward at 0x7fc349879c80>, 'averageOn': 0.001, 'n_jobs': -1, 'verbosity': 6, 'repetitions': 1000, 'finalRanksOnAverage': True, 'environment': [{'params': [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95], 'arm_type': <class 'Arms.Bernoulli.Bernoulli'>}]}
plots/ is already a directory here...
Number of players in the multi-players game: 6
Time horizon: 10000
Number of repetitions: 1000
Using collision model: onlyUniqUserGetsReward
  Detail:  Simple collision model where only the players alone on one arm sample it and receive the reward.

    - This is the default collision model, cf. https://arxiv.org/abs/0910.2065v3 collision model 1.
    - The numpy array 'choices' is increased according to the number of users who collided (it is NOT binary).
    
Creating a new MAB problem ...
  Reading arms of this MAB problem from a dictionnary 'configuration' = {'params': [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95], 'arm_type': <class 'Arms.Bernoulli.Bernoulli'>} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]
 - with 'arms' = [B(0.05), B(0.1), B(0.15), B(0.2), B(0.25), B(0.3), B(0.35), B(0.4), B(0.45), B(0.5), B(0.55), B(0.6), B(0.65), B(0.7), B(0.75), B(0.8), B(0.85), B(0.9), B(0.95)]
 - with 'nbArms' = 19
 - with 'maxArm' = 0.95
Number of environments to try: 1

Evaluating environment: <MAB{'nbArms': 19, 'maxArm': 0.94999999999999996, 'arms': [B(0.05), B(0.1), B(0.15), B(0.2), B(0.25), B(0.3), B(0.35), B(0.4), B(0.45), B(0.5), B(0.55), B(0.6), B(0.65), B(0.7), B(0.75), B(0.8), B(0.85), B(0.9), B(0.95)]}>
- Adding player #1 = #1<ALOHA, Softmax(decreasing), p0: 0.5, alpha_p0: 0.5, ftnext: tnext_beta> ...
  Using this already created player 'player' = #1<ALOHA, Softmax(decreasing), p0: 0.5, alpha_p0: 0.5, ftnext: tnext_beta> ...
- Adding player #2 = #2<ALOHA, Softmax(decreasing), p0: 0.5, alpha_p0: 0.5, ftnext: tnext_beta> ...
  Using this already created player 'player' = #2<ALOHA, Softmax(decreasing), p0: 0.5, alpha_p0: 0.5, ftnext: tnext_beta> ...
- Adding player #3 = #3<ALOHA, Softmax(decreasing), p0: 0.5, alpha_p0: 0.5, ftnext: tnext_beta> ...
  Using this already created player 'player' = #3<ALOHA, Softmax(decreasing), p0: 0.5, alpha_p0: 0.5, ftnext: tnext_beta> ...
- Adding player #4 = #4<ALOHA, Softmax(decreasing), p0: 0.5, alpha_p0: 0.5, ftnext: tnext_beta> ...
  Using this already created player 'player' = #4<ALOHA, Softmax(decreasing), p0: 0.5, alpha_p0: 0.5, ftnext: tnext_beta> ...
- Adding player #5 = #5<ALOHA, Softmax(decreasing), p0: 0.5, alpha_p0: 0.5, ftnext: tnext_beta> ...
  Using this already created player 'player' = #5<ALOHA, Softmax(decreasing), p0: 0.5, alpha_p0: 0.5, ftnext: tnext_beta> ...
- Adding player #6 = #6<ALOHA, Softmax(decreasing), p0: 0.5, alpha_p0: 0.5, ftnext: tnext_beta> ...
  Using this already created player 'player' = #6<ALOHA, Softmax(decreasing), p0: 0.5, alpha_p0: 0.5, ftnext: tnext_beta> ...
