Loaded experiments configuration from 'configuration.py' :
configuration = {'verbosity': 6, 'averageOn': 0.001, 'players': [<PoliciesMultiPlayers.ChildPointer.ChildPointer object at 0x7f76dacfc2b0>, <PoliciesMultiPlayers.ChildPointer.ChildPointer object at 0x7f76b6f3bbe0>, <PoliciesMultiPlayers.ChildPointer.ChildPointer object at 0x7f76b6f41da0>, <PoliciesMultiPlayers.ChildPointer.ChildPointer object at 0x7f76b6e58d68>, <PoliciesMultiPlayers.ChildPointer.ChildPointer object at 0x7f76b6e58be0>, <PoliciesMultiPlayers.ChildPointer.ChildPointer object at 0x7f76b6e58e10>], 'repetitions': 1, 'environment': [{'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': [0.005, 0.01, 0.015, 0.02, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.78, 0.8, 0.82, 0.83, 0.84, 0.85]}], 'horizon': 20000, 'collisionModel': <function onlyUniqUserGetsReward at 0x7f76b6f2b268>, 'finalRanksOnAverage': True, 'n_jobs': 1}
plots is already a directory here...
Number of players in the multi-players game: 6
Time horizon: 20000
Number of repetitions: 1
Using collision model: onlyUniqUserGetsReward
  Detail:  Simple collision model where only the players alone on one arm sample it and receive the reward.

    - This is the default collision model, cf. https://arxiv.org/abs/0910.2065v3 collision model 1.
    - The numpy array 'choices' is increased according to the number of users who collided (it is NOT binary).
    
Creating a new MAB problem ...
  Reading arms of this MAB problem from a dictionnary 'configuration' = {'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': [0.005, 0.01, 0.015, 0.02, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.78, 0.8, 0.82, 0.83, 0.84, 0.85]} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = [0.005, 0.01, 0.015, 0.02, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.78, 0.8, 0.82, 0.83, 0.84, 0.85]
 - with 'arms' = [B(0.005), B(0.01), B(0.015), B(0.02), B(0.3), B(0.35), B(0.4), B(0.45), B(0.5), B(0.55), B(0.6), B(0.78), B(0.8), B(0.82), B(0.83), B(0.84), B(0.85)]
 - with 'nbArms' = 17
 - with 'maxArm' = 0.85
Number of environments to try: 1

Evaluating environment: <MAB{'nbArms': 17, 'arms': [B(0.005), B(0.01), B(0.015), B(0.02), B(0.3), B(0.35), B(0.4), B(0.45), B(0.5), B(0.55), B(0.6), B(0.78), B(0.8), B(0.82), B(0.83), B(0.84), B(0.85)], 'maxArm': 0.85}>
- Adding player #1 = #1<MusicalChair(N*: None)> ...
  Using this already created player 'player' = #1<MusicalChair(N*: None)> ...
- Adding player #2 = #2<MusicalChair(N*: None)> ...
  Using this already created player 'player' = #2<MusicalChair(N*: None)> ...
- Adding player #3 = #3<MusicalChair(N*: None)> ...
  Using this already created player 'player' = #3<MusicalChair(N*: None)> ...
- Adding player #4 = #4<MusicalChair(N*: None)> ...
  Using this already created player 'player' = #4<MusicalChair(N*: None)> ...
- Adding player #5 = #5<MusicalChair(N*: None)> ...
  Using this already created player 'player' = #5<MusicalChair(N*: None)> ...
- Adding player #6 = #6<MusicalChair(N*: None)> ...
  Using this already created player 'player' = #6<MusicalChair(N*: None)> ...
- A MusicalChair player has to switch from InitialPhase to MusicalChair ...
  - self._cumulatedRewards = [  1.   2.   0.   1.  15.   9.  18.  29.  19.  30.  27.  29.  34.  27.  37.
  31.  34.]
  - self._nbObservations = [44 48 37 45 44 42 43 50 47 44 45 40 40 35 40 41 43]
  - empiricalMeans = [ 0.02272727  0.04166667  0.          0.02222222  0.34090909  0.21428571
  0.41860465  0.58        0.40425532  0.68181818  0.6         0.725       0.85
  0.77142857  0.925       0.75609756  0.79069767]
  - self._A = [14 12 16 13 15 11  9 10  7  6  8  4  5  1  0  3  2]
  - self.nbPlayers = 6
  - self._A = [14 12 16 13 15 11]
- A MusicalChair player has to switch from InitialPhase to MusicalChair ...
  - self._cumulatedRewards = [  1.   1.   0.   0.  13.  17.  20.  19.  27.  15.  29.  35.  30.  36.  37.
  40.  33.]
  - self._nbObservations = [57 40 55 37 35 43 45 38 49 35 56 40 43 39 42 48 38]
  - empiricalMeans = [ 0.01754386  0.025       0.          0.          0.37142857  0.39534884
  0.44444444  0.5         0.55102041  0.42857143  0.51785714  0.875
  0.69767442  0.92307692  0.88095238  0.83333333  0.86842105]
  - self._A = [13 14 11 16 15 12  8 10  7  6  9  5  4  1  0  3  2]
  - self.nbPlayers = 6
  - self._A = [13 14 11 16 15 12]
- A MusicalChair player has to switch from InitialPhase to MusicalChair ...
  - self._cumulatedRewards = [  0.   2.   0.   0.  12.  18.  18.  22.  34.  26.  31.  27.  39.  35.  46.
  34.  36.]
  - self._nbObservations = [46 37 46 47 39 50 48 45 57 39 45 32 41 41 51 43 40]
  - empiricalMeans = [ 0.          0.05405405  0.          0.          0.30769231  0.36        0.375
  0.48888889  0.59649123  0.66666667  0.68888889  0.84375     0.95121951
  0.85365854  0.90196078  0.79069767  0.9       ]
  - self._A = [12 14 16 13 11 15 10  9  8  7  6  5  4  1  3  2  0]
  - self.nbPlayers = 6
  - self._A = [12 14 16 13 11 15]
- A MusicalChair player has to switch from InitialPhase to MusicalChair ...
  - self._cumulatedRewards = [  1.   0.   2.   1.  15.  12.  14.  18.  20.  29.  32.  33.  36.  37.  38.
  33.  44.]
  - self._nbObservations = [51 47 34 38 45 33 44 39 48 44 47 41 46 44 46 39 47]
  - empiricalMeans = [ 0.01960784  0.          0.05882353  0.02631579  0.33333333  0.36363636
  0.31818182  0.46153846  0.41666667  0.65909091  0.68085106  0.80487805
  0.7826087   0.84090909  0.82608696  0.84615385  0.93617021]
  - self._A = [16 15 13 14 11 12 10  9  7  8  5  4  6  2  3  0  1]
  - self.nbPlayers = 6
  - self._A = [16 15 13 14 11 12]
- A MusicalChair player has to switch from InitialPhase to MusicalChair ...
  - self._cumulatedRewards = [  0.   1.   0.   1.  13.  11.  15.  24.  26.  25.  24.  45.  29.  45.  30.
  28.  39.]
  - self._nbObservations = [42 48 42 41 45 37 42 46 48 44 37 53 44 54 38 42 43]
  - empiricalMeans = [ 0.          0.02083333  0.          0.02439024  0.28888889  0.2972973
  0.35714286  0.52173913  0.54166667  0.56818182  0.64864865  0.8490566
  0.65909091  0.83333333  0.78947368  0.66666667  0.90697674]
  - self._A = [16 11 13 14 15 12 10  9  8  7  6  5  4  3  1  2  0]
  - self.nbPlayers = 6
  - self._A = [16 11 13 14 15 12]
- A MusicalChair player has to switch from InitialPhase to MusicalChair ...
  - self._cumulatedRewards = [  0.   0.   1.   1.  15.  20.  18.  15.  18.  24.  28.  31.  38.  40.  37.
  31.  38.]
  - self._nbObservations = [40 37 50 43 38 37 42 41 38 42 52 46 43 50 46 36 45]
  - empiricalMeans = [ 0.          0.          0.02        0.02325581  0.39473684  0.54054054
  0.42857143  0.36585366  0.47368421  0.57142857  0.53846154  0.67391304
  0.88372093  0.8         0.80434783  0.86111111  0.84444444]
  - self._A = [12 15 16 14 13 11  9  5 10  8  6  4  7  3  2  1  0]
  - self.nbPlayers = 6
  - self._A = [12 15 16 14 13 11]
Giving the final ranks ...

Final ranking for this environment #0 :
- Player #1, '#1<MusicalChair(N*: None)>'	was ranked	1 / 6 for this simulation (last regret = 421.850).
- Player #2, '#2<MusicalChair(N*: None)>'	was ranked	2 / 6 for this simulation (last regret = 677.850).
- Player #3, '#3<MusicalChair(N*: None)>'	was ranked	3 / 6 for this simulation (last regret = 924.850).
- Player #4, '#4<MusicalChair(N*: None)>'	was ranked	4 / 6 for this simulation (last regret = 1052.850).
- Player #5, '#5<MusicalChair(N*: None)>'	was ranked	5 / 6 for this simulation (last regret = 1533.850).
- Player #6, '#6<MusicalChair(N*: None)>'	was ranked	6 / 6 for this simulation (last regret = 1839.850).
- Plotting the decentralized rewards, and saving the plot to plots/MP__M6_T20000_N1__6_algos/main____env1-1_898940522482933579.png ...
Saving to plots/MP__M6_T20000_N1__6_algos/main____env1-1_898940522482933579.png ...
- Plotting the centralized  rewards, and saving the plot to plots/MP__M6_T20000_N1__6_algos/main_RewardsCentralized____env1-1_898940522482933579.png ...
Saving to plots/MP__M6_T20000_N1__6_algos/main_RewardsCentralized____env1-1_898940522482933579.png ...
 - Plotting the frequency of collision in each arm, and saving the plot to plots/MP__M6_T20000_N1__6_algos/main_FrequencyCollisions_RewardsCentralized____env1-1_898940522482933579.png ...
  - For #$0$: B(0.005),	frequency of collisions is 0.001  ...
  - For #$1$: B(0.01),	frequency of collisions is 0.001  ...
  - For #$2$: B(0.015),	frequency of collisions is 0.001  ...
  - For #$3$: B(0.02),	frequency of collisions is 0.001  ...
  - For #$4$: B(0.3),	frequency of collisions is 0.001  ...
  - For #$5$: B(0.35),	frequency of collisions is 0.001  ...
  - For #$6$: B(0.4),	frequency of collisions is 0.001  ...
  - For #$7$: B(0.45),	frequency of collisions is 0.001  ...
  - For #$8$: B(0.5),	frequency of collisions is 0.001  ...
  - For #$9$: B(0.55),	frequency of collisions is 0.001  ...
  - For #$10$: B(0.6),	frequency of collisions is 0.001  ...
  - For #$11$: B(0.78),	frequency of collisions is 0.001  ...
  - For #$12$: B(0.8),	frequency of collisions is 0.001  ...
  - For #$13$: B(0.82),	frequency of collisions is 0.001  ...
  - For #$14$: B(0.83),	frequency of collisions is 0.001  ...
  - For #$15$: B(0.84),	frequency of collisions is 0.001  ...
  - For #$16$: B(0.85),	frequency of collisions is 0.001  ...
Saving to plots/MP__M6_T20000_N1__6_algos/main_FrequencyCollisions_RewardsCentralized____env1-1_898940522482933579.png ...
