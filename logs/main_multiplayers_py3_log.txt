 - Setting dpi of all figures to 110 ...
 - Setting 'figsize' of all figures to (19.8, 10.8) ...
Loaded experiments configuration from 'configuration.py' :
configuration = {'environment': [{'params': [0.1, 0.9], 'arm_type': <class 'Arms.Bernoulli.Bernoulli'>}], 'repetitions': 50, 'players': [EpsilonDecreasingMEGA(e:2.4999999999999996), EpsilonDecreasingMEGA(e:2.4999999999999996)], 'delta_t_save': 1, 'horizon': 10000, 'n_jobs': -1, 'verbosity': 6, 'collisionModel': <function onlyUniqUserGetsReward at 0x7feeee8e82f0>, 'finalRanksOnAverage': True, 'averageOn': 0.001}
plots/ is already a directory here...
Number of players in the multi-players game: 2
Time horizon: 10000
Number of repetitions: 50
Sampling rate DELTA_T_SAVE: 1
Using collision model: onlyUniqUserGetsReward
  Detail:  Simple collision model where only the players alone on one arm sample it and receive the reward.

    - This is the default collision model, cf. https://arxiv.org/abs/0910.2065v3 collision model 1.
    - The numpy array 'choices' is increased according to the number of users who collided (it is NOT binary).
    
Creating a new MAB problem ...
  Reading arms of this MAB problem from a dictionnary 'configuration' = {'params': [0.1, 0.9], 'arm_type': <class 'Arms.Bernoulli.Bernoulli'>} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = [0.1, 0.9]
 - with 'arms' = [B(0.1), B(0.9)]
 - with 'nbArms' = 2
 - with 'maxArm' = 0.9
 - with 'minArm' = 0.1
Number of environments to try: 1

Evaluating environment: <MAB{'arms': [B(0.1), B(0.9)], 'nbArms': 2, 'maxArm': 0.90000000000000002, 'minArm': 0.10000000000000001}>
- Adding player #1 = #1<ALOHA(EpsilonDecreasingMEGA(e:2.4999999999999996), $p_0=0.6$, $\alpha=0.5$, $f(t)=\sqrt{t}$)> ...
  Using this already created player 'player' = #1<ALOHA(EpsilonDecreasingMEGA(e:2.4999999999999996), $p_0=0.6$, $\alpha=0.5$, $f(t)=\sqrt{t}$)> ...
- Adding player #2 = #2<ALOHA(EpsilonDecreasingMEGA(e:2.4999999999999996), $p_0=0.6$, $\alpha=0.5$, $f(t)=\sqrt{t}$)> ...
  Using this already created player 'player' = #2<ALOHA(EpsilonDecreasingMEGA(e:2.4999999999999996), $p_0=0.6$, $\alpha=0.5$, $f(t)=\sqrt{t}$)> ...
Giving the final ranks ...

Final ranking for this environment #0 :
- Player #1, '#1<ALOHA(EpsilonDecreasingMEGA(e:2.4999999999999996), $p_0=0.6$, $\alpha=0.5$, $f(t)=\sqrt{t}$)>'	was ranked	1 / 2 for this simulation (last rewards = 5309.62).
- Player #2, '#2<ALOHA(EpsilonDecreasingMEGA(e:2.4999999999999996), $p_0=0.6$, $\alpha=0.5$, $f(t)=\sqrt{t}$)>'	was ranked	2 / 2 for this simulation (last rewards = 4674.06).


- Plotting the decentralized rewards


- Plotting the centralized regret
  - For 2 player, our lower bound gave = 0 ...
  - For 2 player, the initial lower bound in Theorem 6 from [Anandkumar et al., 2010] gave = 0 ...
 - Our lowerbound = 0,
 - anandkumar_lowerbound = 0


- Plotting the centralized regret
  - For 2 player, our lower bound gave = 0 ...
  - For 2 player, the initial lower bound in Theorem 6 from [Anandkumar et al., 2010] gave = 0 ...
 - Our lowerbound = 0,
 - anandkumar_lowerbound = 0


- Plotting the normalized centralized rewards
  - For 2 player, our lower bound gave = 0 ...
  - For 2 player, the initial lower bound in Theorem 6 from [Anandkumar et al., 2010] gave = 0 ...
 - Our lowerbound = 0,
 - anandkumar_lowerbound = 0


- Plotting the number of switches


- Plotting the cumulative number of switches
 - Plotting the probability of picking the best arm
 - Plotting the total nb of collision as a function of time
 - Plotting the cumulated total nb of collision as a function of time
 - Plotting the frequency of collision in each arm
  - For #$0$: $B(0.1)$,	frequency of collisions is 0.000238  ...
  - For #$1$: $B(0.9)$,	frequency of collisions is 3.8e-05  ...
