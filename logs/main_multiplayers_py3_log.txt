Loaded experiments configuration from 'configuration.py' :
configuration = {'players': [Thompson, Thompson, Thompson, Thompson, Thompson, Thompson, Thompson, Thompson, Thompson, Thompson, Thompson, Thompson, Thompson, Thompson, Thompson, Thompson, Thompson], 'horizon': 10000, 'delta_t_save': 50, 'repetitions': 20, 'verbosity': 6, 'environment': [{'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]}], 'collisionModel': <function onlyUniqUserGetsReward at 0x7fc60bf956a8>, 'n_jobs': -1, 'finalRanksOnAverage': True, 'averageOn': 0.001}
plots/ is already a directory here...
Number of players in the multi-players game: 17
Time horizon: 10000
Number of repetitions: 20
Sampling rate DELTA_T_SAVE: 50
Using collision model: onlyUniqUserGetsReward
  Detail:  Simple collision model where only the players alone on one arm sample it and receive the reward.

    - This is the default collision model, cf. https://arxiv.org/abs/0910.2065v3 collision model 1.
    - The numpy array 'choices' is increased according to the number of users who collided (it is NOT binary).
    
Creating a new MAB problem ...
  Reading arms of this MAB problem from a dictionnary 'configuration' = {'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]
 - with 'arms' = [B(0.05), B(0.1), B(0.15), B(0.2), B(0.25), B(0.3), B(0.35), B(0.4), B(0.45), B(0.5), B(0.55), B(0.6), B(0.65), B(0.7), B(0.75), B(0.8), B(0.85), B(0.9), B(0.95)]
 - with 'nbArms' = 19
 - with 'maxArm' = 0.95
Number of environments to try: 1

Evaluating environment: <MAB{'arms': [B(0.05), B(0.1), B(0.15), B(0.2), B(0.25), B(0.3), B(0.35), B(0.4), B(0.45), B(0.5), B(0.55), B(0.6), B(0.65), B(0.7), B(0.75), B(0.8), B(0.85), B(0.9), B(0.95)], 'nbArms': 19, 'maxArm': 0.94999999999999996}>
- Adding player #1 = #1<Thompson> ...
  Using this already created player 'player' = #1<Thompson> ...
- Adding player #2 = #2<Thompson> ...
  Using this already created player 'player' = #2<Thompson> ...
- Adding player #3 = #3<Thompson> ...
  Using this already created player 'player' = #3<Thompson> ...
- Adding player #4 = #4<Thompson> ...
  Using this already created player 'player' = #4<Thompson> ...
- Adding player #5 = #5<Thompson> ...
  Using this already created player 'player' = #5<Thompson> ...
- Adding player #6 = #6<Thompson> ...
  Using this already created player 'player' = #6<Thompson> ...
- Adding player #7 = #7<Thompson> ...
  Using this already created player 'player' = #7<Thompson> ...
- Adding player #8 = #8<Thompson> ...
  Using this already created player 'player' = #8<Thompson> ...
- Adding player #9 = #9<Thompson> ...
  Using this already created player 'player' = #9<Thompson> ...
- Adding player #10 = #10<Thompson> ...
  Using this already created player 'player' = #10<Thompson> ...
- Adding player #11 = #11<Thompson> ...
  Using this already created player 'player' = #11<Thompson> ...
- Adding player #12 = #12<Thompson> ...
  Using this already created player 'player' = #12<Thompson> ...
- Adding player #13 = #13<Thompson> ...
  Using this already created player 'player' = #13<Thompson> ...
- Adding player #14 = #14<Thompson> ...
  Using this already created player 'player' = #14<Thompson> ...
- Adding player #15 = #15<Thompson> ...
  Using this already created player 'player' = #15<Thompson> ...
- Adding player #16 = #16<Thompson> ...
  Using this already created player 'player' = #16<Thompson> ...
- Adding player #17 = #17<Thompson> ...
  Using this already created player 'player' = #17<Thompson> ...
Giving the final ranks ...

Final ranking for this environment #0 :
- Player #5, '#5<Thompson>'	was ranked	1 / 17 for this simulation (last rewards = 0.45).
- Player #2, '#2<Thompson>'	was ranked	2 / 17 for this simulation (last rewards = 0.3).
- Player #8, '#8<Thompson>'	was ranked	3 / 17 for this simulation (last rewards = 0.3).
- Player #16, '#16<Thompson>'	was ranked	4 / 17 for this simulation (last rewards = 0.3).
- Player #1, '#1<Thompson>'	was ranked	5 / 17 for this simulation (last rewards = 0.25).
- Player #4, '#4<Thompson>'	was ranked	6 / 17 for this simulation (last rewards = 0.25).
- Player #6, '#6<Thompson>'	was ranked	7 / 17 for this simulation (last rewards = 0.25).
- Player #15, '#15<Thompson>'	was ranked	8 / 17 for this simulation (last rewards = 0.2).
- Player #13, '#13<Thompson>'	was ranked	9 / 17 for this simulation (last rewards = 0.15).
- Player #12, '#12<Thompson>'	was ranked	10 / 17 for this simulation (last rewards = 0.15).
- Player #17, '#17<Thompson>'	was ranked	11 / 17 for this simulation (last rewards = 0.15).
- Player #3, '#3<Thompson>'	was ranked	12 / 17 for this simulation (last rewards = 0.15).
- Player #11, '#11<Thompson>'	was ranked	13 / 17 for this simulation (last rewards = 0.15).
- Player #10, '#10<Thompson>'	was ranked	14 / 17 for this simulation (last rewards = 0.1).
- Player #7, '#7<Thompson>'	was ranked	15 / 17 for this simulation (last rewards = 0.1).
- Player #9, '#9<Thompson>'	was ranked	16 / 17 for this simulation (last rewards = 0.1).
- Player #14, '#14<Thompson>'	was ranked	17 / 17 for this simulation (last rewards = 0.05).


- Plotting the decentralized rewards


- Plotting the centralized regret
  - For 17 player, our lower bound gave = 111.51811116431456 ...
  - For 17 player, the initial lower bound in Theorem 6 from [Anandkumar et al., 2010] gave = 14.624845750420596 ...
 - Our lowerbound = 111.51811116431456,
 - anandkumar_lowerbound = 14.624845750420596


- Plotting the centralized regret
  - For 17 player, our lower bound gave = 111.51811116431456 ...
  - For 17 player, the initial lower bound in Theorem 6 from [Anandkumar et al., 2010] gave = 14.624845750420596 ...
 - Our lowerbound = 111.51811116431456,
 - anandkumar_lowerbound = 14.624845750420596


- Plotting the normalized centralized rewards
  - For 17 player, our lower bound gave = 111.51811116431456 ...
  - For 17 player, the initial lower bound in Theorem 6 from [Anandkumar et al., 2010] gave = 14.624845750420596 ...
 - Our lowerbound = 111.51811116431456,
 - anandkumar_lowerbound = 14.624845750420596


- Plotting the number of switches


- Plotting the cumulative number of switches
 - Plotting the probability of picking the best arm
 - Plotting the total nb of collision as a function of time
 - Plotting the cumulated total nb of collision as a function of time
 - Plotting the frequency of collision in each arm
  - For #$0$: $B(0.05)$,	frequency of collisions is 1.23529e-05  ...
  - For #$1$: $B(0.1)$,	frequency of collisions is 1.26471e-05  ...
  - For #$2$: $B(0.15)$,	frequency of collisions is 3.20588e-05  ...
  - For #$3$: $B(0.2)$,	frequency of collisions is 8.88235e-05  ...
  - For #$4$: $B(0.25)$,	frequency of collisions is 0.000106176  ...
  - For #$5$: $B(0.3)$,	frequency of collisions is 0.000153529  ...
  - For #$6$: $B(0.35)$,	frequency of collisions is 0.00018  ...
  - For #$7$: $B(0.4)$,	frequency of collisions is 0.000231471  ...
  - For #$8$: $B(0.45)$,	frequency of collisions is 0.000193235  ...
  - For #$9$: $B(0.5)$,	frequency of collisions is 0.000240294  ...
  - For #$10$: $B(0.55)$,	frequency of collisions is 0.000268235  ...
  - For #$11$: $B(0.6)$,	frequency of collisions is 0.000251765  ...
  - For #$12$: $B(0.65)$,	frequency of collisions is 0.000242941  ...
  - For #$13$: $B(0.7)$,	frequency of collisions is 0.000296471  ...
  - For #$14$: $B(0.75)$,	frequency of collisions is 0.000275588  ...
  - For #$15$: $B(0.8)$,	frequency of collisions is 0.000290294  ...
  - For #$16$: $B(0.85)$,	frequency of collisions is 0.000261176  ...
  - For #$17$: $B(0.9)$,	frequency of collisions is 0.00027  ...
  - For #$18$: $B(0.95)$,	frequency of collisions is 0.000316765  ...
