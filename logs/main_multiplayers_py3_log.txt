Loaded experiments configuration from 'configuration.py' :
configuration = {'averageOn': 0.001, 'environment': [{'params': [0.1, 0.5, 0.9], 'arm_type': <class 'Arms.Bernoulli.Bernoulli'>}], 'n_jobs': 1, 'players': [<PoliciesMultiPlayers.rhoRand.oneRhoRand object at 0x7f49428fa198>, <PoliciesMultiPlayers.rhoRand.oneRhoRand object at 0x7f494291d710>], 'verbosity': 6, 'finalRanksOnAverage': True, 'horizon': 10000, 'repetitions': 1, 'collisionModel': <function onlyUniqUserGetsReward at 0x7f4942a41a60>}
plots/ is already a directory here...
Number of players in the multi-players game: 2
Time horizon: 10000
Number of repetitions: 1
Using collision model: onlyUniqUserGetsReward
  Detail:  Simple collision model where only the players alone on one arm sample it and receive the reward.

    - This is the default collision model, cf. https://arxiv.org/abs/0910.2065v3 collision model 1.
    - The numpy array 'choices' is increased according to the number of users who collided (it is NOT binary).
    
Creating a new MAB problem ...
  Reading arms of this MAB problem from a dictionnary 'configuration' = {'params': [0.1, 0.5, 0.9], 'arm_type': <class 'Arms.Bernoulli.Bernoulli'>} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = [0.1, 0.5, 0.9]
 - with 'arms' = [B(0.1), B(0.5), B(0.9)]
 - with 'nbArms' = 3
 - with 'maxArm' = 0.9
Number of environments to try: 1

Evaluating environment: <MAB{'arms': [B(0.1), B(0.5), B(0.9)], 'maxArm': 0.90000000000000002, 'nbArms': 3}>
- Adding player #1 = #1<rhoRand, Thompson, rank:1> ...
  Using this already created player 'player' = #1<rhoRand, Thompson, rank:1> ...
- Adding player #2 = #2<rhoRand, Thompson, rank:1> ...
  Using this already created player 'player' = #2<rhoRand, Thompson, rank:1> ...
 - End of one game, rhoRand found orthogonal ranks: ranks = [1, 2] ...
Giving the final ranks ...

Final ranking for this environment #0 :
- Player #1, '#1<rhoRand, Thompson, rank:1>'	was ranked	1 / 2 for this simulation (last rewards = 8983).
- Player #2, '#2<rhoRand, Thompson, rank:1>'	was ranked	2 / 2 for this simulation (last rewards = 4996).
plots/MP__M2_T10000_N1__2_algos is already a directory here...


- Plotting the decentralized rewards, and saving the plot to plots/MP__M2_T10000_N1__2_algos/main____env1-1_6558509752773373976.png ...
Saving to plots/MP__M2_T10000_N1__2_algos/main____env1-1_6558509752773373976.png ...
fairness = [  5.56049822e-05   5.56049822e-05   5.56049822e-05 ...,   2.21975089e-01
   2.21975089e-01   2.22030694e-01]
Saving to plots/MP__M2_T10000_N1__2_algos/main____env1-1_6558509752773373976.png ...


- Plotting the centralized regret, and saving the plot to plots/MP__M2_T10000_N1__2_algos/main_RegretCentralized____env1-1_6558509752773373976.png ...
    Using oneLR = <function Bernoulli.oneLR at 0x7f4942a5b950>
  - Our lower bound gave = 2.173533813989595 ...
    Using kl = <function Bernoulli.kl at 0x7f4942a5b8c8>
  - The initial lower bound in Theorem 6 from [Anandkumar et al., 2010] gave = 1.3143267136515069 ...
Saving to plots/MP__M2_T10000_N1__2_algos/main_RegretCentralized____env1-1_6558509752773373976.png ...


- Plotting the normalized centralized rewards, and saving the plot to plots/MP__M2_T10000_N1__2_algos/main_NormalizedRewardsCentralized____env1-1_6558509752773373976.png ...
[0;31m---------------------------------------------------------------------------[0m
[0;31mAttributeError[0m                            Traceback (most recent call last)
[0;32m/home/lilian/ownCloud/cloud.openmailbox.org/Th√®se_2016-17/src/AlgoBandits.git/main_multiplayers.py[0m in [0;36m<module>[0;34m()[0m
[1;32m     90[0m         [0mprint[0m[0;34m([0m[0;34m"\n\n- Plotting the normalized centralized rewards, and saving the plot to {} ..."[0m[0;34m.[0m[0mformat[0m[0;34m([0m[0msavefig[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0m
[1;32m     91[0m         [0;31m# evaluation.plotRegretsCentralized(envId, savefig=savefig, semilogx=semilogx, normalized=True)[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 92[0;31m         [0mevaluation[0m[0;34m.[0m[0mplotRegretsCentralized[0m[0;34m([0m[0menvId[0m[0;34m,[0m [0msemilogx[0m[0;34m=[0m[0msemilogx[0m[0;34m,[0m [0mnormalized[0m[0;34m=[0m[0;32mTrue[0m[0;34m)[0m  [0;31m# XXX To plot without saving[0m[0;34m[0m[0m
[0m[1;32m     93[0m [0;34m[0m[0m
[1;32m     94[0m         [0;31m# Plotting the number of switches[0m[0;34m[0m[0;34m[0m[0m

[0;31mAttributeError[0m: 'EvaluatorMultiPlayers' object has no attribute 'plotRegretsCentralized'
