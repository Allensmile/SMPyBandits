 - Setting dpi of all figures to 110 ...
 - Setting 'figsize' of all figures to (19.8, 10.8) ...
Info: Using the regular tqdm() decorator ...
Info: numba.jit seems to be available.
Info: numba.jit seems to be available.
  Using this not-yet created player 'children[0]' = <class 'Policies.UCBalpha.UCBalpha'> ...
  Using this not-yet created player 'children[1]' = <class 'Policies.Thompson.Thompson'> ...
  Using this not-yet created player 'children[2]' = <class 'Policies.klUCBPlus.klUCBPlus'> ...
  Using this not-yet created player 'children[3]' = <class 'Policies.BayesUCB.BayesUCB'> ...
  Using this not-yet created player 'children[0]' = <class 'Policies.UCBalpha.UCBalpha'> ...
  Using this not-yet created player 'children[1]' = <class 'Policies.Thompson.Thompson'> ...
  Using this not-yet created player 'children[2]' = <class 'Policies.klUCBPlus.klUCBPlus'> ...
  Using this not-yet created player 'children[3]' = <class 'Policies.BayesUCB.BayesUCB'> ...
  Using this not-yet created player 'children[0]' = <class 'Policies.UCBalpha.UCBalpha'> ...
  Using this not-yet created player 'children[1]' = <class 'Policies.Thompson.Thompson'> ...
  Using this not-yet created player 'children[2]' = <class 'Policies.klUCBPlus.klUCBPlus'> ...
  Using this not-yet created player 'children[3]' = <class 'Policies.BayesUCB.BayesUCB'> ...
  Using this not-yet created player 'children[0]' = <class 'Policies.UCBalpha.UCBalpha'> ...
  Using this not-yet created player 'children[1]' = <class 'Policies.Thompson.Thompson'> ...
  Using this not-yet created player 'children[2]' = <class 'Policies.klUCBPlus.klUCBPlus'> ...
  Using this not-yet created player 'children[3]' = <class 'Policies.BayesUCB.BayesUCB'> ...
  Using this not-yet created player 'children[0]' = <class 'Policies.UCBalpha.UCBalpha'> ...
  Using this not-yet created player 'children[1]' = <class 'Policies.Thompson.Thompson'> ...
  Using this not-yet created player 'children[2]' = <class 'Policies.klUCBPlus.klUCBPlus'> ...
  Using this not-yet created player 'children[3]' = <class 'Policies.BayesUCB.BayesUCB'> ...
  Using this not-yet created player 'children[0]' = <class 'Policies.UCBalpha.UCBalpha'> ...
  Using this not-yet created player 'children[1]' = <class 'Policies.Thompson.Thompson'> ...
  Using this not-yet created player 'children[2]' = <class 'Policies.klUCBPlus.klUCBPlus'> ...
  Using this not-yet created player 'children[3]' = <class 'Policies.BayesUCB.BayesUCB'> ...
  Using this not-yet created player 'children[0]' = <class 'Policies.UCBalpha.UCBalpha'> ...
  Using this not-yet created player 'children[1]' = <class 'Policies.Thompson.Thompson'> ...
  Using this not-yet created player 'children[2]' = <class 'Policies.klUCBPlus.klUCBPlus'> ...
  Using this not-yet created player 'children[3]' = <class 'Policies.BayesUCB.BayesUCB'> ...
  Using this not-yet created player 'children[0]' = <class 'Policies.UCBalpha.UCBalpha'> ...
  Using this not-yet created player 'children[1]' = <class 'Policies.Thompson.Thompson'> ...
  Using this not-yet created player 'children[2]' = <class 'Policies.klUCBPlus.klUCBPlus'> ...
  Using this not-yet created player 'children[3]' = <class 'Policies.BayesUCB.BayesUCB'> ...
  Using this not-yet created player 'children[0]' = <class 'Policies.UCBalpha.UCBalpha'> ...
  Using this not-yet created player 'children[1]' = <class 'Policies.Thompson.Thompson'> ...
  Using this not-yet created player 'children[2]' = <class 'Policies.klUCBPlus.klUCBPlus'> ...
  Using this not-yet created player 'children[3]' = <class 'Policies.BayesUCB.BayesUCB'> ...
  Using this not-yet created player 'children[0]' = <class 'Policies.UCBalpha.UCBalpha'> ...
  Using this not-yet created player 'children[1]' = <class 'Policies.Thompson.Thompson'> ...
  Using this not-yet created player 'children[2]' = <class 'Policies.klUCBPlus.klUCBPlus'> ...
  Using this not-yet created player 'children[3]' = <class 'Policies.BayesUCB.BayesUCB'> ...
  Using this not-yet created player 'children[0]' = <class 'Policies.UCBalpha.UCBalpha'> ...
  Using this not-yet created player 'children[1]' = <class 'Policies.Thompson.Thompson'> ...
  Using this not-yet created player 'children[2]' = <class 'Policies.klUCBPlus.klUCBPlus'> ...
  Using this not-yet created player 'children[3]' = <class 'Policies.BayesUCB.BayesUCB'> ...
  Using this not-yet created player 'children[0]' = <class 'Policies.UCBalpha.UCBalpha'> ...
  Using this not-yet created player 'children[1]' = <class 'Policies.Thompson.Thompson'> ...
  Using this not-yet created player 'children[2]' = <class 'Policies.klUCBPlus.klUCBPlus'> ...
  Using this not-yet created player 'children[3]' = <class 'Policies.BayesUCB.BayesUCB'> ...
  Using this not-yet created player 'children[0]' = <class 'Policies.UCBalpha.UCBalpha'> ...
  Using this not-yet created player 'children[1]' = <class 'Policies.Thompson.Thompson'> ...
  Using this not-yet created player 'children[2]' = <class 'Policies.klUCBPlus.klUCBPlus'> ...
  Using this not-yet created player 'children[3]' = <class 'Policies.BayesUCB.BayesUCB'> ...
  Using this not-yet created player 'children[0]' = <class 'Policies.UCBalpha.UCBalpha'> ...
  Using this not-yet created player 'children[1]' = <class 'Policies.Thompson.Thompson'> ...
  Using this not-yet created player 'children[2]' = <class 'Policies.klUCBPlus.klUCBPlus'> ...
  Using this not-yet created player 'children[3]' = <class 'Policies.BayesUCB.BayesUCB'> ...
  Using this not-yet created player 'children[0]' = <class 'Policies.UCBalpha.UCBalpha'> ...
  Using this not-yet created player 'children[1]' = <class 'Policies.Thompson.Thompson'> ...
  Using this not-yet created player 'children[2]' = <class 'Policies.klUCBPlus.klUCBPlus'> ...
  Using this not-yet created player 'children[3]' = <class 'Policies.BayesUCB.BayesUCB'> ...
  Using this not-yet created player 'children[0]' = <class 'Policies.UCBalpha.UCBalpha'> ...
  Using this not-yet created player 'children[1]' = <class 'Policies.Thompson.Thompson'> ...
  Using this not-yet created player 'children[2]' = <class 'Policies.klUCBPlus.klUCBPlus'> ...
  Using this not-yet created player 'children[3]' = <class 'Policies.BayesUCB.BayesUCB'> ...
  Using this not-yet created player 'children[0]' = <class 'Policies.UCBalpha.UCBalpha'> ...
  Using this not-yet created player 'children[1]' = <class 'Policies.Thompson.Thompson'> ...
  Using this not-yet created player 'children[2]' = <class 'Policies.klUCBPlus.klUCBPlus'> ...
  Using this not-yet created player 'children[3]' = <class 'Policies.BayesUCB.BayesUCB'> ...
  Using this not-yet created player 'children[0]' = <class 'Policies.UCBalpha.UCBalpha'> ...
  Using this not-yet created player 'children[1]' = <class 'Policies.Thompson.Thompson'> ...
  Using this not-yet created player 'children[2]' = <class 'Policies.klUCBPlus.klUCBPlus'> ...
  Using this not-yet created player 'children[3]' = <class 'Policies.BayesUCB.BayesUCB'> ...
  Using this not-yet created player 'children[0]' = <class 'Policies.UCBalpha.UCBalpha'> ...
  Using this not-yet created player 'children[1]' = <class 'Policies.Thompson.Thompson'> ...
  Using this not-yet created player 'children[2]' = <class 'Policies.klUCBPlus.klUCBPlus'> ...
  Using this not-yet created player 'children[3]' = <class 'Policies.BayesUCB.BayesUCB'> ...
  Using this not-yet created player 'children[0]' = <class 'Policies.UCBalpha.UCBalpha'> ...
  Using this not-yet created player 'children[1]' = <class 'Policies.Thompson.Thompson'> ...
  Using this not-yet created player 'children[2]' = <class 'Policies.klUCBPlus.klUCBPlus'> ...
  Using this not-yet created player 'children[3]' = <class 'Policies.BayesUCB.BayesUCB'> ...
  Using this not-yet created player 'children[0]' = <class 'Policies.UCBalpha.UCBalpha'> ...
  Using this not-yet created player 'children[1]' = <class 'Policies.Thompson.Thompson'> ...
  Using this not-yet created player 'children[2]' = <class 'Policies.klUCBPlus.klUCBPlus'> ...
  Using this not-yet created player 'children[3]' = <class 'Policies.BayesUCB.BayesUCB'> ...
  Using this not-yet created player 'children[0]' = <class 'Policies.UCBalpha.UCBalpha'> ...
  Using this not-yet created player 'children[1]' = <class 'Policies.Thompson.Thompson'> ...
  Using this not-yet created player 'children[2]' = <class 'Policies.klUCBPlus.klUCBPlus'> ...
  Using this not-yet created player 'children[3]' = <class 'Policies.BayesUCB.BayesUCB'> ...
  Using this not-yet created player 'children[0]' = <class 'Policies.UCBalpha.UCBalpha'> ...
  Using this not-yet created player 'children[1]' = <class 'Policies.Thompson.Thompson'> ...
  Using this not-yet created player 'children[2]' = <class 'Policies.klUCBPlus.klUCBPlus'> ...
  Using this not-yet created player 'children[3]' = <class 'Policies.BayesUCB.BayesUCB'> ...
  Using this not-yet created player 'children[0]' = <class 'Policies.UCBalpha.UCBalpha'> ...
  Using this not-yet created player 'children[1]' = <class 'Policies.Thompson.Thompson'> ...
  Using this not-yet created player 'children[2]' = <class 'Policies.klUCBPlus.klUCBPlus'> ...
  Using this not-yet created player 'children[3]' = <class 'Policies.BayesUCB.BayesUCB'> ...
Loaded experiments configuration from 'configuration.py' :
configuration = {'delta_t_save': 1, 'repetitions': 1, 'n_jobs': 2, 'successive_players': [[Selfish(BayesUCB), Selfish(BayesUCB), Selfish(BayesUCB), Selfish(BayesUCB), Selfish(BayesUCB), Selfish(BayesUCB), Selfish(BayesUCB), Selfish(BayesUCB), Selfish(BayesUCB), Selfish(BayesUCB), Selfish(BayesUCB), Selfish(BayesUCB)], [rhoRand(BayesUCB), rhoRand(BayesUCB), rhoRand(BayesUCB), rhoRand(BayesUCB), rhoRand(BayesUCB), rhoRand(BayesUCB), rhoRand(BayesUCB), rhoRand(BayesUCB), rhoRand(BayesUCB), rhoRand(BayesUCB), rhoRand(BayesUCB), rhoRand(BayesUCB)], [Selfish(Aggr($N=4$)), Selfish(Aggr($N=4$)), Selfish(Aggr($N=4$)), Selfish(Aggr($N=4$)), Selfish(Aggr($N=4$)), Selfish(Aggr($N=4$)), Selfish(Aggr($N=4$)), Selfish(Aggr($N=4$)), Selfish(Aggr($N=4$)), Selfish(Aggr($N=4$)), Selfish(Aggr($N=4$)), Selfish(Aggr($N=4$))], [rhoRand(Aggr($N=4$)), rhoRand(Aggr($N=4$)), rhoRand(Aggr($N=4$)), rhoRand(Aggr($N=4$)), rhoRand(Aggr($N=4$)), rhoRand(Aggr($N=4$)), rhoRand(Aggr($N=4$)), rhoRand(Aggr($N=4$)), rhoRand(Aggr($N=4$)), rhoRand(Aggr($N=4$)), rhoRand(Aggr($N=4$)), rhoRand(Aggr($N=4$))]], 'collisionModel': <function onlyUniqUserGetsReward at 0x7f693c8bfc80>, 'finalRanksOnAverage': True, 'verbosity': 6, 'players': [rhoRand(UCB), rhoRand(UCB), rhoRand(UCB), rhoRand(UCB), rhoRand(UCB), rhoRand(UCB), rhoRand(UCB), rhoRand(UCB), rhoRand(UCB), rhoRand(UCB), rhoRand(UCB), rhoRand(UCB)], 'averageOn': 0.001, 'horizon': 10000, 'environment': [{'params': array([ 0.06666667,  0.13333333,  0.2       ,  0.26666667,  0.33333333,
        0.4       ,  0.46666667,  0.53333333,  0.6       ,  0.66666667,
        0.73333333,  0.8       ,  0.86666667,  0.93333333]), 'arm_type': <class 'Arms.Bernoulli.Bernoulli'>}]}
plots/ is already a directory here...
Number of players in the multi-players game: 12
Time horizon: 10000
Number of repetitions: 1
Sampling rate for saving, delta_t_save: 1
Sampling rate for plotting, delta_t_plot: 1
Number of jobs for parallelization: 2
Using collision model: onlyUniqUserGetsReward
  Detail:  Simple collision model where only the players alone on one arm sample it and receive the reward.

    - This is the default collision model, cf. https://arxiv.org/abs/0910.2065v3 collision model 1.
    - The numpy array 'choices' is increased according to the number of users who collided (it is NOT binary).
    
Creating a new MAB problem ...
  Reading arms of this MAB problem from a dictionnary 'configuration' = {'params': array([ 0.06666667,  0.13333333,  0.2       ,  0.26666667,  0.33333333,
        0.4       ,  0.46666667,  0.53333333,  0.6       ,  0.66666667,
        0.73333333,  0.8       ,  0.86666667,  0.93333333]), 'arm_type': <class 'Arms.Bernoulli.Bernoulli'>} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = [ 0.06666667  0.13333333  0.2         0.26666667  0.33333333  0.4
  0.46666667  0.53333333  0.6         0.66666667  0.73333333  0.8
  0.86666667  0.93333333]
 - with 'arms' = [B(0.0667), B(0.133), B(0.2), B(0.267), B(0.333), B(0.4), B(0.467), B(0.533), B(0.6), B(0.667), B(0.733), B(0.8), B(0.867), B(0.933)]
 - with 'nbArms' = 14
 - with 'maxArm' = 0.933333333333
 - with 'minArm' = 0.0666666666667

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 10.4 ... 
 - a Optimal Arm Identification factor H_OI(mu) = 49.52% ...
Number of environments to try: 1

Evaluating environment: MAB(nbArms: 14, arms: [B(0.0667), B(0.133), B(0.2), B(0.267), B(0.333), B(0.4), B(0.467), B(0.533), B(0.6), B(0.667), B(0.733), B(0.8), B(0.867), B(0.933)], minArm: 0.0667, maxArm: 0.933)
- Adding player #1 = #1<$\rho^{\mathrm{Rand}}$, UCB> ...
  Using this already created player 'player' = #1<$\rho^{\mathrm{Rand}}$, UCB> ...
- Adding player #2 = #2<$\rho^{\mathrm{Rand}}$, UCB> ...
  Using this already created player 'player' = #2<$\rho^{\mathrm{Rand}}$, UCB> ...
- Adding player #3 = #3<$\rho^{\mathrm{Rand}}$, UCB> ...
  Using this already created player 'player' = #3<$\rho^{\mathrm{Rand}}$, UCB> ...
- Adding player #4 = #4<$\rho^{\mathrm{Rand}}$, UCB> ...
  Using this already created player 'player' = #4<$\rho^{\mathrm{Rand}}$, UCB> ...
- Adding player #5 = #5<$\rho^{\mathrm{Rand}}$, UCB> ...
  Using this already created player 'player' = #5<$\rho^{\mathrm{Rand}}$, UCB> ...
- Adding player #6 = #6<$\rho^{\mathrm{Rand}}$, UCB> ...
  Using this already created player 'player' = #6<$\rho^{\mathrm{Rand}}$, UCB> ...
- Adding player #7 = #7<$\rho^{\mathrm{Rand}}$, UCB> ...
  Using this already created player 'player' = #7<$\rho^{\mathrm{Rand}}$, UCB> ...
- Adding player #8 = #8<$\rho^{\mathrm{Rand}}$, UCB> ...
  Using this already created player 'player' = #8<$\rho^{\mathrm{Rand}}$, UCB> ...
- Adding player #9 = #9<$\rho^{\mathrm{Rand}}$, UCB> ...
  Using this already created player 'player' = #9<$\rho^{\mathrm{Rand}}$, UCB> ...
- Adding player #10 = #10<$\rho^{\mathrm{Rand}}$, UCB> ...
  Using this already created player 'player' = #10<$\rho^{\mathrm{Rand}}$, UCB> ...
- Adding player #11 = #11<$\rho^{\mathrm{Rand}}$, UCB> ...
  Using this already created player 'player' = #11<$\rho^{\mathrm{Rand}}$, UCB> ...
- Adding player #12 = #12<$\rho^{\mathrm{Rand}}$, UCB> ...
  Using this already created player 'player' = #12<$\rho^{\mathrm{Rand}}$, UCB> ...

Estimated order by the policy #1<$\rho^{\mathrm{Rand}}$, UCB, rank:1> after 10000 steps: [ 0  1  2  3  4  6  5  7  8  9 10 11 12 13] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 97.96% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 100.00% (relative success)...
  ==> Spearman    distance from optimal ordering: 100.00% (relative success)...
  ==> Gestalt     distance from optimal ordering: 92.86% (relative success)...
  ==> Mean distance from optimal ordering: 97.70% (relative success)...

Estimated order by the policy #2<$\rho^{\mathrm{Rand}}$, UCB, rank:10> after 10000 steps: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 100.00% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 100.00% (relative success)...
  ==> Spearman    distance from optimal ordering: 100.00% (relative success)...
  ==> Gestalt     distance from optimal ordering: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 100.00% (relative success)...

Estimated order by the policy #3<$\rho^{\mathrm{Rand}}$, UCB, rank:6> after 10000 steps: [ 0  1  2  3  5  4  6  7  8  9 10 11 12 13] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 97.96% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 100.00% (relative success)...
  ==> Spearman    distance from optimal ordering: 100.00% (relative success)...
  ==> Gestalt     distance from optimal ordering: 92.86% (relative success)...
  ==> Mean distance from optimal ordering: 97.70% (relative success)...

Estimated order by the policy #4<$\rho^{\mathrm{Rand}}$, UCB, rank:12> after 10000 steps: [ 1  2  0  3  4  5  6  7  8  9 10 11 12 13] ...
  ==> Optimal arm identification: 98.04% (relative success)...
  ==> Manhattan   distance from optimal ordering: 95.92% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 100.00% (relative success)...
  ==> Spearman    distance from optimal ordering: 100.00% (relative success)...
  ==> Gestalt     distance from optimal ordering: 92.86% (relative success)...
  ==> Mean distance from optimal ordering: 97.19% (relative success)...

Estimated order by the policy #5<$\rho^{\mathrm{Rand}}$, UCB, rank:7> after 10000 steps: [ 1  0  2  3  4  5  6  7  8  9 11 10 12 13] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 95.92% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 100.00% (relative success)...
  ==> Spearman    distance from optimal ordering: 100.00% (relative success)...
  ==> Gestalt     distance from optimal ordering: 85.71% (relative success)...
  ==> Mean distance from optimal ordering: 95.41% (relative success)...

Estimated order by the policy #6<$\rho^{\mathrm{Rand}}$, UCB, rank:4> after 10000 steps: [ 1  0  2  3  4  5  6  7  8 10  9 11 12 13] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 95.92% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 100.00% (relative success)...
  ==> Spearman    distance from optimal ordering: 100.00% (relative success)...
  ==> Gestalt     distance from optimal ordering: 85.71% (relative success)...
  ==> Mean distance from optimal ordering: 95.41% (relative success)...

Estimated order by the policy #7<$\rho^{\mathrm{Rand}}$, UCB, rank:9> after 10000 steps: [ 1  0  2  3  4  5  6  7  8  9 10 11 12 13] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 97.96% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 100.00% (relative success)...
  ==> Spearman    distance from optimal ordering: 100.00% (relative success)...
  ==> Gestalt     distance from optimal ordering: 92.86% (relative success)...
  ==> Mean distance from optimal ordering: 97.70% (relative success)...

Estimated order by the policy #8<$\rho^{\mathrm{Rand}}$, UCB, rank:11> after 10000 steps: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 100.00% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 100.00% (relative success)...
  ==> Spearman    distance from optimal ordering: 100.00% (relative success)...
  ==> Gestalt     distance from optimal ordering: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 100.00% (relative success)...

Estimated order by the policy #9<$\rho^{\mathrm{Rand}}$, UCB, rank:8> after 10000 steps: [ 1  0  2  3  4  5  6  7  8  9 10 11 12 13] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 97.96% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 100.00% (relative success)...
  ==> Spearman    distance from optimal ordering: 100.00% (relative success)...
  ==> Gestalt     distance from optimal ordering: 92.86% (relative success)...
  ==> Mean distance from optimal ordering: 97.70% (relative success)...

Estimated order by the policy #10<$\rho^{\mathrm{Rand}}$, UCB, rank:2> after 10000 steps: [ 0  1  3  2  4  5  6  7  8 10  9 11 12 13] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 95.92% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 100.00% (relative success)...
  ==> Spearman    distance from optimal ordering: 100.00% (relative success)...
  ==> Gestalt     distance from optimal ordering: 85.71% (relative success)...
  ==> Mean distance from optimal ordering: 95.41% (relative success)...

Estimated order by the policy #11<$\rho^{\mathrm{Rand}}$, UCB, rank:3> after 10000 steps: [ 1  0  2  3  4  5  6  7  8  9 10 11 12 13] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 97.96% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 100.00% (relative success)...
  ==> Spearman    distance from optimal ordering: 100.00% (relative success)...
  ==> Gestalt     distance from optimal ordering: 92.86% (relative success)...
  ==> Mean distance from optimal ordering: 97.70% (relative success)...

Estimated order by the policy #12<$\rho^{\mathrm{Rand}}$, UCB, rank:12> after 10000 steps: [ 1  2  0  3  4  5  6  7  8  9 10 11 12 13] ...
  ==> Optimal arm identification: 98.04% (relative success)...
  ==> Manhattan   distance from optimal ordering: 95.92% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 100.00% (relative success)...
  ==> Spearman    distance from optimal ordering: 100.00% (relative success)...
  ==> Gestalt     distance from optimal ordering: 92.86% (relative success)...
  ==> Mean distance from optimal ordering: 97.19% (relative success)...
Giving the final ranks ...

Final ranking for this environment #0 :
- Player #9, '#9<$\rho^{\mathrm{Rand}}$, UCB>'	was ranked	1 / 12 for this simulation (last rewards = 2674).
- Player #7, '#7<$\rho^{\mathrm{Rand}}$, UCB>'	was ranked	2 / 12 for this simulation (last rewards = 2669).
- Player #12, '#12<$\rho^{\mathrm{Rand}}$, UCB>'	was ranked	3 / 12 for this simulation (last rewards = 2655).
- Player #10, '#10<$\rho^{\mathrm{Rand}}$, UCB>'	was ranked	4 / 12 for this simulation (last rewards = 2654).
- Player #1, '#1<$\rho^{\mathrm{Rand}}$, UCB>'	was ranked	5 / 12 for this simulation (last rewards = 2645).
- Player #4, '#4<$\rho^{\mathrm{Rand}}$, UCB>'	was ranked	6 / 12 for this simulation (last rewards = 2636).
- Player #2, '#2<$\rho^{\mathrm{Rand}}$, UCB>'	was ranked	7 / 12 for this simulation (last rewards = 2623).
- Player #5, '#5<$\rho^{\mathrm{Rand}}$, UCB>'	was ranked	8 / 12 for this simulation (last rewards = 2614).
- Player #3, '#3<$\rho^{\mathrm{Rand}}$, UCB>'	was ranked	9 / 12 for this simulation (last rewards = 2603).
- Player #6, '#6<$\rho^{\mathrm{Rand}}$, UCB>'	was ranked	10 / 12 for this simulation (last rewards = 2575).
- Player #11, '#11<$\rho^{\mathrm{Rand}}$, UCB>'	was ranked	11 / 12 for this simulation (last rewards = 2525).
- Player #8, '#8<$\rho^{\mathrm{Rand}}$, UCB>'	was ranked	12 / 12 for this simulation (last rewards = 2455).


- Plotting the decentralized rewards


- Plotting the centralized fairness (STD)


- Plotting the centralized regret
  - For 12 players, our lower bound gave = 74.9 ...
  - For 12 players, the initial lower bound in Theorem 6 from [Anandkumar et al., 2010] gave = 12.9 ...

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 10.4 for 1-player problem ... 
 - a Optimal Arm Identification factor H_OI(mu) = 49.52% ...
 - Our lowerbound = 74.9,
 - [Anandkumar et al] lowerbound = 12.9


- Plotting the centralized regret
  - For 12 players, our lower bound gave = 74.9 ...
  - For 12 players, the initial lower bound in Theorem 6 from [Anandkumar et al., 2010] gave = 12.9 ...

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 10.4 for 1-player problem ... 
 - a Optimal Arm Identification factor H_OI(mu) = 49.52% ...
 - Our lowerbound = 74.9,
 - [Anandkumar et al] lowerbound = 12.9


- Plotting the centralized regret
  - For 12 players, our lower bound gave = 74.9 ...
  - For 12 players, the initial lower bound in Theorem 6 from [Anandkumar et al., 2010] gave = 12.9 ...

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 10.4 for 1-player problem ... 
 - a Optimal Arm Identification factor H_OI(mu) = 49.52% ...
 - Our lowerbound = 74.9,
 - [Anandkumar et al] lowerbound = 12.9
  - For 12 players, our lower bound gave = 74.9 ...
  - For 12 players, the initial lower bound in Theorem 6 from [Anandkumar et al., 2010] gave = 12.9 ...

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 10.4 for 1-player problem ... 
 - a Optimal Arm Identification factor H_OI(mu) = 49.52% ...
 - Our lowerbound = 74.9,
 - [Anandkumar et al] lowerbound = 12.9
  - For 12 players, our lower bound gave = 74.9 ...
  - For 12 players, the initial lower bound in Theorem 6 from [Anandkumar et al., 2010] gave = 12.9 ...

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 10.4 for 1-player problem ... 
 - a Optimal Arm Identification factor H_OI(mu) = 49.52% ...
 - Our lowerbound = 74.9,
 - [Anandkumar et al] lowerbound = 12.9


- Plotting the centralized regret
  - For 12 players, our lower bound gave = 74.9 ...
  - For 12 players, the initial lower bound in Theorem 6 from [Anandkumar et al., 2010] gave = 12.9 ...

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 10.4 for 1-player problem ... 
 - a Optimal Arm Identification factor H_OI(mu) = 49.52% ...
 - Our lowerbound = 74.9,
 - [Anandkumar et al] lowerbound = 12.9


- Plotting the number of switches
