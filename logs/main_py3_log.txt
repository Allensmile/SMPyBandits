Loaded experiments configuration from 'configuration.py' :
configuration['policies'] = [{'params': {'decreaseRate': 'auto', 'children': [<Policies.TakeFixedArm.TakeFixedArm object at 0x7f7f194d5080>, <Policies.TakeFixedArm.TakeFixedArm object at 0x7f7f0b377400>, <Policies.TakeFixedArm.TakeFixedArm object at 0x7f7f2f162be0>], 'update_like_exp4': True, 'unbiased': False, 'update_all_children': False, 'learningRate': 0.01}, 'archtype': <class 'Policies.Aggr.Aggr'>}, {'params': {'decreaseRate': 'auto', 'children': [<Policies.TakeFixedArm.TakeFixedArm object at 0x7f7f194d5080>, <Policies.TakeFixedArm.TakeFixedArm object at 0x7f7f0b377400>, <Policies.TakeFixedArm.TakeFixedArm object at 0x7f7f2f162be0>], 'update_like_exp4': False, 'unbiased': False, 'update_all_children': False, 'learningRate': 0.01}, 'archtype': <class 'Policies.Aggr.Aggr'>}, <Policies.TakeFixedArm.TakeFixedArm object at 0x7f7f194d5080>, <Policies.TakeFixedArm.TakeFixedArm object at 0x7f7f0b377400>, <Policies.TakeFixedArm.TakeFixedArm object at 0x7f7f2f162be0>]
plots/ is already a directory here...
Number of policies in this comparaison: 5
Time horizon: 2000
Number of repetitions: 4
Sampling rate DELTA_T_SAVE: 1
Creating a new MAB problem ...
  Reading arms of this MAB problem from a dictionnary 'configuration' = {'params': [0.1, 0.5, 0.9], 'arm_type': <class 'Arms.Bernoulli.Bernoulli'>} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = [0.1, 0.5, 0.9]
 - with 'arms' = [B(0.1), B(0.5), B(0.9)]
 - with 'nbArms' = 3
 - with 'maxArm' = 0.9
 - with 'minArm' = 0.1
Number of environments to try: 1

Evaluating environment: <MAB{'arms': [B(0.1), B(0.5), B(0.9)], 'nbArms': 3, 'maxArm': 0.90000000000000002, 'minArm': 0.10000000000000001}>
- Adding policy #1 = {'params': {'decreaseRate': 'auto', 'children': [<Policies.TakeFixedArm.TakeFixedArm object at 0x7f7f194d5080>, <Policies.TakeFixedArm.TakeFixedArm object at 0x7f7f0b377400>, <Policies.TakeFixedArm.TakeFixedArm object at 0x7f7f2f162be0>], 'update_like_exp4': True, 'unbiased': False, 'update_all_children': False, 'learningRate': 0.01}, 'archtype': <class 'Policies.Aggr.Aggr'>} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][0]' = {'params': {'decreaseRate': 'auto', 'children': [<Policies.TakeFixedArm.TakeFixedArm object at 0x7f7f194d5080>, <Policies.TakeFixedArm.TakeFixedArm object at 0x7f7f0b377400>, <Policies.TakeFixedArm.TakeFixedArm object at 0x7f7f2f162be0>], 'update_like_exp4': True, 'unbiased': False, 'update_all_children': False, 'learningRate': 0.01}, 'archtype': <class 'Policies.Aggr.Aggr'>} ...
  Using this already created player 'children[0]' = TakeFixedArm(0) ...
  Using this already created player 'children[1]' = TakeFixedArm(1) ...
  Using this already created player 'children[2]' = TakeFixedArm(2) ...
- Adding policy #2 = {'params': {'decreaseRate': 'auto', 'children': [<Policies.TakeFixedArm.TakeFixedArm object at 0x7f7f194d5080>, <Policies.TakeFixedArm.TakeFixedArm object at 0x7f7f0b377400>, <Policies.TakeFixedArm.TakeFixedArm object at 0x7f7f2f162be0>], 'update_like_exp4': False, 'unbiased': False, 'update_all_children': False, 'learningRate': 0.01}, 'archtype': <class 'Policies.Aggr.Aggr'>} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][1]' = {'params': {'decreaseRate': 'auto', 'children': [<Policies.TakeFixedArm.TakeFixedArm object at 0x7f7f194d5080>, <Policies.TakeFixedArm.TakeFixedArm object at 0x7f7f0b377400>, <Policies.TakeFixedArm.TakeFixedArm object at 0x7f7f2f162be0>], 'update_like_exp4': False, 'unbiased': False, 'update_all_children': False, 'learningRate': 0.01}, 'archtype': <class 'Policies.Aggr.Aggr'>} ...
  Using this already created player 'children[0]' = TakeFixedArm(0) ...
  Using this already created player 'children[1]' = TakeFixedArm(1) ...
  Using this already created player 'children[2]' = TakeFixedArm(2) ...
- Adding policy #3 = TakeFixedArm(0) ...
  Using this already created policy 'self.cfg['policies'][2]' = TakeFixedArm(0) ...
- Adding policy #4 = TakeFixedArm(1) ...
  Using this already created policy 'self.cfg['policies'][3]' = TakeFixedArm(1) ...
- Adding policy #5 = TakeFixedArm(2) ...
  Using this already created policy 'self.cfg['policies'][4]' = TakeFixedArm(2) ...

===> Pre-computing the rewards ... Of shape (3, 2000) ...
    In order for all simulated algorithms to face the same random rewards (robust comparaison of A1,..,An vs Aggr(A1,..,An)) ...


- Evaluating policy #1/5: Aggr($N=3$, Exp4) ...

- Evaluating policy #2/5: Aggr($N=3$) ...

- Evaluating policy #3/5: TakeFixedArm(0) ...

- Evaluating policy #4/5: TakeFixedArm(1) ...

- Evaluating policy #5/5: TakeFixedArm(2) ...
Giving the final ranks ...

Final ranking for this environment #0 :
- Policy 'TakeFixedArm(2)'	was ranked	1 / 5 for this simulation (last regret = -5.85).
- Policy 'Aggr($N=3$, Exp4)'	was ranked	2 / 5 for this simulation (last regret = 13.15).
- Policy 'Aggr($N=3$)'	was ranked	3 / 5 for this simulation (last regret = 201.9).
- Policy 'TakeFixedArm(1)'	was ranked	4 / 5 for this simulation (last regret = 785.15).
- Policy 'TakeFixedArm(0)'	was ranked	5 / 5 for this simulation (last regret = 1596.65).
