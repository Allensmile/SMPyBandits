Loaded experiments configuration from 'configuration.py' :
configuration['policies'] = [{'archtype': <class 'Policies.UCBtuned.UCBtuned'>, 'params': {}}, {'archtype': <class 'Policies.Softmax.Softmax'>, 'params': {'temperature': 0.05}}, {'archtype': <class 'Policies.Softmax.SoftmaxDecreasing'>, 'params': {}}, {'archtype': <class 'Policies.Softmax.SoftmaxWithHorizon'>, 'params': {'horizon': 10000}}, {'archtype': <class 'Policies.MOSS.MOSS'>, 'params': {}}, {'archtype': <class 'Policies.Thompson.Thompson'>, 'params': {}}, {'archtype': <class 'Policies.klUCB.klUCB'>, 'params': {}}, {'archtype': <class 'Policies.BayesUCB.BayesUCB'>, 'params': {}}]
plots/ is already a directory here...
Number of policies in this comparaison: 8
Time horizon: 10000
Number of repetitions: 50
Creating a new MAB problem ...
  Reading arms of this MAB problem from a dictionnary 'configuration' = {'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]
 - with 'arms' = [B(0.1), B(0.2), B(0.3), B(0.4), B(0.5), B(0.6), B(0.7), B(0.8), B(0.9)]
 - with 'nbArms' = 9
 - with 'maxArm' = 0.9
Number of environments to try: 1

Evaluating environment: <MAB{'maxArm': 0.9, 'nbArms': 9, 'arms': [B(0.1), B(0.2), B(0.3), B(0.4), B(0.5), B(0.6), B(0.7), B(0.8), B(0.9)]}>
- Adding policy #1 = {'archtype': <class 'Policies.UCBtuned.UCBtuned'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][0]' = {'archtype': <class 'Policies.UCBtuned.UCBtuned'>, 'params': {}} ...
- Adding policy #2 = {'archtype': <class 'Policies.Softmax.Softmax'>, 'params': {'temperature': 0.05}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][1]' = {'archtype': <class 'Policies.Softmax.Softmax'>, 'params': {'temperature': 0.05}} ...
- Adding policy #3 = {'archtype': <class 'Policies.Softmax.SoftmaxDecreasing'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][2]' = {'archtype': <class 'Policies.Softmax.SoftmaxDecreasing'>, 'params': {}} ...
- Adding policy #4 = {'archtype': <class 'Policies.Softmax.SoftmaxWithHorizon'>, 'params': {'horizon': 10000}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][3]' = {'archtype': <class 'Policies.Softmax.SoftmaxWithHorizon'>, 'params': {'horizon': 10000}} ...
- Adding policy #5 = {'archtype': <class 'Policies.MOSS.MOSS'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][4]' = {'archtype': <class 'Policies.MOSS.MOSS'>, 'params': {}} ...
- Adding policy #6 = {'archtype': <class 'Policies.Thompson.Thompson'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][5]' = {'archtype': <class 'Policies.Thompson.Thompson'>, 'params': {}} ...
- Adding policy #7 = {'archtype': <class 'Policies.klUCB.klUCB'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][6]' = {'archtype': <class 'Policies.klUCB.klUCB'>, 'params': {}} ...
- Adding policy #8 = {'archtype': <class 'Policies.BayesUCB.BayesUCB'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][7]' = {'archtype': <class 'Policies.BayesUCB.BayesUCB'>, 'params': {}} ...

- Evaluating policy #1/8: UCBtuned ...

- Evaluating policy #2/8: Softmax(temp: 0.05) ...

- Evaluating policy #3/8: Softmax(decreasing) ...

- Evaluating policy #4/8: Softmax(horizon: 10000) ...

- Evaluating policy #5/8: MOSS ...

- Evaluating policy #6/8: Thompson ...

- Evaluating policy #7/8: klUCB ...

- Evaluating policy #8/8: BayesUCB ...
Giving the final ranks ...

Final ranking for this environment #0 :
- Policy 'Thompson'	was ranked	1 / 8 for this simulation (last regret = 46.4).
- Policy 'BayesUCB'	was ranked	2 / 8 for this simulation (last regret = 46.7).
- Policy 'klUCB'	was ranked	3 / 8 for this simulation (last regret = 61.8).
- Policy 'MOSS'	was ranked	4 / 8 for this simulation (last regret = 84.66).
- Policy 'UCBtuned'	was ranked	5 / 8 for this simulation (last regret = 146.32).
- Policy 'Softmax(decreasing)'	was ranked	6 / 8 for this simulation (last regret = 215.38).
- Policy 'Softmax(temp: 0.05)'	was ranked	7 / 8 for this simulation (last regret = 338.52).
- Policy 'Softmax(horizon: 10000)'	was ranked	8 / 8 for this simulation (last regret = 347.58).
plots/T10000_N50__8_algos is already a directory here...
