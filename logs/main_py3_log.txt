 - Setting dpi of all figures to 110 ...
 - Setting 'figsize' of all figures to (19.8, 10.8) ...
Info: Using the regular tqdm() decorator ...
Info: numba.jit seems to be available.
Info: numba.jit seems to be available.
Loaded experiments configuration from 'configuration.py' :
configuration['policies'] = [{'archtype': <class 'Policies.BlackBoxOpt.BlackBoxOpt'>, 'params': {}}, {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 4}}, {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 1}}, {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.5}}, {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.05}}, {'archtype': <class 'Policies.DMED.DMED'>, 'params': {'genuine': True}}, {'archtype': <class 'Policies.Thompson.Thompson'>, 'params': {'posterior': <class 'Policies.Posterior.Beta.Beta'>}}, {'archtype': <class 'Policies.klUCB.klUCB'>, 'params': {'klucb': <built-in function klucbBern>}}, {'archtype': <class 'Policies.klUCBPlus.klUCBPlus'>, 'params': {'klucb': <built-in function klucbBern>}}, {'archtype': <class 'Policies.BayesUCB.BayesUCB'>, 'params': {'posterior': <class 'Policies.Posterior.Beta.Beta'>}}]
====> TURNING DEBUG MODE ON <=====
plots/ is already a directory here...
Number of policies in this comparison: 10
Time horizon: 10000
Number of repetitions: 4
Sampling rate for saving, delta_t_save: 1
Sampling rate for plotting, delta_t_plot: 1
Number of jobs for parallelization: 4
Creating a new MAB problem ...
  Reading arms of this MAB problem from a dictionnary 'configuration' = {'arm_type': <class 'Arms.Gaussian.Gaussian'>, 'params': [(0.2, 0.05), (0.5, 0.05), (0.8, 0.05)]} ...
 - with 'arm_type' = <class 'Arms.Gaussian.Gaussian'>
 - with 'params' = [(0.2, 0.05), (0.5, 0.05), (0.8, 0.05)]
 - with 'arms' = [G(0.2, 0.05), G(0.5, 0.05), G(0.8, 0.05)]
 - with 'means' = [ 0.2  0.5  0.8]
 - with 'nbArms' = 3
 - with 'maxArm' = 0.8
 - with 'minArm' = 0.2

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 0.5 ... 
 - a Optimal Arm Identification factor H_OI(mu) = 36.67% ...
 - with 'arms' represented as: $[G(0.2, 0.05), G(0.5, 0.05), G(0.8, 0.05)^*]$
Number of environments to try: 1

Evaluating environment: MAB(nbArms: 3, arms: [G(0.2, 0.05), G(0.5, 0.05), G(0.8, 0.05)], minArm: 0.2, maxArm: 0.8)
- Adding policy #1 = {'archtype': <class 'Policies.BlackBoxOpt.BlackBoxOpt'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][0]' = {'archtype': <class 'Policies.BlackBoxOpt.BlackBoxOpt'>, 'params': {}} ...
- Adding policy #2 = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 4}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][1]' = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 4}} ...
- Adding policy #3 = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 1}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][2]' = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 1}} ...
- Adding policy #4 = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.5}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][3]' = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.5}} ...
- Adding policy #5 = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.05}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][4]' = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.05}} ...
- Adding policy #6 = {'archtype': <class 'Policies.DMED.DMED'>, 'params': {'genuine': True}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][5]' = {'archtype': <class 'Policies.DMED.DMED'>, 'params': {'genuine': True}} ...
- Adding policy #7 = {'archtype': <class 'Policies.Thompson.Thompson'>, 'params': {'posterior': <class 'Policies.Posterior.Beta.Beta'>}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][6]' = {'archtype': <class 'Policies.Thompson.Thompson'>, 'params': {'posterior': <class 'Policies.Posterior.Beta.Beta'>}} ...
- Adding policy #8 = {'archtype': <class 'Policies.klUCB.klUCB'>, 'params': {'klucb': <built-in function klucbBern>}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][7]' = {'archtype': <class 'Policies.klUCB.klUCB'>, 'params': {'klucb': <built-in function klucbBern>}} ...
- Adding policy #9 = {'archtype': <class 'Policies.klUCBPlus.klUCBPlus'>, 'params': {'klucb': <built-in function klucbBern>}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][8]' = {'archtype': <class 'Policies.klUCBPlus.klUCBPlus'>, 'params': {'klucb': <built-in function klucbBern>}} ...
- Adding policy #10 = {'archtype': <class 'Policies.BayesUCB.BayesUCB'>, 'params': {'posterior': <class 'Policies.Posterior.Beta.Beta'>}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][9]' = {'archtype': <class 'Policies.BayesUCB.BayesUCB'>, 'params': {'posterior': <class 'Policies.Posterior.Beta.Beta'>}} ...

- Evaluating policy #1/10: BlackBoxOpt(default_optimizer, default_estimator) ...

- Evaluating policy #2/10: UCB($\alpha=4$) ...

Estimated order by the policy UCB($\alpha=4$) after 10000 steps: [0 1 2] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 100.00% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 88.28% (relative success)...
  ==> Spearman    distance from optimal ordering: 100.00% (relative success)...
  ==> Gestalt     distance from optimal ordering: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 97.07% (relative success)...

- Evaluating policy #3/10: UCB($\alpha=1$) ...

Estimated order by the policy UCB($\alpha=1$) after 10000 steps: [0 1 2] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 100.00% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 88.28% (relative success)...
  ==> Spearman    distance from optimal ordering: 100.00% (relative success)...
  ==> Gestalt     distance from optimal ordering: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 97.07% (relative success)...

- Evaluating policy #4/10: UCB($\alpha=0.5$) ...

Estimated order by the policy UCB($\alpha=0.5$) after 10000 steps: [0 1 2] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 100.00% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 88.28% (relative success)...
  ==> Spearman    distance from optimal ordering: 100.00% (relative success)...
  ==> Gestalt     distance from optimal ordering: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 97.07% (relative success)...

- Evaluating policy #5/10: UCB($\alpha=0.05$) ...

Estimated order by the policy UCB($\alpha=0.05$) after 10000 steps: [0 1 2] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 100.00% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 88.28% (relative success)...
  ==> Spearman    distance from optimal ordering: 100.00% (relative success)...
  ==> Gestalt     distance from optimal ordering: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 97.07% (relative success)...

- Evaluating policy #6/10: DMED+(Bern) ...

- Evaluating policy #7/10: Thompson ...

Estimated order by the policy Thompson after 10000 steps: [1 0 2] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 55.56% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 39.85% (relative success)...
  ==> Spearman    distance from optimal ordering: 33.33% (relative success)...
  ==> Gestalt     distance from optimal ordering: 66.67% (relative success)...
  ==> Mean distance from optimal ordering: 48.85% (relative success)...

- Evaluating policy #8/10: KL-UCB(Bern) ...

Estimated order by the policy KL-UCB(Bern) after 10000 steps: [0 1 2] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 100.00% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 88.28% (relative success)...
  ==> Spearman    distance from optimal ordering: 100.00% (relative success)...
  ==> Gestalt     distance from optimal ordering: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 97.07% (relative success)...

- Evaluating policy #9/10: KL-UCB+(Bern) ...

Estimated order by the policy KL-UCB+(Bern) after 10000 steps: [1 0 2] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 55.56% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 39.85% (relative success)...
  ==> Spearman    distance from optimal ordering: 33.33% (relative success)...
  ==> Gestalt     distance from optimal ordering: 66.67% (relative success)...
  ==> Mean distance from optimal ordering: 48.85% (relative success)...

- Evaluating policy #10/10: BayesUCB ...

Estimated order by the policy BayesUCB after 10000 steps: [0 1 2] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 100.00% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 88.28% (relative success)...
  ==> Spearman    distance from optimal ordering: 100.00% (relative success)...
  ==> Gestalt     distance from optimal ordering: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 97.07% (relative success)...
Giving the final ranks ...

Final ranking for this environment #0 :
- Policy 'UCB($\alpha=0.05$)'	was ranked	1 / 10 for this simulation (last regret = -3.60543).
- Policy 'BlackBoxOpt(default_optimizer, default_estimator)'	was ranked	2 / 10 for this simulation (last regret = 0.0875055).
- Policy 'KL-UCB+(Bern)'	was ranked	3 / 10 for this simulation (last regret = 8.36474).
- Policy 'Thompson'	was ranked	4 / 10 for this simulation (last regret = 10.1208).
- Policy 'UCB($\alpha=0.5$)'	was ranked	5 / 10 for this simulation (last regret = 12.2659).
- Policy 'BayesUCB'	was ranked	6 / 10 for this simulation (last regret = 12.4787).
- Policy 'DMED+(Bern)'	was ranked	7 / 10 for this simulation (last regret = 15.571).
- Policy 'KL-UCB(Bern)'	was ranked	8 / 10 for this simulation (last regret = 16.924).
- Policy 'UCB($\alpha=1$)'	was ranked	9 / 10 for this simulation (last regret = 23.1539).
- Policy 'UCB($\alpha=4$)'	was ranked	10 / 10 for this simulation (last regret = 72.5057).

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 0.5 for 1-player problem... 
 - a Optimal Arm Identification factor H_OI(mu) = 36.67% ...

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 0.5 for 1-player problem... 
 - a Optimal Arm Identification factor H_OI(mu) = 36.67% ...

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 0.5 for 1-player problem... 
 - a Optimal Arm Identification factor H_OI(mu) = 36.67% ...
Done for simulations main.py ...
