Loaded experiments configuration from 'configuration.py' :
configuration['policies'] = [{'params': {}, 'archtype': <class 'Policies.UCB.UCB'>}, {'params': {}, 'archtype': <class 'Policies.UCBV.UCBV'>}, {'params': {'temperature': 0.05}, 'archtype': <class 'Policies.Softmax.Softmax'>}, {'params': {}, 'archtype': <class 'Policies.Thompson.Thompson'>}, {'params': {}, 'archtype': <class 'Policies.klUCB.klUCB'>}, {'params': {}, 'archtype': <class 'Policies.BayesUCB.BayesUCB'>}, {'params': {'alpha': 0.5, 'horizon': 10000}, 'archtype': <class 'Policies.AdBandits.AdBandit'>}, {'params': {'alpha': 0.125, 'horizon': 10000}, 'archtype': <class 'Policies.AdBandits.AdBandit'>}, {'params': {'children': [{'params': {}, 'archtype': <class 'Policies.UCB.UCB'>}, {'params': {}, 'archtype': <class 'Policies.UCBV.UCBV'>}, {'params': {'temperature': 0.05}, 'archtype': <class 'Policies.Softmax.Softmax'>}, {'params': {}, 'archtype': <class 'Policies.Thompson.Thompson'>}, {'params': {}, 'archtype': <class 'Policies.klUCB.klUCB'>}, {'params': {}, 'archtype': <class 'Policies.BayesUCB.BayesUCB'>}, {'params': {'alpha': 0.5, 'horizon': 10000}, 'archtype': <class 'Policies.AdBandits.AdBandit'>}, {'params': {'alpha': 0.125, 'horizon': 10000}, 'archtype': <class 'Policies.AdBandits.AdBandit'>}], 'update_all_children': True, 'learningRate': 0.01, 'decreaseRate': 5000.0}, 'archtype': <class 'Policies.Aggr.Aggr'>}]
plots is already a directory here...
Number of policies in this comparaison: 9
Time horizon: 10000
Number of repetitions: 20
Creating a new MAB problem ...
  Reading arms of this MAB problem from a dictionnary 'configuration' = {'arm_type': <class 'Arms.Exponential.Exponential'>, 'params': [2, 3, 4, 5, 6, 7, 8, 9, 10]} ...
 - with 'arm_type' = <class 'Arms.Exponential.Exponential'>
 - with 'params' = [2, 3, 4, 5, 6, 7, 8, 9, 10]
 - with 'arms' = [Exp(2, 1), Exp(3, 1), Exp(4, 1), Exp(5, 1), Exp(6, 1), Exp(7, 1), Exp(8, 1), Exp(9, 1), Exp(10, 1)]
 - with 'nbArms' = 9
 - with 'maxArm' = 0.43233235838169365
Number of environments to try: 1

Evaluating environment: <MAB{'arms': [Exp(2, 1), Exp(3, 1), Exp(4, 1), Exp(5, 1), Exp(6, 1), Exp(7, 1), Exp(8, 1), Exp(9, 1), Exp(10, 1)], 'nbArms': 9, 'maxArm': 0.43233235838169365}>
- Adding policy #1 = {'params': {}, 'archtype': <class 'Policies.UCB.UCB'>} ...
- Adding policy #2 = {'params': {}, 'archtype': <class 'Policies.UCBV.UCBV'>} ...
- Adding policy #3 = {'params': {'temperature': 0.05}, 'archtype': <class 'Policies.Softmax.Softmax'>} ...
- Adding policy #4 = {'params': {}, 'archtype': <class 'Policies.Thompson.Thompson'>} ...
- Adding policy #5 = {'params': {}, 'archtype': <class 'Policies.klUCB.klUCB'>} ...
- Adding policy #6 = {'params': {}, 'archtype': <class 'Policies.BayesUCB.BayesUCB'>} ...
- Adding policy #7 = {'params': {'alpha': 0.5, 'horizon': 10000}, 'archtype': <class 'Policies.AdBandits.AdBandit'>} ...
- Adding policy #8 = {'params': {'alpha': 0.125, 'horizon': 10000}, 'archtype': <class 'Policies.AdBandits.AdBandit'>} ...
- Adding policy #9 = {'params': {'children': [{'params': {}, 'archtype': <class 'Policies.UCB.UCB'>}, {'params': {}, 'archtype': <class 'Policies.UCBV.UCBV'>}, {'params': {'temperature': 0.05}, 'archtype': <class 'Policies.Softmax.Softmax'>}, {'params': {}, 'archtype': <class 'Policies.Thompson.Thompson'>}, {'params': {}, 'archtype': <class 'Policies.klUCB.klUCB'>}, {'params': {}, 'archtype': <class 'Policies.BayesUCB.BayesUCB'>}, {'params': {'alpha': 0.5, 'horizon': 10000}, 'archtype': <class 'Policies.AdBandits.AdBandit'>}, {'params': {'alpha': 0.125, 'horizon': 10000}, 'archtype': <class 'Policies.AdBandits.AdBandit'>}], 'update_all_children': True, 'learningRate': 0.01, 'decreaseRate': 5000.0}, 'archtype': <class 'Policies.Aggr.Aggr'>} ...
  Creating this child player from a dictionnary 'children[0]' = {'params': {}, 'archtype': <class 'Policies.UCB.UCB'>} ...
  Creating this child player from a dictionnary 'children[1]' = {'params': {}, 'archtype': <class 'Policies.UCBV.UCBV'>} ...
  Creating this child player from a dictionnary 'children[2]' = {'params': {'temperature': 0.05}, 'archtype': <class 'Policies.Softmax.Softmax'>} ...
  Creating this child player from a dictionnary 'children[3]' = {'params': {}, 'archtype': <class 'Policies.Thompson.Thompson'>} ...
  Creating this child player from a dictionnary 'children[4]' = {'params': {}, 'archtype': <class 'Policies.klUCB.klUCB'>} ...
  Creating this child player from a dictionnary 'children[5]' = {'params': {}, 'archtype': <class 'Policies.BayesUCB.BayesUCB'>} ...
  Creating this child player from a dictionnary 'children[6]' = {'params': {'alpha': 0.5, 'horizon': 10000}, 'archtype': <class 'Policies.AdBandits.AdBandit'>} ...
  Creating this child player from a dictionnary 'children[7]' = {'params': {'alpha': 0.125, 'horizon': 10000}, 'archtype': <class 'Policies.AdBandits.AdBandit'>} ...

- Evaluating policy #1/9: UCB ...

- Evaluating policy #2/9: UCBV ...

- Evaluating policy #3/9: Softmax (temperature:0.05) ...

- Evaluating policy #4/9: Thompson ...

- Evaluating policy #5/9: klUCB ...

- Evaluating policy #6/9: BayesUCB ...

- Evaluating policy #7/9: AdBandit (alpha:0.5) ...

- Evaluating policy #8/9: AdBandit (alpha:0.125) ...

- Evaluating policy #9/9: Aggr (nb:8, rate:0.01) ...
Giving the final ranks ...

Final ranking for this environment #0 :
- Policy 'AdBandit (alpha:0.125)'	was ranked	1 / 9 for this simulation (last regret = 69.662).
- Policy 'AdBandit (alpha:0.5)'	was ranked	2 / 9 for this simulation (last regret = 101.169).
- Policy 'klUCB'	was ranked	3 / 9 for this simulation (last regret = 111.187).
- Policy 'Thompson'	was ranked	4 / 9 for this simulation (last regret = 113.429).
- Policy 'Aggr (nb:8, rate:0.01)'	was ranked	5 / 9 for this simulation (last regret = 130.665).
- Policy 'BayesUCB'	was ranked	6 / 9 for this simulation (last regret = 135.225).
- Policy 'Softmax (temperature:0.05)'	was ranked	7 / 9 for this simulation (last regret = 199.468).
- Policy 'UCBV'	was ranked	8 / 9 for this simulation (last regret = 286.814).
- Policy 'UCB'	was ranked	9 / 9 for this simulation (last regret = 429.798).
plots/T10000_N20__9_algos is already a directory here...
 - Plotting the results, and saving the plot to plots/T10000_N20__9_algos/main____env1-1_8604208438918899849.png ...
Saving to plots/T10000_N20__9_algos/main____env1-1_8604208438918899849.png ...
 - Plotting the results, and saving the plot to plots/T10000_N20__9_algos/main_BestArmPulls____env1-1_8604208438918899849.png ...
Saving to plots/T10000_N20__9_algos/main_BestArmPulls____env1-1_8604208438918899849.png ...
