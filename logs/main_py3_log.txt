Loaded experiments configuration from 'configuration.py' :
configuration['policies'] = [{'params': {}, 'archtype': <class 'Policies.Thompson.Thompson'>}, {'params': {}, 'archtype': <class 'Policies.klUCB.klUCB'>}, {'params': {}, 'archtype': <class 'Policies.BayesUCB.BayesUCB'>}, {'params': {'alpha': 0.5, 'horizon': 2000}, 'archtype': <class 'Policies.AdBandits.AdBandit'>}, {'params': {'alpha': 0.125, 'horizon': 2000}, 'archtype': <class 'Policies.AdBandits.AdBandit'>}, {'params': {'verbosity': 0, 'update_all_children': True, 'learningRate': 10, 'decreaseRate': 1000.0, 'children': [{'params': {}, 'archtype': <class 'Policies.Thompson.Thompson'>}, {'params': {}, 'archtype': <class 'Policies.klUCB.klUCB'>}, {'params': {}, 'archtype': <class 'Policies.BayesUCB.BayesUCB'>}, {'params': {'alpha': 0.5, 'horizon': 2000}, 'archtype': <class 'Policies.AdBandits.AdBandit'>}, {'params': {'alpha': 0.125, 'horizon': 2000}, 'archtype': <class 'Policies.AdBandits.AdBandit'>}], 'one_job_by_children': False, 'n_jobs': 1}, 'archtype': <class 'Policies.Aggr.Aggr'>}, {'params': {'verbosity': 0, 'update_all_children': True, 'learningRate': 1, 'decreaseRate': 1000.0, 'children': [{'params': {}, 'archtype': <class 'Policies.Thompson.Thompson'>}, {'params': {}, 'archtype': <class 'Policies.klUCB.klUCB'>}, {'params': {}, 'archtype': <class 'Policies.BayesUCB.BayesUCB'>}, {'params': {'alpha': 0.5, 'horizon': 2000}, 'archtype': <class 'Policies.AdBandits.AdBandit'>}, {'params': {'alpha': 0.125, 'horizon': 2000}, 'archtype': <class 'Policies.AdBandits.AdBandit'>}], 'one_job_by_children': False, 'n_jobs': 1}, 'archtype': <class 'Policies.Aggr.Aggr'>}, {'params': {'verbosity': 0, 'update_all_children': True, 'learningRate': 0.1, 'decreaseRate': 1000.0, 'children': [{'params': {}, 'archtype': <class 'Policies.Thompson.Thompson'>}, {'params': {}, 'archtype': <class 'Policies.klUCB.klUCB'>}, {'params': {}, 'archtype': <class 'Policies.BayesUCB.BayesUCB'>}, {'params': {'alpha': 0.5, 'horizon': 2000}, 'archtype': <class 'Policies.AdBandits.AdBandit'>}, {'params': {'alpha': 0.125, 'horizon': 2000}, 'archtype': <class 'Policies.AdBandits.AdBandit'>}], 'one_job_by_children': False, 'n_jobs': 1}, 'archtype': <class 'Policies.Aggr.Aggr'>}, {'params': {'verbosity': 0, 'update_all_children': True, 'learningRate': 0.01, 'decreaseRate': 1000.0, 'children': [{'params': {}, 'archtype': <class 'Policies.Thompson.Thompson'>}, {'params': {}, 'archtype': <class 'Policies.klUCB.klUCB'>}, {'params': {}, 'archtype': <class 'Policies.BayesUCB.BayesUCB'>}, {'params': {'alpha': 0.5, 'horizon': 2000}, 'archtype': <class 'Policies.AdBandits.AdBandit'>}, {'params': {'alpha': 0.125, 'horizon': 2000}, 'archtype': <class 'Policies.AdBandits.AdBandit'>}], 'one_job_by_children': False, 'n_jobs': 1}, 'archtype': <class 'Policies.Aggr.Aggr'>}, {'params': {'verbosity': 0, 'update_all_children': True, 'learningRate': 0.001, 'decreaseRate': 1000.0, 'children': [{'params': {}, 'archtype': <class 'Policies.Thompson.Thompson'>}, {'params': {}, 'archtype': <class 'Policies.klUCB.klUCB'>}, {'params': {}, 'archtype': <class 'Policies.BayesUCB.BayesUCB'>}, {'params': {'alpha': 0.5, 'horizon': 2000}, 'archtype': <class 'Policies.AdBandits.AdBandit'>}, {'params': {'alpha': 0.125, 'horizon': 2000}, 'archtype': <class 'Policies.AdBandits.AdBandit'>}], 'one_job_by_children': False, 'n_jobs': 1}, 'archtype': <class 'Policies.Aggr.Aggr'>}]
plots is already a directory here...
Number of algorithms to compare: 10
Number of environments to try: 1
Time horizon: 2000
Number of repetitions: 100

Evaluating environment: <MAB{'maxArm': 0.5, 'arms': [B(0.001), B(0.001), B(0.001), B(0.001), B(0.005), B(0.005), B(0.005), B(0.005), B(0.01), B(0.01), B(0.01), B(0.01), B(0.02), B(0.02), B(0.02), B(0.02), B(0.02), B(0.02), B(0.05), B(0.05), B(0.05), B(0.05), B(0.05), B(0.05), B(0.05), B(0.05), B(0.05), B(0.05), B(0.05), B(0.05), B(0.1), B(0.1), B(0.2), B(0.5)], 'nbArms': 34}>
policy = {'params': {}, 'archtype': <class 'Policies.Thompson.Thompson'>}
policy = {'params': {}, 'archtype': <class 'Policies.klUCB.klUCB'>}
policy = {'params': {}, 'archtype': <class 'Policies.BayesUCB.BayesUCB'>}
policy = {'params': {'alpha': 0.5, 'horizon': 2000}, 'archtype': <class 'Policies.AdBandits.AdBandit'>}
policy = {'params': {'alpha': 0.125, 'horizon': 2000}, 'archtype': <class 'Policies.AdBandits.AdBandit'>}
policy = {'params': {'verbosity': 0, 'update_all_children': True, 'learningRate': 10, 'decreaseRate': 1000.0, 'children': [{'params': {}, 'archtype': <class 'Policies.Thompson.Thompson'>}, {'params': {}, 'archtype': <class 'Policies.klUCB.klUCB'>}, {'params': {}, 'archtype': <class 'Policies.BayesUCB.BayesUCB'>}, {'params': {'alpha': 0.5, 'horizon': 2000}, 'archtype': <class 'Policies.AdBandits.AdBandit'>}, {'params': {'alpha': 0.125, 'horizon': 2000}, 'archtype': <class 'Policies.AdBandits.AdBandit'>}], 'one_job_by_children': False, 'n_jobs': 1}, 'archtype': <class 'Policies.Aggr.Aggr'>}
policy = {'params': {'verbosity': 0, 'update_all_children': True, 'learningRate': 1, 'decreaseRate': 1000.0, 'children': [{'params': {}, 'archtype': <class 'Policies.Thompson.Thompson'>}, {'params': {}, 'archtype': <class 'Policies.klUCB.klUCB'>}, {'params': {}, 'archtype': <class 'Policies.BayesUCB.BayesUCB'>}, {'params': {'alpha': 0.5, 'horizon': 2000}, 'archtype': <class 'Policies.AdBandits.AdBandit'>}, {'params': {'alpha': 0.125, 'horizon': 2000}, 'archtype': <class 'Policies.AdBandits.AdBandit'>}], 'one_job_by_children': False, 'n_jobs': 1}, 'archtype': <class 'Policies.Aggr.Aggr'>}
policy = {'params': {'verbosity': 0, 'update_all_children': True, 'learningRate': 0.1, 'decreaseRate': 1000.0, 'children': [{'params': {}, 'archtype': <class 'Policies.Thompson.Thompson'>}, {'params': {}, 'archtype': <class 'Policies.klUCB.klUCB'>}, {'params': {}, 'archtype': <class 'Policies.BayesUCB.BayesUCB'>}, {'params': {'alpha': 0.5, 'horizon': 2000}, 'archtype': <class 'Policies.AdBandits.AdBandit'>}, {'params': {'alpha': 0.125, 'horizon': 2000}, 'archtype': <class 'Policies.AdBandits.AdBandit'>}], 'one_job_by_children': False, 'n_jobs': 1}, 'archtype': <class 'Policies.Aggr.Aggr'>}
policy = {'params': {'verbosity': 0, 'update_all_children': True, 'learningRate': 0.01, 'decreaseRate': 1000.0, 'children': [{'params': {}, 'archtype': <class 'Policies.Thompson.Thompson'>}, {'params': {}, 'archtype': <class 'Policies.klUCB.klUCB'>}, {'params': {}, 'archtype': <class 'Policies.BayesUCB.BayesUCB'>}, {'params': {'alpha': 0.5, 'horizon': 2000}, 'archtype': <class 'Policies.AdBandits.AdBandit'>}, {'params': {'alpha': 0.125, 'horizon': 2000}, 'archtype': <class 'Policies.AdBandits.AdBandit'>}], 'one_job_by_children': False, 'n_jobs': 1}, 'archtype': <class 'Policies.Aggr.Aggr'>}
policy = {'params': {'verbosity': 0, 'update_all_children': True, 'learningRate': 0.001, 'decreaseRate': 1000.0, 'children': [{'params': {}, 'archtype': <class 'Policies.Thompson.Thompson'>}, {'params': {}, 'archtype': <class 'Policies.klUCB.klUCB'>}, {'params': {}, 'archtype': <class 'Policies.BayesUCB.BayesUCB'>}, {'params': {'alpha': 0.5, 'horizon': 2000}, 'archtype': <class 'Policies.AdBandits.AdBandit'>}, {'params': {'alpha': 0.125, 'horizon': 2000}, 'archtype': <class 'Policies.AdBandits.AdBandit'>}], 'one_job_by_children': False, 'n_jobs': 1}, 'archtype': <class 'Policies.Aggr.Aggr'>}

- Evaluating policy #1/10: Thompson ...

- Evaluating policy #2/10: klUCB ...

- Evaluating policy #3/10: BayesUCB ...

- Evaluating policy #4/10: AdBandit (alpha:0.5) ...

- Evaluating policy #5/10: AdBandit (alpha:0.125) ...

- Evaluating policy #6/10: Aggr (nb:5, rate:10) ...

- Evaluating policy #7/10: Aggr (nb:5, rate:1) ...

- Evaluating policy #8/10: Aggr (nb:5, rate:0.1) ...

- Evaluating policy #9/10: Aggr (nb:5, rate:0.01) ...

- Evaluating policy #10/10: Aggr (nb:5, rate:0.001) ...
Giving the final ranks ...

Final ranking for this environment #0 :
- Policy 'AdBandit (alpha:0.125)'	was ranked	1 / 10 for this simulation (last regret = 67.870).
- Policy 'Aggr (nb:5, rate:0.1)'	was ranked	2 / 10 for this simulation (last regret = 86.990).
- Policy 'AdBandit (alpha:0.5)'	was ranked	3 / 10 for this simulation (last regret = 125.370).
- Policy 'Aggr (nb:5, rate:10)'	was ranked	4 / 10 for this simulation (last regret = 139.430).
- Policy 'Aggr (nb:5, rate:1)'	was ranked	5 / 10 for this simulation (last regret = 140.980).
- Policy 'Aggr (nb:5, rate:0.01)'	was ranked	6 / 10 for this simulation (last regret = 160.810).
- Policy 'BayesUCB'	was ranked	7 / 10 for this simulation (last regret = 164.560).
- Policy 'Thompson'	was ranked	8 / 10 for this simulation (last regret = 170.510).
- Policy 'klUCB'	was ranked	9 / 10 for this simulation (last regret = 178.170).
- Policy 'Aggr (nb:5, rate:0.001)'	was ranked	10 / 10 for this simulation (last regret = 185.170).
Plotting the results, and saving the plot to plots/T2000_N100__10_algos/main____env1-1_6926902653944795009.png ...
Using color b for policy number #1/10 and called Thompson...
Using color g for policy number #2/10 and called klUCB...
Using color r for policy number #3/10 and called BayesUCB...
Using color c for policy number #4/10 and called AdBandit (alpha:0.5)...
Using color m for policy number #5/10 and called AdBandit (alpha:0.125)...
Using color y for policy number #6/10 and called Aggr (nb:5, rate:10)...
Using color k for policy number #7/10 and called Aggr (nb:5, rate:1)...
Using color orange for policy number #8/10 and called Aggr (nb:5, rate:0.1)...
Using color purple for policy number #9/10 and called Aggr (nb:5, rate:0.01)...
Using color darkgreen for policy number #10/10 and called Aggr (nb:5, rate:0.001)...
Saving to plots/T2000_N100__10_algos/main____env1-1_6926902653944795009.png ...
