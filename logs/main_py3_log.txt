 - Setting dpi of all figures to 110 ...
 - Setting 'figsize' of all figures to (19.8, 10.8) ...
Info: Using the regular tqdm() decorator ...
Info: numba.jit seems to be available.
Info: numba.jit seems to be available.
Loaded experiments configuration from 'configuration.py' :
configuration['policies'] = [{'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 1}}, {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.5}}, {'archtype': <class 'Policies.DMED.DMED'>, 'params': {'genuine': True}}, {'archtype': <class 'Policies.Thompson.Thompson'>, 'params': {}}, {'archtype': <class 'Policies.klUCB.klUCB'>, 'params': {'klucb': <built-in function klucbBern>}}, {'archtype': <class 'Policies.klUCBPlus.klUCBPlus'>, 'params': {'klucb': <built-in function klucbBern>}}, {'archtype': <class 'Policies.BayesUCB.BayesUCB'>, 'params': {}}, {'archtype': <class 'Policies.ApproximatedFHGittins.ApproximatedFHGittins'>, 'params': {'horizon': 11000.0, 'alpha': 1}}, {'archtype': <class 'Policies.ApproximatedFHGittins.ApproximatedFHGittins'>, 'params': {'horizon': 11000.0, 'alpha': 0.5}}]
====> TURNING DEBUG MODE ON <=====
plots/ is already a directory here...
Number of policies in this comparison: 9
Time horizon: 10000
Number of repetitions: 1000
Sampling rate for saving, delta_t_save: 1
Sampling rate for plotting, delta_t_plot: 1
Number of jobs for parallelization: 4
Creating a new MarkovianMAB problem ...
 - Using these transition matrices: [array([[ 0.7,  0.5],
       [ 0.3,  0.5]]), array([[ 0.2,  0.8],
       [ 0.6,  0.4]])]
 - Using these transition dictionaries: [{(0, 1): 0.3, (1, 0): 0.5, (0, 0): 0.7, (1, 1): 0.5}, {(0, 1): 0.8, (1, 0): 0.6, (0, 0): 0.2, (1, 1): 0.4}]
 - For these Markov chains: [Chain([((0, 1), 0.3), ((1, 0), 0.5), ((0, 0), 0.7), ((1, 1), 0.5)]), Chain([((0, 1), 0.8), ((1, 0), 0.6), ((0, 0), 0.2), ((1, 1), 0.4)])]
 - Rested: True
 - with 'nbArms' = 2
 - and states: [array([0, 1]), array([0, 1])]
 - and steady state distributions: [array([ 0.625,  0.375]), array([ 0.42857143,  0.57142857])]
 - so it gives arms of means: [ 0.375       0.57142857]
 - so arms asymptotically equivalent to: [B(0.375), B(0.571)]
 - represented as: Rested Markovian rewards, $[[[0.7, 0.5], [0.3, 0.5]] : B(0.375), [[0.2, 0.8], [0.6, 0.4]] : B(0.571)^*]$
 - with 'maxArm' = 0.571428571429
 - with 'minArm' = 0.375
DONE for creating this MarkovianMAB problem...
Number of environments to try: 1

Evaluating environment: MarkovianMAB(nbArms: 2, chains: [array([[ 0.7,  0.5],
       [ 0.3,  0.5]]), array([[ 0.2,  0.8],
       [ 0.6,  0.4]])], arms: [B(0.375), B(0.571)])
- Adding policy #1 = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 1}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][0]' = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 1}} ...
- Adding policy #2 = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.5}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][1]' = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.5}} ...
- Adding policy #3 = {'archtype': <class 'Policies.DMED.DMED'>, 'params': {'genuine': True}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][2]' = {'archtype': <class 'Policies.DMED.DMED'>, 'params': {'genuine': True}} ...
- Adding policy #4 = {'archtype': <class 'Policies.Thompson.Thompson'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][3]' = {'archtype': <class 'Policies.Thompson.Thompson'>, 'params': {}} ...
- Adding policy #5 = {'archtype': <class 'Policies.klUCB.klUCB'>, 'params': {'klucb': <built-in function klucbBern>}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][4]' = {'archtype': <class 'Policies.klUCB.klUCB'>, 'params': {'klucb': <built-in function klucbBern>}} ...
- Adding policy #6 = {'archtype': <class 'Policies.klUCBPlus.klUCBPlus'>, 'params': {'klucb': <built-in function klucbBern>}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][5]' = {'archtype': <class 'Policies.klUCBPlus.klUCBPlus'>, 'params': {'klucb': <built-in function klucbBern>}} ...
- Adding policy #7 = {'archtype': <class 'Policies.BayesUCB.BayesUCB'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][6]' = {'archtype': <class 'Policies.BayesUCB.BayesUCB'>, 'params': {}} ...
- Adding policy #8 = {'archtype': <class 'Policies.ApproximatedFHGittins.ApproximatedFHGittins'>, 'params': {'horizon': 11000.0, 'alpha': 1}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][7]' = {'archtype': <class 'Policies.ApproximatedFHGittins.ApproximatedFHGittins'>, 'params': {'horizon': 11000.0, 'alpha': 1}} ...
- Adding policy #9 = {'archtype': <class 'Policies.ApproximatedFHGittins.ApproximatedFHGittins'>, 'params': {'horizon': 11000.0, 'alpha': 0.5}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][8]' = {'archtype': <class 'Policies.ApproximatedFHGittins.ApproximatedFHGittins'>, 'params': {'horizon': 11000.0, 'alpha': 0.5}} ...

- Evaluating policy #1/9: UCB($\alpha=1$) ...

Estimated order by the policy UCB($\alpha=1$) after 10000 steps: [0 1] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 100.00% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 0.00% (relative success)...
  ==> Spearman    distance from optimal ordering: 0.00% (relative success)...
  ==> Gestalt     distance from optimal ordering: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 50.00% (relative success)...

- Evaluating policy #2/9: UCB($\alpha=0.5$) ...

Estimated order by the policy UCB($\alpha=0.5$) after 10000 steps: [0 1] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 100.00% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 0.00% (relative success)...
  ==> Spearman    distance from optimal ordering: 0.00% (relative success)...
  ==> Gestalt     distance from optimal ordering: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 50.00% (relative success)...

- Evaluating policy #3/9: DMED+(Bern) ...

- Evaluating policy #4/9: Thompson ...

Estimated order by the policy Thompson after 10000 steps: [0 1] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 100.00% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 0.00% (relative success)...
  ==> Spearman    distance from optimal ordering: 0.00% (relative success)...
  ==> Gestalt     distance from optimal ordering: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 50.00% (relative success)...

- Evaluating policy #5/9: KL-UCB(Bern) ...

Estimated order by the policy KL-UCB(Bern) after 10000 steps: [0 1] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 100.00% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 0.00% (relative success)...
  ==> Spearman    distance from optimal ordering: 0.00% (relative success)...
  ==> Gestalt     distance from optimal ordering: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 50.00% (relative success)...

- Evaluating policy #6/9: KL-UCB+(Bern) ...

Estimated order by the policy KL-UCB+(Bern) after 10000 steps: [0 1] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 100.00% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 0.00% (relative success)...
  ==> Spearman    distance from optimal ordering: 0.00% (relative success)...
  ==> Gestalt     distance from optimal ordering: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 50.00% (relative success)...

- Evaluating policy #7/9: BayesUCB ...

Estimated order by the policy BayesUCB after 10000 steps: [0 1] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 100.00% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 0.00% (relative success)...
  ==> Spearman    distance from optimal ordering: 0.00% (relative success)...
  ==> Gestalt     distance from optimal ordering: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 50.00% (relative success)...

- Evaluating policy #8/9: ApproximatedFHGittins($\alpha=1$) ...

Estimated order by the policy ApproximatedFHGittins($\alpha=1$) after 10000 steps: [0 1] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 100.00% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 0.00% (relative success)...
  ==> Spearman    distance from optimal ordering: 0.00% (relative success)...
  ==> Gestalt     distance from optimal ordering: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 50.00% (relative success)...

- Evaluating policy #9/9: ApproximatedFHGittins($\alpha=0.5$) ...

Estimated order by the policy ApproximatedFHGittins($\alpha=0.5$) after 10000 steps: [0 1] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 100.00% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 0.00% (relative success)...
  ==> Spearman    distance from optimal ordering: 0.00% (relative success)...
  ==> Gestalt     distance from optimal ordering: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 50.00% (relative success)...
Giving the final ranks ...

Final ranking for this environment #0 :
- Policy 'ApproximatedFHGittins($\alpha=0.5$)'	was ranked	1 / 9 for this simulation (last regret = 5.55629).
- Policy 'ApproximatedFHGittins($\alpha=1$)'	was ranked	2 / 9 for this simulation (last regret = 8.90929).
- Policy 'UCB($\alpha=0.5$)'	was ranked	3 / 9 for this simulation (last regret = 9.60429).
- Policy 'Thompson'	was ranked	4 / 9 for this simulation (last regret = 11.3983).
- Policy 'KL-UCB+(Bern)'	was ranked	5 / 9 for this simulation (last regret = 11.9663).
- Policy 'DMED+(Bern)'	was ranked	6 / 9 for this simulation (last regret = 12.7743).
- Policy 'BayesUCB'	was ranked	7 / 9 for this simulation (last regret = 13.7613).
- Policy 'UCB($\alpha=1$)'	was ranked	8 / 9 for this simulation (last regret = 16.4363).
- Policy 'KL-UCB(Bern)'	was ranked	9 / 9 for this simulation (last regret = 18.7793).

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 2.52 for 1-player problem... 
 - a Optimal Arm Identification factor H_OI(mu) = 40.18% ...

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 2.52 for 1-player problem... 
 - a Optimal Arm Identification factor H_OI(mu) = 40.18% ...

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 2.52 for 1-player problem... 
 - a Optimal Arm Identification factor H_OI(mu) = 40.18% ...
Done for simulations main.py ...
