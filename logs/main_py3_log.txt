Loaded experiments configuration from 'configuration.py' :
configuration['policies'] = [{'params': {'decreaseRate': 'auto', 'unbiased': False, 'update_all_children': False, 'learningRate': 0.01, 'children': [{'params': {}, 'archtype': <class 'Policies.Softmax.SoftmaxDecreasing'>}, {'params': {}, 'archtype': <class 'Policies.Thompson.Thompson'>}, {'params': {}, 'archtype': <class 'Policies.klUCB.klUCB'>}, {'params': {}, 'archtype': <class 'Policies.klUCBPlus.klUCBPlus'>}, {'params': {}, 'archtype': <class 'Policies.BayesUCB.BayesUCB'>}, {'params': {'horizon': 10000, 'alpha': 0.5}, 'archtype': <class 'Policies.AdBandits.AdBandits'>}, {'params': {'horizon': 10000, 'alpha': 0.125}, 'archtype': <class 'Policies.AdBandits.AdBandits'>}, {'params': {'horizon': 10000, 'alpha': 0.01}, 'archtype': <class 'Policies.AdBandits.AdBandits'>}], 'update_like_exp4': True}, 'archtype': <class 'Policies.Aggr.Aggr'>}, {'params': {'decreaseRate': 'auto', 'unbiased': False, 'update_all_children': False, 'learningRate': 0.01, 'children': [{'params': {}, 'archtype': <class 'Policies.Softmax.SoftmaxDecreasing'>}, {'params': {}, 'archtype': <class 'Policies.Thompson.Thompson'>}, {'params': {}, 'archtype': <class 'Policies.klUCB.klUCB'>}, {'params': {}, 'archtype': <class 'Policies.klUCBPlus.klUCBPlus'>}, {'params': {}, 'archtype': <class 'Policies.BayesUCB.BayesUCB'>}, {'params': {'horizon': 10000, 'alpha': 0.5}, 'archtype': <class 'Policies.AdBandits.AdBandits'>}, {'params': {'horizon': 10000, 'alpha': 0.125}, 'archtype': <class 'Policies.AdBandits.AdBandits'>}, {'params': {'horizon': 10000, 'alpha': 0.01}, 'archtype': <class 'Policies.AdBandits.AdBandits'>}], 'update_like_exp4': False}, 'archtype': <class 'Policies.Aggr.Aggr'>}, {'params': {}, 'archtype': <class 'Policies.Softmax.SoftmaxDecreasing'>}, {'params': {}, 'archtype': <class 'Policies.Thompson.Thompson'>}, {'params': {}, 'archtype': <class 'Policies.klUCB.klUCB'>}, {'params': {}, 'archtype': <class 'Policies.klUCBPlus.klUCBPlus'>}, {'params': {}, 'archtype': <class 'Policies.BayesUCB.BayesUCB'>}, {'params': {'horizon': 10000, 'alpha': 0.5}, 'archtype': <class 'Policies.AdBandits.AdBandits'>}, {'params': {'horizon': 10000, 'alpha': 0.125}, 'archtype': <class 'Policies.AdBandits.AdBandits'>}, {'params': {'horizon': 10000, 'alpha': 0.01}, 'archtype': <class 'Policies.AdBandits.AdBandits'>}]
plots/ is already a directory here...
Number of policies in this comparaison: 10
Time horizon: 10000
Number of repetitions: 20
Sampling rate DELTA_T_SAVE: 1
Creating a new MAB problem ...
  Reading arms of this MAB problem from a dictionnary 'configuration' = {'params': [0.01, 0.02, 0.3, 0.4, 0.5, 0.6, 0.78, 0.8, 0.82], 'arm_type': <class 'Arms.Bernoulli.Bernoulli'>} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = [0.01, 0.02, 0.3, 0.4, 0.5, 0.6, 0.78, 0.8, 0.82]
 - with 'arms' = [B(0.01), B(0.02), B(0.3), B(0.4), B(0.5), B(0.6), B(0.78), B(0.8), B(0.82)]
 - with 'nbArms' = 9
 - with 'maxArm' = 0.82
Number of environments to try: 1

Evaluating environment: <MAB{'maxArm': 0.81999999999999995, 'nbArms': 9, 'arms': [B(0.01), B(0.02), B(0.3), B(0.4), B(0.5), B(0.6), B(0.78), B(0.8), B(0.82)]}>
- Adding policy #1 = {'params': {'decreaseRate': 'auto', 'unbiased': False, 'update_all_children': False, 'learningRate': 0.01, 'children': [{'params': {}, 'archtype': <class 'Policies.Softmax.SoftmaxDecreasing'>}, {'params': {}, 'archtype': <class 'Policies.Thompson.Thompson'>}, {'params': {}, 'archtype': <class 'Policies.klUCB.klUCB'>}, {'params': {}, 'archtype': <class 'Policies.klUCBPlus.klUCBPlus'>}, {'params': {}, 'archtype': <class 'Policies.BayesUCB.BayesUCB'>}, {'params': {'horizon': 10000, 'alpha': 0.5}, 'archtype': <class 'Policies.AdBandits.AdBandits'>}, {'params': {'horizon': 10000, 'alpha': 0.125}, 'archtype': <class 'Policies.AdBandits.AdBandits'>}, {'params': {'horizon': 10000, 'alpha': 0.01}, 'archtype': <class 'Policies.AdBandits.AdBandits'>}], 'update_like_exp4': True}, 'archtype': <class 'Policies.Aggr.Aggr'>} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][0]' = {'params': {'decreaseRate': 'auto', 'unbiased': False, 'update_all_children': False, 'learningRate': 0.01, 'children': [{'params': {}, 'archtype': <class 'Policies.Softmax.SoftmaxDecreasing'>}, {'params': {}, 'archtype': <class 'Policies.Thompson.Thompson'>}, {'params': {}, 'archtype': <class 'Policies.klUCB.klUCB'>}, {'params': {}, 'archtype': <class 'Policies.klUCBPlus.klUCBPlus'>}, {'params': {}, 'archtype': <class 'Policies.BayesUCB.BayesUCB'>}, {'params': {'horizon': 10000, 'alpha': 0.5}, 'archtype': <class 'Policies.AdBandits.AdBandits'>}, {'params': {'horizon': 10000, 'alpha': 0.125}, 'archtype': <class 'Policies.AdBandits.AdBandits'>}, {'params': {'horizon': 10000, 'alpha': 0.01}, 'archtype': <class 'Policies.AdBandits.AdBandits'>}], 'update_like_exp4': True}, 'archtype': <class 'Policies.Aggr.Aggr'>} ...
  Creating this child player from a dictionnary 'children[0]' = {'params': {}, 'archtype': <class 'Policies.Softmax.SoftmaxDecreasing'>} ...
  Creating this child player from a dictionnary 'children[1]' = {'params': {}, 'archtype': <class 'Policies.Thompson.Thompson'>} ...
  Creating this child player from a dictionnary 'children[2]' = {'params': {}, 'archtype': <class 'Policies.klUCB.klUCB'>} ...
  Creating this child player from a dictionnary 'children[3]' = {'params': {}, 'archtype': <class 'Policies.klUCBPlus.klUCBPlus'>} ...
  Creating this child player from a dictionnary 'children[4]' = {'params': {}, 'archtype': <class 'Policies.BayesUCB.BayesUCB'>} ...
  Creating this child player from a dictionnary 'children[5]' = {'params': {'horizon': 10000, 'alpha': 0.5}, 'archtype': <class 'Policies.AdBandits.AdBandits'>} ...
  Creating this child player from a dictionnary 'children[6]' = {'params': {'horizon': 10000, 'alpha': 0.125}, 'archtype': <class 'Policies.AdBandits.AdBandits'>} ...
  Creating this child player from a dictionnary 'children[7]' = {'params': {'horizon': 10000, 'alpha': 0.01}, 'archtype': <class 'Policies.AdBandits.AdBandits'>} ...
- Adding policy #2 = {'params': {'decreaseRate': 'auto', 'unbiased': False, 'update_all_children': False, 'learningRate': 0.01, 'children': [{'params': {}, 'archtype': <class 'Policies.Softmax.SoftmaxDecreasing'>}, {'params': {}, 'archtype': <class 'Policies.Thompson.Thompson'>}, {'params': {}, 'archtype': <class 'Policies.klUCB.klUCB'>}, {'params': {}, 'archtype': <class 'Policies.klUCBPlus.klUCBPlus'>}, {'params': {}, 'archtype': <class 'Policies.BayesUCB.BayesUCB'>}, {'params': {'horizon': 10000, 'alpha': 0.5}, 'archtype': <class 'Policies.AdBandits.AdBandits'>}, {'params': {'horizon': 10000, 'alpha': 0.125}, 'archtype': <class 'Policies.AdBandits.AdBandits'>}, {'params': {'horizon': 10000, 'alpha': 0.01}, 'archtype': <class 'Policies.AdBandits.AdBandits'>}], 'update_like_exp4': False}, 'archtype': <class 'Policies.Aggr.Aggr'>} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][1]' = {'params': {'decreaseRate': 'auto', 'unbiased': False, 'update_all_children': False, 'learningRate': 0.01, 'children': [{'params': {}, 'archtype': <class 'Policies.Softmax.SoftmaxDecreasing'>}, {'params': {}, 'archtype': <class 'Policies.Thompson.Thompson'>}, {'params': {}, 'archtype': <class 'Policies.klUCB.klUCB'>}, {'params': {}, 'archtype': <class 'Policies.klUCBPlus.klUCBPlus'>}, {'params': {}, 'archtype': <class 'Policies.BayesUCB.BayesUCB'>}, {'params': {'horizon': 10000, 'alpha': 0.5}, 'archtype': <class 'Policies.AdBandits.AdBandits'>}, {'params': {'horizon': 10000, 'alpha': 0.125}, 'archtype': <class 'Policies.AdBandits.AdBandits'>}, {'params': {'horizon': 10000, 'alpha': 0.01}, 'archtype': <class 'Policies.AdBandits.AdBandits'>}], 'update_like_exp4': False}, 'archtype': <class 'Policies.Aggr.Aggr'>} ...
  Creating this child player from a dictionnary 'children[0]' = {'params': {}, 'archtype': <class 'Policies.Softmax.SoftmaxDecreasing'>} ...
  Creating this child player from a dictionnary 'children[1]' = {'params': {}, 'archtype': <class 'Policies.Thompson.Thompson'>} ...
  Creating this child player from a dictionnary 'children[2]' = {'params': {}, 'archtype': <class 'Policies.klUCB.klUCB'>} ...
  Creating this child player from a dictionnary 'children[3]' = {'params': {}, 'archtype': <class 'Policies.klUCBPlus.klUCBPlus'>} ...
  Creating this child player from a dictionnary 'children[4]' = {'params': {}, 'archtype': <class 'Policies.BayesUCB.BayesUCB'>} ...
  Creating this child player from a dictionnary 'children[5]' = {'params': {'horizon': 10000, 'alpha': 0.5}, 'archtype': <class 'Policies.AdBandits.AdBandits'>} ...
  Creating this child player from a dictionnary 'children[6]' = {'params': {'horizon': 10000, 'alpha': 0.125}, 'archtype': <class 'Policies.AdBandits.AdBandits'>} ...
  Creating this child player from a dictionnary 'children[7]' = {'params': {'horizon': 10000, 'alpha': 0.01}, 'archtype': <class 'Policies.AdBandits.AdBandits'>} ...
- Adding policy #3 = {'params': {}, 'archtype': <class 'Policies.Softmax.SoftmaxDecreasing'>} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][2]' = {'params': {}, 'archtype': <class 'Policies.Softmax.SoftmaxDecreasing'>} ...
- Adding policy #4 = {'params': {}, 'archtype': <class 'Policies.Thompson.Thompson'>} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][3]' = {'params': {}, 'archtype': <class 'Policies.Thompson.Thompson'>} ...
- Adding policy #5 = {'params': {}, 'archtype': <class 'Policies.klUCB.klUCB'>} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][4]' = {'params': {}, 'archtype': <class 'Policies.klUCB.klUCB'>} ...
- Adding policy #6 = {'params': {}, 'archtype': <class 'Policies.klUCBPlus.klUCBPlus'>} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][5]' = {'params': {}, 'archtype': <class 'Policies.klUCBPlus.klUCBPlus'>} ...
- Adding policy #7 = {'params': {}, 'archtype': <class 'Policies.BayesUCB.BayesUCB'>} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][6]' = {'params': {}, 'archtype': <class 'Policies.BayesUCB.BayesUCB'>} ...
- Adding policy #8 = {'params': {'horizon': 10000, 'alpha': 0.5}, 'archtype': <class 'Policies.AdBandits.AdBandits'>} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][7]' = {'params': {'horizon': 10000, 'alpha': 0.5}, 'archtype': <class 'Policies.AdBandits.AdBandits'>} ...
- Adding policy #9 = {'params': {'horizon': 10000, 'alpha': 0.125}, 'archtype': <class 'Policies.AdBandits.AdBandits'>} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][8]' = {'params': {'horizon': 10000, 'alpha': 0.125}, 'archtype': <class 'Policies.AdBandits.AdBandits'>} ...
- Adding policy #10 = {'params': {'horizon': 10000, 'alpha': 0.01}, 'archtype': <class 'Policies.AdBandits.AdBandits'>} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][9]' = {'params': {'horizon': 10000, 'alpha': 0.01}, 'archtype': <class 'Policies.AdBandits.AdBandits'>} ...

===> Pre-computing the rewards ... Of shape (9, 10000) ...
    In order for all simulated algorithms to face the same random rewards (robust comparaison of A1,..,An vs Aggr(A1,..,An)) ...


- Evaluating policy #1/10: Aggr($N=8$, Exp4) ...

- Evaluating policy #2/10: Aggr($N=8$) ...

- Evaluating policy #3/10: Softmax(decreasing) ...

- Evaluating policy #4/10: Thompson ...

- Evaluating policy #5/10: klUCB ...

- Evaluating policy #6/10: klUCBPlus ...

- Evaluating policy #7/10: BayesUCB ...

- Evaluating policy #8/10: AdBandits($\alpha=0.5$, $T=10000$) ...

- Evaluating policy #9/10: AdBandits($\alpha=0.125$, $T=10000$) ...

- Evaluating policy #10/10: AdBandits($\alpha=0.01$, $T=10000$) ...
Giving the final ranks ...

Final ranking for this environment #0 :
- Policy 'klUCBPlus'	was ranked	1 / 10 for this simulation (last regret = 77.67).
- Policy 'AdBandits($\alpha=0.5$, $T=10000$)'	was ranked	2 / 10 for this simulation (last regret = 83.32).
- Policy 'BayesUCB'	was ranked	3 / 10 for this simulation (last regret = 86.12).
- Policy 'AdBandits($\alpha=0.01$, $T=10000$)'	was ranked	4 / 10 for this simulation (last regret = 89.22).
- Policy 'Thompson'	was ranked	5 / 10 for this simulation (last regret = 91.32).
- Policy 'Aggr($N=8$, Exp4)'	was ranked	6 / 10 for this simulation (last regret = 93.02).
- Policy 'AdBandits($\alpha=0.125$, $T=10000$)'	was ranked	7 / 10 for this simulation (last regret = 95.22).
- Policy 'klUCB'	was ranked	8 / 10 for this simulation (last regret = 98.52).
- Policy 'Softmax(decreasing)'	was ranked	9 / 10 for this simulation (last regret = 118.17).
- Policy 'Aggr($N=8$)'	was ranked	10 / 10 for this simulation (last regret = 144.92).
plots/T10000_N20__10_algos is already a directory here...
 - Plotting the cumulative rewards, and saving the plot to plots/T10000_N20__10_algos/main____env1-1_9145436788485254183.png ...
Saving to plots/T10000_N20__10_algos/main____env1-1_9145436788485254183.png ...
 - Plotting the mean rewards, and saving the plot to plots/T10000_N20__10_algos/main_MeanRewards____env1-1_9145436788485254183.png ...
Saving to plots/T10000_N20__10_algos/main_MeanRewards____env1-1_9145436788485254183.png ...
 - Plotting the mean rewards, and saving the plot to plots/T10000_N20__10_algos/main_Normalized_MeanRewards____env1-1_9145436788485254183.png ...
Saving to plots/T10000_N20__10_algos/main_Normalized_MeanRewards____env1-1_9145436788485254183.png ...
 - Plotting the results, and saving the plot to plots/T10000_N20__10_algos/main_BestArmPulls_Normalized_MeanRewards____env1-1_9145436788485254183.png ...
Saving to plots/T10000_N20__10_algos/main_BestArmPulls_Normalized_MeanRewards____env1-1_9145436788485254183.png ...
