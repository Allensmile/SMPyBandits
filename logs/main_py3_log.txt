Loaded experiments configuration from 'configuration.py' :
configuration['policies'] = [{'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 4}}, {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 3}}, {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 2}}, {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 1}}, {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.5}}, {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.25}}, {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.1}}, {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.05}}, {'archtype': <class 'Policies.MOSS.MOSS'>, 'params': {}}, {'archtype': <class 'Policies.Thompson.Thompson'>, 'params': {}}, {'archtype': <class 'Policies.klUCB.klUCB'>, 'params': {}}, {'archtype': <class 'Policies.klUCBlog10.klUCBlog10'>, 'params': {}}, {'archtype': <class 'Policies.klUCBPlus.klUCBPlus'>, 'params': {}}, {'archtype': <class 'Policies.BayesUCB.BayesUCB'>, 'params': {}}]
plots/ is already a directory here...
Number of policies in this comparaison: 14
Time horizon: 20000
Number of repetitions: 50
Sampling rate DELTA_T_SAVE: 1
Creating a new MAB problem ...
  Reading arms of this MAB problem from a dictionnary 'configuration' = {'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': [0.01, 0.02, 0.3, 0.4, 0.5, 0.6, 0.78, 0.8, 0.82]} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = [0.01, 0.02, 0.3, 0.4, 0.5, 0.6, 0.78, 0.8, 0.82]
 - with 'arms' = [B(0.01), B(0.02), B(0.3), B(0.4), B(0.5), B(0.6), B(0.78), B(0.8), B(0.82)]
 - with 'nbArms' = 9
 - with 'maxArm' = 0.82
 - with 'minArm' = 0.01
Number of environments to try: 1

Evaluating environment: <MAB{'arms': [B(0.01), B(0.02), B(0.3), B(0.4), B(0.5), B(0.6), B(0.78), B(0.8), B(0.82)], 'maxArm': 0.81999999999999995, 'minArm': 0.01, 'nbArms': 9}>
- Adding policy #1 = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 4}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][0]' = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 4}} ...
- Adding policy #2 = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 3}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][1]' = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 3}} ...
- Adding policy #3 = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 2}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][2]' = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 2}} ...
- Adding policy #4 = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 1}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][3]' = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 1}} ...
- Adding policy #5 = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.5}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][4]' = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.5}} ...
- Adding policy #6 = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.25}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][5]' = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.25}} ...
- Adding policy #7 = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.1}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][6]' = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.1}} ...
- Adding policy #8 = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.05}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][7]' = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.05}} ...
- Adding policy #9 = {'archtype': <class 'Policies.MOSS.MOSS'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][8]' = {'archtype': <class 'Policies.MOSS.MOSS'>, 'params': {}} ...
- Adding policy #10 = {'archtype': <class 'Policies.Thompson.Thompson'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][9]' = {'archtype': <class 'Policies.Thompson.Thompson'>, 'params': {}} ...
- Adding policy #11 = {'archtype': <class 'Policies.klUCB.klUCB'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][10]' = {'archtype': <class 'Policies.klUCB.klUCB'>, 'params': {}} ...
- Adding policy #12 = {'archtype': <class 'Policies.klUCBlog10.klUCBlog10'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][11]' = {'archtype': <class 'Policies.klUCBlog10.klUCBlog10'>, 'params': {}} ...
- Adding policy #13 = {'archtype': <class 'Policies.klUCBPlus.klUCBPlus'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][12]' = {'archtype': <class 'Policies.klUCBPlus.klUCBPlus'>, 'params': {}} ...
- Adding policy #14 = {'archtype': <class 'Policies.BayesUCB.BayesUCB'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][13]' = {'archtype': <class 'Policies.BayesUCB.BayesUCB'>, 'params': {}} ...

===> Pre-computing the rewards ... Of shape (9, 20000) ...
    In order for all simulated algorithms to face the same random rewards (robust comparaison of A1,..,An vs Aggr(A1,..,An)) ...


- Evaluating policy #1/14: UCB($\alpha=4$) ...

- Evaluating policy #2/14: UCB($\alpha=3$) ...

- Evaluating policy #3/14: UCB($\alpha=2$) ...

- Evaluating policy #4/14: UCB($\alpha=1$) ...

- Evaluating policy #5/14: UCB($\alpha=0.5$) ...

- Evaluating policy #6/14: UCB($\alpha=0.25$) ...

- Evaluating policy #7/14: UCB($\alpha=0.1$) ...

- Evaluating policy #8/14: UCB($\alpha=0.05$) ...

- Evaluating policy #9/14: MOSS ...

- Evaluating policy #10/14: Thompson ...

- Evaluating policy #11/14: klUCB ...

- Evaluating policy #12/14: klUCBlog10 ...

- Evaluating policy #13/14: klUCBPlus ...

- Evaluating policy #14/14: BayesUCB ...
Saving the 'evaluation' objet to <_io.BufferedWriter name='plots/T20000_N50__14_algos/main____env1-1_735249527828791358.pickle'> ...
Giving the final ranks ...

Final ranking for this environment #0 :
- Policy 'klUCBlog10'	was ranked	1 / 14 for this simulation (last regret = 82.26).
- Policy 'Thompson'	was ranked	2 / 14 for this simulation (last regret = 92.54).
- Policy 'UCB($\alpha=0.25$)'	was ranked	3 / 14 for this simulation (last regret = 103.22).
- Policy 'UCB($\alpha=0.5$)'	was ranked	4 / 14 for this simulation (last regret = 113.08).
- Policy 'klUCBPlus'	was ranked	5 / 14 for this simulation (last regret = 115.22).
- Policy 'BayesUCB'	was ranked	6 / 14 for this simulation (last regret = 118.74).
- Policy 'UCB($\alpha=0.1$)'	was ranked	7 / 14 for this simulation (last regret = 121.68).
- Policy 'MOSS'	was ranked	8 / 14 for this simulation (last regret = 132.32).
- Policy 'UCB($\alpha=0.05$)'	was ranked	9 / 14 for this simulation (last regret = 139.54).
- Policy 'klUCB'	was ranked	10 / 14 for this simulation (last regret = 142.72).
- Policy 'UCB($\alpha=1$)'	was ranked	11 / 14 for this simulation (last regret = 194.18).
- Policy 'UCB($\alpha=2$)'	was ranked	12 / 14 for this simulation (last regret = 306.76).
- Policy 'UCB($\alpha=3$)'	was ranked	13 / 14 for this simulation (last regret = 377.58).
- Policy 'UCB($\alpha=4$)'	was ranked	14 / 14 for this simulation (last regret = 455.22).
 - Plotting the cumulative rewards, and saving the plot to plots/T20000_N50__14_algos/main____env1-1_735249527828791358.png ...
Saving to plots/T20000_N50__14_algos/main____env1-1_735249527828791358.png ...
