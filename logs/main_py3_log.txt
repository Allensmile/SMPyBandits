Loaded experiments configuration from 'configuration.py' :
configuration['policies'] = [{'params': {'children': [<Policies.TakeFixedArm.TakeFixedArm object at 0x7ffae63105f8>, <Policies.TakeFixedArm.TakeFixedArm object at 0x7ffaf8fe5cc0>, <Policies.TakeFixedArm.TakeFixedArm object at 0x7ffaf8fe5c88>, <Policies.TakeFixedArm.TakeFixedArm object at 0x7ffad526e9e8>, <Policies.TakeFixedArm.TakeFixedArm object at 0x7ffad526ea20>, <Policies.TakeFixedArm.TakeFixedArm object at 0x7ffad5283ef0>, <Policies.TakeFixedArm.TakeFixedArm object at 0x7ffad50e7748>, <Policies.TakeFixedArm.TakeFixedArm object at 0x7ffad50e76d8>, <Policies.TakeFixedArm.TakeFixedArm object at 0x7ffad50e7780>], 'update_all_children': False, 'update_like_exp4': False, 'decreaseRate': 'auto', 'unbiased': False, 'learningRate': 0.01}, 'archtype': <class 'Policies.Aggr.Aggr'>}, <Policies.TakeFixedArm.TakeFixedArm object at 0x7ffae63105f8>, <Policies.TakeFixedArm.TakeFixedArm object at 0x7ffaf8fe5cc0>, <Policies.TakeFixedArm.TakeFixedArm object at 0x7ffaf8fe5c88>, <Policies.TakeFixedArm.TakeFixedArm object at 0x7ffad526e9e8>, <Policies.TakeFixedArm.TakeFixedArm object at 0x7ffad526ea20>, <Policies.TakeFixedArm.TakeFixedArm object at 0x7ffad5283ef0>, <Policies.TakeFixedArm.TakeFixedArm object at 0x7ffad50e7748>, <Policies.TakeFixedArm.TakeFixedArm object at 0x7ffad50e76d8>, <Policies.TakeFixedArm.TakeFixedArm object at 0x7ffad50e7780>]
plots/ is already a directory here...
Number of policies in this comparaison: 10
Time horizon: 30000
Number of repetitions: 50
Sampling rate DELTA_T_SAVE: 1
Creating a new MAB problem ...
  Reading arms of this MAB problem from a dictionnary 'configuration' = {'params': [0.01, 0.02, 0.3, 0.4, 0.5, 0.6, 0.78, 0.8, 0.82], 'arm_type': <class 'Arms.Bernoulli.Bernoulli'>} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = [0.01, 0.02, 0.3, 0.4, 0.5, 0.6, 0.78, 0.8, 0.82]
 - with 'arms' = [B(0.01), B(0.02), B(0.3), B(0.4), B(0.5), B(0.6), B(0.78), B(0.8), B(0.82)]
 - with 'nbArms' = 9
 - with 'maxArm' = 0.82
Number of environments to try: 1

Evaluating environment: <MAB{'maxArm': 0.81999999999999995, 'nbArms': 9, 'arms': [B(0.01), B(0.02), B(0.3), B(0.4), B(0.5), B(0.6), B(0.78), B(0.8), B(0.82)]}>
- Adding policy #1 = {'params': {'children': [<Policies.TakeFixedArm.TakeFixedArm object at 0x7ffae63105f8>, <Policies.TakeFixedArm.TakeFixedArm object at 0x7ffaf8fe5cc0>, <Policies.TakeFixedArm.TakeFixedArm object at 0x7ffaf8fe5c88>, <Policies.TakeFixedArm.TakeFixedArm object at 0x7ffad526e9e8>, <Policies.TakeFixedArm.TakeFixedArm object at 0x7ffad526ea20>, <Policies.TakeFixedArm.TakeFixedArm object at 0x7ffad5283ef0>, <Policies.TakeFixedArm.TakeFixedArm object at 0x7ffad50e7748>, <Policies.TakeFixedArm.TakeFixedArm object at 0x7ffad50e76d8>, <Policies.TakeFixedArm.TakeFixedArm object at 0x7ffad50e7780>], 'update_all_children': False, 'update_like_exp4': False, 'decreaseRate': 'auto', 'unbiased': False, 'learningRate': 0.01}, 'archtype': <class 'Policies.Aggr.Aggr'>} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][0]' = {'params': {'children': [<Policies.TakeFixedArm.TakeFixedArm object at 0x7ffae63105f8>, <Policies.TakeFixedArm.TakeFixedArm object at 0x7ffaf8fe5cc0>, <Policies.TakeFixedArm.TakeFixedArm object at 0x7ffaf8fe5c88>, <Policies.TakeFixedArm.TakeFixedArm object at 0x7ffad526e9e8>, <Policies.TakeFixedArm.TakeFixedArm object at 0x7ffad526ea20>, <Policies.TakeFixedArm.TakeFixedArm object at 0x7ffad5283ef0>, <Policies.TakeFixedArm.TakeFixedArm object at 0x7ffad50e7748>, <Policies.TakeFixedArm.TakeFixedArm object at 0x7ffad50e76d8>, <Policies.TakeFixedArm.TakeFixedArm object at 0x7ffad50e7780>], 'update_all_children': False, 'update_like_exp4': False, 'decreaseRate': 'auto', 'unbiased': False, 'learningRate': 0.01}, 'archtype': <class 'Policies.Aggr.Aggr'>} ...
  Using this already created player 'children[0]' = TakeFixedArm(0) ...
  Using this already created player 'children[1]' = TakeFixedArm(1) ...
  Using this already created player 'children[2]' = TakeFixedArm(2) ...
  Using this already created player 'children[3]' = TakeFixedArm(3) ...
  Using this already created player 'children[4]' = TakeFixedArm(4) ...
  Using this already created player 'children[5]' = TakeFixedArm(5) ...
  Using this already created player 'children[6]' = TakeFixedArm(6) ...
  Using this already created player 'children[7]' = TakeFixedArm(7) ...
  Using this already created player 'children[8]' = TakeFixedArm(8) ...
- Adding policy #2 = TakeFixedArm(0) ...
  Using this already created policy 'self.cfg['policies'][1]' = TakeFixedArm(0) ...
- Adding policy #3 = TakeFixedArm(1) ...
  Using this already created policy 'self.cfg['policies'][2]' = TakeFixedArm(1) ...
- Adding policy #4 = TakeFixedArm(2) ...
  Using this already created policy 'self.cfg['policies'][3]' = TakeFixedArm(2) ...
- Adding policy #5 = TakeFixedArm(3) ...
  Using this already created policy 'self.cfg['policies'][4]' = TakeFixedArm(3) ...
- Adding policy #6 = TakeFixedArm(4) ...
  Using this already created policy 'self.cfg['policies'][5]' = TakeFixedArm(4) ...
- Adding policy #7 = TakeFixedArm(5) ...
  Using this already created policy 'self.cfg['policies'][6]' = TakeFixedArm(5) ...
- Adding policy #8 = TakeFixedArm(6) ...
  Using this already created policy 'self.cfg['policies'][7]' = TakeFixedArm(6) ...
- Adding policy #9 = TakeFixedArm(7) ...
  Using this already created policy 'self.cfg['policies'][8]' = TakeFixedArm(7) ...
- Adding policy #10 = TakeFixedArm(8) ...
  Using this already created policy 'self.cfg['policies'][9]' = TakeFixedArm(8) ...

===> Pre-computing the rewards ... Of shape (9, 30000) ...
    In order for all simulated algorithms to face the same random rewards (robust comparaison of A1,..,An vs Aggr(A1,..,An)) ...


- Evaluating policy #1/10: Aggr($N=9$) ...
