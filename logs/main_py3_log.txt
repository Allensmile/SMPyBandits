Loaded experiments configuration from 'configuration.py' :
configuration['policies'] = [{'params': {}, 'archtype': <class 'Policies.Softmax.SoftmaxDecreasing'>}, {'params': {}, 'archtype': <class 'Policies.UCBopt.UCBopt'>}, {'params': {}, 'archtype': <class 'Policies.MOSS.MOSS'>}, {'params': {}, 'archtype': <class 'Policies.Thompson.Thompson'>}, {'params': {}, 'archtype': <class 'Policies.klUCB.klUCB'>}, {'params': {}, 'archtype': <class 'Policies.BayesUCB.BayesUCB'>}]
plots/ is already a directory here...
Number of policies in this comparaison: 6
Time horizon: 500
Number of repetitions: 20
Creating a new MAB problem ...
  Reading arms of this MAB problem from a dictionnary 'configuration' = {'params': [0.005, 0.01, 0.015, 0.02, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.78, 0.8, 0.82, 0.83, 0.84, 0.85], 'arm_type': <class 'Arms.Bernoulli.Bernoulli'>} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = [0.005, 0.01, 0.015, 0.02, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.78, 0.8, 0.82, 0.83, 0.84, 0.85]
 - with 'arms' = [B(0.005), B(0.01), B(0.015), B(0.02), B(0.3), B(0.35), B(0.4), B(0.45), B(0.5), B(0.55), B(0.6), B(0.78), B(0.8), B(0.82), B(0.83), B(0.84), B(0.85)]
 - with 'nbArms' = 17
 - with 'maxArm' = 0.85
Number of environments to try: 1

Evaluating environment: <MAB{'nbArms': 17, 'maxArm': 0.84999999999999998, 'arms': [B(0.005), B(0.01), B(0.015), B(0.02), B(0.3), B(0.35), B(0.4), B(0.45), B(0.5), B(0.55), B(0.6), B(0.78), B(0.8), B(0.82), B(0.83), B(0.84), B(0.85)]}>
- Adding policy #1 = {'params': {}, 'archtype': <class 'Policies.Softmax.SoftmaxDecreasing'>} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][0]' = {'params': {}, 'archtype': <class 'Policies.Softmax.SoftmaxDecreasing'>} ...
- Adding policy #2 = {'params': {}, 'archtype': <class 'Policies.UCBopt.UCBopt'>} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][1]' = {'params': {}, 'archtype': <class 'Policies.UCBopt.UCBopt'>} ...
- Adding policy #3 = {'params': {}, 'archtype': <class 'Policies.MOSS.MOSS'>} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][2]' = {'params': {}, 'archtype': <class 'Policies.MOSS.MOSS'>} ...
- Adding policy #4 = {'params': {}, 'archtype': <class 'Policies.Thompson.Thompson'>} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][3]' = {'params': {}, 'archtype': <class 'Policies.Thompson.Thompson'>} ...
- Adding policy #5 = {'params': {}, 'archtype': <class 'Policies.klUCB.klUCB'>} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][4]' = {'params': {}, 'archtype': <class 'Policies.klUCB.klUCB'>} ...
- Adding policy #6 = {'params': {}, 'archtype': <class 'Policies.BayesUCB.BayesUCB'>} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][5]' = {'params': {}, 'archtype': <class 'Policies.BayesUCB.BayesUCB'>} ...

- Evaluating policy #1/6: Softmax(decreasing) ...

- Evaluating policy #2/6: UCBopt ...

- Evaluating policy #3/6: MOSS ...

- Evaluating policy #4/6: Thompson ...

- Evaluating policy #5/6: klUCB ...

- Evaluating policy #6/6: BayesUCB ...
Giving the final ranks ...

Final ranking for this environment #0 :
- Policy 'Softmax(decreasing)'	was ranked	1 / 6 for this simulation (last regret = 19.25).
- Policy 'BayesUCB'	was ranked	2 / 6 for this simulation (last regret = 29.9).
- Policy 'Thompson'	was ranked	3 / 6 for this simulation (last regret = 36).
- Policy 'UCBopt'	was ranked	4 / 6 for this simulation (last regret = 40.9).
- Policy 'klUCB'	was ranked	5 / 6 for this simulation (last regret = 40.95).
- Policy 'MOSS'	was ranked	6 / 6 for this simulation (last regret = 45.35).
plots/T500_N20__6_algos is already a directory here...
