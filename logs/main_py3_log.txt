 - Setting dpi of all figures to 110 ...
 - Setting 'figsize' of all figures to (19.8, 10.8) ...
Info: Using the regular tqdm() decorator ...
Info: numba.jit seems to be available.
Info: numba.jit seems to be available.
Loaded experiments configuration from 'configuration.py' :
configuration['policies'] = [{'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.5}}, {'archtype': <class 'Policies.Thompson.Thompson'>, 'params': {}}, {'archtype': <class 'Policies.klUCB.klUCB'>, 'params': {'klucb': <built-in function klucbBern>}}, {'archtype': <class 'Policies.klUCBPlus.klUCBPlus'>, 'params': {'klucb': <built-in function klucbBern>}}, {'archtype': <class 'Policies.BayesUCB.BayesUCB'>, 'params': {}}]
====> TURNING DEBUG MODE ON <=====
plots/ is already a directory here...
Number of policies in this comparison: 5
Time horizon: 30000
Number of repetitions: 50
Sampling rate for saving, delta_t_save: 1
Sampling rate for plotting, delta_t_plot: 50
Number of jobs for parallelization: 4
Creating a new MAB problem ...
  Reading arms of this MAB problem from a dictionnary 'configuration' = {'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': [0.001, 0.001, 0.005, 0.005, 0.01, 0.01, 0.02, 0.02, 0.02, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.3]} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = [0.001, 0.001, 0.005, 0.005, 0.01, 0.01, 0.02, 0.02, 0.02, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.3]
 - with 'arms' = [B(0.001), B(0.001), B(0.005), B(0.005), B(0.01), B(0.01), B(0.02), B(0.02), B(0.02), B(0.05), B(0.05), B(0.05), B(0.05), B(0.05), B(0.05), B(0.3)]
 - with 'means' = [ 0.001  0.001  0.005  0.005  0.01   0.01   0.02   0.02   0.02   0.05   0.05
  0.05   0.05   0.05   0.05   0.3  ]
 - with 'nbArms' = 16
 - with 'maxArm' = 0.3
 - with 'minArm' = 0.001

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 15.9 ... 
 - a Optimal Arm Identification factor H_OI(mu) = 68.08% ...
Number of environments to try: 1

Evaluating environment: MAB(nbArms: 16, arms: [B(0.001), B(0.001), B(0.005), B(0.005), B(0.01), B(0.01), B(0.02), B(0.02), B(0.02), B(0.05), B(0.05), B(0.05), B(0.05), B(0.05), B(0.05), B(0.3)], minArm: 0.001, maxArm: 0.3)
- Adding policy #1 = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.5}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][0]' = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.5}} ...
- Adding policy #2 = {'archtype': <class 'Policies.Thompson.Thompson'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][1]' = {'archtype': <class 'Policies.Thompson.Thompson'>, 'params': {}} ...
- Adding policy #3 = {'archtype': <class 'Policies.klUCB.klUCB'>, 'params': {'klucb': <built-in function klucbBern>}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][2]' = {'archtype': <class 'Policies.klUCB.klUCB'>, 'params': {'klucb': <built-in function klucbBern>}} ...
- Adding policy #4 = {'archtype': <class 'Policies.klUCBPlus.klUCBPlus'>, 'params': {'klucb': <built-in function klucbBern>}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][3]' = {'archtype': <class 'Policies.klUCBPlus.klUCBPlus'>, 'params': {'klucb': <built-in function klucbBern>}} ...
- Adding policy #5 = {'archtype': <class 'Policies.BayesUCB.BayesUCB'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][4]' = {'archtype': <class 'Policies.BayesUCB.BayesUCB'>, 'params': {}} ...

- Evaluating policy #1/5: UCB($\alpha=0.5$) ...

Estimated order by the policy UCB($\alpha=0.5$) after 30000 steps: [ 0  1  2  3  8  5  6 13 11 14 12 10  4  7  9 15] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 68.75% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 99.60% (relative success)...
  ==> Spearman    distance from optimal ordering: 99.64% (relative success)...
  ==> Gestalt     distance from optimal ordering: 56.25% (relative success)...
  ==> Mean distance from optimal ordering: 81.06% (relative success)...

- Evaluating policy #2/5: Thompson ...

Estimated order by the policy Thompson after 30000 steps: [14  6  7  0  8  4 12 13  1  2  3 11  5  9 10 15] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 37.50% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 58.23% (relative success)...
  ==> Spearman    distance from optimal ordering: 44.31% (relative success)...
  ==> Gestalt     distance from optimal ordering: 50.00% (relative success)...
  ==> Mean distance from optimal ordering: 47.51% (relative success)...

- Evaluating policy #3/5: KL-UCB(Bern) ...

Estimated order by the policy KL-UCB(Bern) after 30000 steps: [11 10 12  0  1  2  3  4  5  6  7  8 13 14  9 15] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 50.00% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 98.50% (relative success)...
  ==> Spearman    distance from optimal ordering: 87.22% (relative success)...
  ==> Gestalt     distance from optimal ordering: 75.00% (relative success)...
  ==> Mean distance from optimal ordering: 77.68% (relative success)...

- Evaluating policy #4/5: KL-UCB+(Bern) ...

Estimated order by the policy KL-UCB+(Bern) after 30000 steps: [ 0  1  2  3  4  5  8 10 12  6  7 11 14 13  9 15] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 82.81% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 100.00% (relative success)...
  ==> Spearman    distance from optimal ordering: 100.00% (relative success)...
  ==> Gestalt     distance from optimal ordering: 68.75% (relative success)...
  ==> Mean distance from optimal ordering: 87.89% (relative success)...

- Evaluating policy #5/5: BayesUCB ...

Estimated order by the policy BayesUCB after 30000 steps: [ 0  1  2  3  4  5  8  6 14  9 10  7 11 12 13 15] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 87.50% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 100.00% (relative success)...
  ==> Spearman    distance from optimal ordering: 100.00% (relative success)...
  ==> Gestalt     distance from optimal ordering: 81.25% (relative success)...
  ==> Mean distance from optimal ordering: 92.19% (relative success)...
Giving the final ranks ...

Final ranking for this environment #0 :
- Policy 'KL-UCB+(Bern)'	was ranked	1 / 5 for this simulation (last regret = 109.3).
- Policy 'UCB($\alpha=0.5$)'	was ranked	2 / 5 for this simulation (last regret = 109.56).
- Policy 'Thompson'	was ranked	3 / 5 for this simulation (last regret = 114.46).
- Policy 'BayesUCB'	was ranked	4 / 5 for this simulation (last regret = 138.8).
- Policy 'KL-UCB(Bern)'	was ranked	5 / 5 for this simulation (last regret = 144.08).

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 15.9 for 1-player problem... 
 - a Optimal Arm Identification factor H_OI(mu) = 68.08% ...

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 15.9 for 1-player problem... 
 - a Optimal Arm Identification factor H_OI(mu) = 68.08% ...
FIXME plotting STD ...
[0;31m---------------------------------------------------------------------------[0m
[0;31mValueError[0m                                Traceback (most recent call last)
[0;32m/home/lilian/ownCloud/cloud.openmailbox.org/Th√®se_2016-17/src/AlgoBandits.git/main.py[0m in [0;36m<module>[0;34m()[0m
[1;32m    148[0m             [0;32mif[0m [0mconfiguration[0m[0;34m[[0m[0;34m'repetitions'[0m[0;34m][0m [0;34m>[0m [0;36m1[0m [0;32mand[0m [0mplotSTD[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    149[0m                 [0mprint[0m[0;34m([0m[0;34m"FIXME plotting STD ..."[0m[0;34m)[0m  [0;31m# DEBUG[0m[0;34m[0m[0m
[0;32m--> 150[0;31m                 [0mevaluation[0m[0;34m.[0m[0mplotRegrets[0m[0;34m([0m[0menvId[0m[0;34m,[0m [0msemilogx[0m[0;34m=[0m[0msemilogx[0m[0;34m,[0m [0mplotSTD[0m[0;34m=[0m[0;32mTrue[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    151[0m             [0;32mif[0m [0mconfiguration[0m[0;34m[[0m[0;34m'repetitions'[0m[0;34m][0m [0;34m>[0m [0;36m1[0m [0;32mand[0m [0mplotMaxMin[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    152[0m                 [0mprint[0m[0;34m([0m[0;34m"FIXME plotting MaxMin ..."[0m[0;34m)[0m  [0;31m# DEBUG[0m[0;34m[0m[0m

[0;32m/home/lilian/ownCloud/cloud.openmailbox.org/Th√®se_2016-17/src/AlgoBandits.git/Environment/Evaluator.py[0m in [0;36mplotRegrets[0;34m(self, envId, savefig, meanRegret, plotSTD, plotMaxMin, semilogx, normalizedRegret, drawUpperBound)[0m
[1;32m    278[0m                 [0;32mif[0m [0mnormalizedRegret[0m[0;34m:[0m[0;34m[0m[0m
[1;32m    279[0m                     [0mstdY[0m [0;34m/=[0m [0mnp[0m[0;34m.[0m[0mlog[0m[0;34m([0m[0;36m2[0m [0;34m+[0m [0mX[0m[0;34m)[0m[0;34m[0m[0m
[0;32m--> 280[0;31m                 [0mplt[0m[0;34m.[0m[0mfill_between[0m[0;34m([0m[0mX[0m[0;34m[[0m[0;34m:[0m[0;34m:[0m[0mself[0m[0;34m.[0m[0mdelta_t_plot[0m[0;34m][0m[0;34m,[0m [0mY[0m[0;34m[[0m[0;34m:[0m[0;34m:[0m[0mself[0m[0;34m.[0m[0mdelta_t_plot[0m[0;34m][0m [0;34m-[0m [0mstdY[0m[0;34m,[0m [0mY[0m[0;34m[[0m[0;34m:[0m[0;34m:[0m[0mself[0m[0;34m.[0m[0mdelta_t_plot[0m[0;34m][0m [0;34m+[0m [0mstdY[0m[0;34m,[0m [0mfacecolor[0m[0;34m=[0m[0mcolors[0m[0;34m[[0m[0mi[0m[0;34m][0m[0;34m,[0m [0malpha[0m[0;34m=[0m[0;36m0.3[0m[0;34m)[0m[0;34m[0m[0m
[0m[1;32m    281[0m             [0;31m# Print amplitude of regret[0m[0;34m[0m[0;34m[0m[0m
[1;32m    282[0m             [0;32mif[0m [0mplotMaxMin[0m [0;32mand[0m [0mself[0m[0;34m.[0m[0mrepetitions[0m [0;34m>[0m [0;36m1[0m[0;34m:[0m[0;34m[0m[0m

[0;31mValueError[0m: operands could not be broadcast together with shapes (600,) (30000,) 
