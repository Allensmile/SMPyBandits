 - Setting dpi of all figures to 110 ...
 - Setting 'figsize' of all figures to (19.8, 10.8) ...
Info: Using the regular tqdm() decorator ...
Info: numba.jit seems to be available.
Info: numba.jit seems to be available.
Loaded experiments configuration from 'configuration.py' :
configuration['policies'] = [{'params': {'alpha': 1}, 'archtype': <class 'Policies.UCBalpha.UCBalpha'>}, {'params': {'alpha': 0.5}, 'archtype': <class 'Policies.UCBalpha.UCBalpha'>}, {'params': {'posterior': <class 'Policies.Posterior.Beta.Beta'>}, 'archtype': <class 'Policies.Thompson.Thompson'>}, {'params': {'posterior': <class 'Policies.Posterior.Gauss.Gauss'>}, 'archtype': <class 'Policies.Thompson.Thompson'>}, {'params': {'klucb': <built-in function klucbBern>}, 'archtype': <class 'Policies.klUCB.klUCB'>}, {'params': {'klucb': <built-in function klucbBern>}, 'archtype': <class 'Policies.klUCBPlus.klUCBPlus'>}, {'params': {'klucb': <built-in function klucbBern>, 'horizon': 30000}, 'archtype': <class 'Policies.klUCBHPlus.klUCBHPlus'>}, {'params': {'klucb': <built-in function klucbBern>, 'horizon': 30000}, 'archtype': <class 'Policies.klUCBPlusPlus.klUCBPlusPlus'>}, {'params': {'posterior': <class 'Policies.Posterior.Beta.Beta'>}, 'archtype': <class 'Policies.BayesUCB.BayesUCB'>}, {'params': {'posterior': <class 'Policies.Posterior.Gauss.Gauss'>}, 'archtype': <class 'Policies.BayesUCB.BayesUCB'>}]
====> TURNING DEBUG MODE ON <=====
plots/ is already a directory here...
Number of policies in this comparison: 10
Time horizon: 30000
Number of repetitions: 50
Sampling rate for saving, delta_t_save: 1
Sampling rate for plotting, delta_t_plot: 50
Number of jobs for parallelization: 4
Creating a new MAB problem ...
  Reading arms of this MAB problem from a dictionnary 'configuration' = {'params': [(0.1, 0.05), (0.2, 0.05), (0.3, 0.05), (0.4, 0.05), (0.5, 0.05), (0.6, 0.05), (0.7, 0.05), (0.8, 0.05), (0.9, 0.05)], 'arm_type': <class 'Arms.Gaussian.Gaussian'>} ...
 - with 'arm_type' = <class 'Arms.Gaussian.Gaussian'>
 - with 'params' = [(0.1, 0.05), (0.2, 0.05), (0.3, 0.05), (0.4, 0.05), (0.5, 0.05), (0.6, 0.05), (0.7, 0.05), (0.8, 0.05), (0.9, 0.05)]
 - with 'arms' = [G(0.1, 0.05), G(0.2, 0.05), G(0.3, 0.05), G(0.4, 0.05), G(0.5, 0.05), G(0.6, 0.05), G(0.7, 0.05), G(0.8, 0.05), G(0.9, 0.05)]
 - with 'means' = [ 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9]
 - with 'nbArms' = 9
 - with 'maxArm' = 0.9
 - with 'minArm' = 0.1

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 2.72 ... 
 - a Optimal Arm Identification factor H_OI(mu) = 48.89% ...
 - with 'arms' represented as: $[G(0.1, 0.05), G(0.2, 0.05), G(0.3, 0.05), G(0.4, 0.05), G(0.5, 0.05), G(0.6, 0.05), G(0.7, 0.05), G(0.8,$
$0.05), G(0.9, 0.05)^*]$
Number of environments to try: 1

Evaluating environment: MAB(nbArms: 9, arms: [G(0.1, 0.05), G(0.2, 0.05), G(0.3, 0.05), G(0.4, 0.05), G(0.5, 0.05), G(0.6, 0.05), G(0.7, 0.05), G(0.8, 0.05), G(0.9, 0.05)], minArm: 0.1, maxArm: 0.9)
- Adding policy #1 = {'params': {'alpha': 1}, 'archtype': <class 'Policies.UCBalpha.UCBalpha'>} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][0]' = {'params': {'alpha': 1}, 'archtype': <class 'Policies.UCBalpha.UCBalpha'>} ...
- Adding policy #2 = {'params': {'alpha': 0.5}, 'archtype': <class 'Policies.UCBalpha.UCBalpha'>} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][1]' = {'params': {'alpha': 0.5}, 'archtype': <class 'Policies.UCBalpha.UCBalpha'>} ...
- Adding policy #3 = {'params': {'posterior': <class 'Policies.Posterior.Beta.Beta'>}, 'archtype': <class 'Policies.Thompson.Thompson'>} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][2]' = {'params': {'posterior': <class 'Policies.Posterior.Beta.Beta'>}, 'archtype': <class 'Policies.Thompson.Thompson'>} ...
- Adding policy #4 = {'params': {'posterior': <class 'Policies.Posterior.Gauss.Gauss'>}, 'archtype': <class 'Policies.Thompson.Thompson'>} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][3]' = {'params': {'posterior': <class 'Policies.Posterior.Gauss.Gauss'>}, 'archtype': <class 'Policies.Thompson.Thompson'>} ...
- Adding policy #5 = {'params': {'klucb': <built-in function klucbBern>}, 'archtype': <class 'Policies.klUCB.klUCB'>} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][4]' = {'params': {'klucb': <built-in function klucbBern>}, 'archtype': <class 'Policies.klUCB.klUCB'>} ...
- Adding policy #6 = {'params': {'klucb': <built-in function klucbBern>}, 'archtype': <class 'Policies.klUCBPlus.klUCBPlus'>} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][5]' = {'params': {'klucb': <built-in function klucbBern>}, 'archtype': <class 'Policies.klUCBPlus.klUCBPlus'>} ...
- Adding policy #7 = {'params': {'klucb': <built-in function klucbBern>, 'horizon': 30000}, 'archtype': <class 'Policies.klUCBHPlus.klUCBHPlus'>} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][6]' = {'params': {'klucb': <built-in function klucbBern>, 'horizon': 30000}, 'archtype': <class 'Policies.klUCBHPlus.klUCBHPlus'>} ...
- Adding policy #8 = {'params': {'klucb': <built-in function klucbBern>, 'horizon': 30000}, 'archtype': <class 'Policies.klUCBPlusPlus.klUCBPlusPlus'>} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][7]' = {'params': {'klucb': <built-in function klucbBern>, 'horizon': 30000}, 'archtype': <class 'Policies.klUCBPlusPlus.klUCBPlusPlus'>} ...
- Adding policy #9 = {'params': {'posterior': <class 'Policies.Posterior.Beta.Beta'>}, 'archtype': <class 'Policies.BayesUCB.BayesUCB'>} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][8]' = {'params': {'posterior': <class 'Policies.Posterior.Beta.Beta'>}, 'archtype': <class 'Policies.BayesUCB.BayesUCB'>} ...
- Adding policy #10 = {'params': {'posterior': <class 'Policies.Posterior.Gauss.Gauss'>}, 'archtype': <class 'Policies.BayesUCB.BayesUCB'>} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][9]' = {'params': {'posterior': <class 'Policies.Posterior.Gauss.Gauss'>}, 'archtype': <class 'Policies.BayesUCB.BayesUCB'>} ...

- Evaluating policy #1/10: UCB($\alpha=1$) ...

Estimated order by the policy UCB($\alpha=1$) after 30000 steps: [1 0 3 4 2 5 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 85.19% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 99.82% (relative success)...
  ==> Spearman    distance from optimal ordering: 99.98% (relative success)...
  ==> Gestalt     distance from optimal ordering: 77.78% (relative success)...
  ==> Mean distance from optimal ordering: 90.69% (relative success)...

- Evaluating policy #2/10: UCB($\alpha=0.5$) ...

Estimated order by the policy UCB($\alpha=0.5$) after 30000 steps: [2 0 1 3 4 5 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 90.12% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 99.92% (relative success)...
  ==> Spearman    distance from optimal ordering: 99.99% (relative success)...
  ==> Gestalt     distance from optimal ordering: 88.89% (relative success)...
  ==> Mean distance from optimal ordering: 94.73% (relative success)...

- Evaluating policy #3/10: Thompson(<class 'Policies.Posterior.Beta.Beta'>) ...

Estimated order by the policy Thompson(<class 'Policies.Posterior.Beta.Beta'>) after 30000 steps: [2 0 3 1 4 5 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 85.19% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 99.82% (relative success)...
  ==> Spearman    distance from optimal ordering: 99.95% (relative success)...
  ==> Gestalt     distance from optimal ordering: 77.78% (relative success)...
  ==> Mean distance from optimal ordering: 90.68% (relative success)...

- Evaluating policy #4/10: Thompson(<class 'Policies.Posterior.Gauss.Gauss'>) ...

Estimated order by the policy Thompson(<class 'Policies.Posterior.Gauss.Gauss'>) after 30000 steps: [1 0 2 3 4 5 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 95.06% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 99.96% (relative success)...
  ==> Spearman    distance from optimal ordering: 100.00% (relative success)...
  ==> Gestalt     distance from optimal ordering: 88.89% (relative success)...
  ==> Mean distance from optimal ordering: 95.98% (relative success)...

- Evaluating policy #5/10: KL-UCB(Bern) ...

Estimated order by the policy KL-UCB(Bern) after 30000 steps: [0 1 2 4 5 3 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 90.12% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 99.92% (relative success)...
  ==> Spearman    distance from optimal ordering: 99.99% (relative success)...
  ==> Gestalt     distance from optimal ordering: 88.89% (relative success)...
  ==> Mean distance from optimal ordering: 94.73% (relative success)...

- Evaluating policy #6/10: KL-UCB+(Bern) ...

Estimated order by the policy KL-UCB+(Bern) after 30000 steps: [2 3 0 1 5 4 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 75.31% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 99.33% (relative success)...
  ==> Spearman    distance from optimal ordering: 99.63% (relative success)...
  ==> Gestalt     distance from optimal ordering: 66.67% (relative success)...
  ==> Mean distance from optimal ordering: 85.23% (relative success)...

- Evaluating policy #7/10: KL-UCB-H+(Bern) ...

Estimated order by the policy KL-UCB-H+(Bern) after 30000 steps: [2 0 1 3 5 4 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 85.19% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 99.82% (relative success)...
  ==> Spearman    distance from optimal ordering: 99.98% (relative success)...
  ==> Gestalt     distance from optimal ordering: 77.78% (relative success)...
  ==> Mean distance from optimal ordering: 90.69% (relative success)...

- Evaluating policy #8/10: KL-UCB++(Bern) ...

Estimated order by the policy KL-UCB++(Bern) after 30000 steps: [2 0 1 3 4 6 5 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 85.19% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 99.82% (relative success)...
  ==> Spearman    distance from optimal ordering: 99.98% (relative success)...
  ==> Gestalt     distance from optimal ordering: 77.78% (relative success)...
  ==> Mean distance from optimal ordering: 90.69% (relative success)...

- Evaluating policy #9/10: BayesUCB(<class 'Policies.Posterior.Beta.Beta'>) ...

Estimated order by the policy BayesUCB(<class 'Policies.Posterior.Beta.Beta'>) after 30000 steps: [0 1 2 3 6 4 5 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 90.12% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 99.92% (relative success)...
  ==> Spearman    distance from optimal ordering: 99.99% (relative success)...
  ==> Gestalt     distance from optimal ordering: 88.89% (relative success)...
  ==> Mean distance from optimal ordering: 94.73% (relative success)...

- Evaluating policy #10/10: BayesUCB(<class 'Policies.Posterior.Gauss.Gauss'>) ...

Estimated order by the policy BayesUCB(<class 'Policies.Posterior.Gauss.Gauss'>) after 30000 steps: [0 1 4 3 6 7 8 2 5] ...
  ==> Optimal arm identification: 66.67% (relative success)...
  ==> Manhattan   distance from optimal ordering: 60.49% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 93.94% (relative success)...
  ==> Spearman    distance from optimal ordering: 90.08% (relative success)...
  ==> Gestalt     distance from optimal ordering: 66.67% (relative success)...
  ==> Mean distance from optimal ordering: 77.80% (relative success)...
Giving the final ranks ...

Final ranking for this environment #0 :
- Policy 'Thompson(<class 'Policies.Posterior.Beta.Beta'>)'	was ranked	1 / 10 for this simulation (last regret = 60.2096).
- Policy 'BayesUCB(<class 'Policies.Posterior.Beta.Beta'>)'	was ranked	2 / 10 for this simulation (last regret = 65.711).
- Policy 'KL-UCB-H+(Bern)'	was ranked	3 / 10 for this simulation (last regret = 65.9825).
- Policy 'KL-UCB+(Bern)'	was ranked	4 / 10 for this simulation (last regret = 68.0382).
- Policy 'KL-UCB++(Bern)'	was ranked	5 / 10 for this simulation (last regret = 71.7672).
- Policy 'UCB($\alpha=0.5$)'	was ranked	6 / 10 for this simulation (last regret = 78.1147).
- Policy 'KL-UCB(Bern)'	was ranked	7 / 10 for this simulation (last regret = 85.9128).
- Policy 'Thompson(<class 'Policies.Posterior.Gauss.Gauss'>)'	was ranked	8 / 10 for this simulation (last regret = 110.42).
- Policy 'UCB($\alpha=1$)'	was ranked	9 / 10 for this simulation (last regret = 135.946).
- Policy 'BayesUCB(<class 'Policies.Posterior.Gauss.Gauss'>)'	was ranked	10 / 10 for this simulation (last regret = 3685.98).

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 2.72 for 1-player problem... 
 - a Optimal Arm Identification factor H_OI(mu) = 48.89% ...

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 2.72 for 1-player problem... 
 - a Optimal Arm Identification factor H_OI(mu) = 48.89% ...

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 2.72 for 1-player problem... 
 - a Optimal Arm Identification factor H_OI(mu) = 48.89% ...
Done for simulations main.py ...
