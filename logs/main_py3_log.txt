Loaded experiments configuration from 'configuration.py' :
configuration['policies'] = [{'params': {}, 'archtype': <class 'Policies.UCB.UCB'>}, {'params': {'alpha': 4}, 'archtype': <class 'Policies.UCBalpha.UCBalpha'>}, {'params': {'alpha': 0.5}, 'archtype': <class 'Policies.UCBalpha.UCBalpha'>}, {'params': {'alpha': 0.1}, 'archtype': <class 'Policies.UCBalpha.UCBalpha'>}, {'params': {'temperature': 0.05}, 'archtype': <class 'Policies.Softmax.Softmax'>}, {'params': {}, 'archtype': <class 'Policies.Thompson.Thompson'>}, {'params': {}, 'archtype': <class 'Policies.klUCB.klUCB'>}, {'params': {}, 'archtype': <class 'Policies.BayesUCB.BayesUCB'>}, {'params': {'alpha': 0.5, 'horizon': 30000}, 'archtype': <class 'Policies.AdBandits.AdBandit'>}, {'params': {'alpha': 0.125, 'horizon': 30000}, 'archtype': <class 'Policies.AdBandits.AdBandit'>}, {'params': {'learningRate': 0.01, 'decreaseRate': 15000.0, 'update_all_children': True, 'children': [{'params': {}, 'archtype': <class 'Policies.UCB.UCB'>}, {'params': {'alpha': 4}, 'archtype': <class 'Policies.UCBalpha.UCBalpha'>}, {'params': {'alpha': 0.5}, 'archtype': <class 'Policies.UCBalpha.UCBalpha'>}, {'params': {'alpha': 0.1}, 'archtype': <class 'Policies.UCBalpha.UCBalpha'>}, {'params': {'temperature': 0.05}, 'archtype': <class 'Policies.Softmax.Softmax'>}, {'params': {}, 'archtype': <class 'Policies.Thompson.Thompson'>}, {'params': {}, 'archtype': <class 'Policies.klUCB.klUCB'>}, {'params': {}, 'archtype': <class 'Policies.BayesUCB.BayesUCB'>}, {'params': {'alpha': 0.5, 'horizon': 30000}, 'archtype': <class 'Policies.AdBandits.AdBandit'>}, {'params': {'alpha': 0.125, 'horizon': 30000}, 'archtype': <class 'Policies.AdBandits.AdBandit'>}]}, 'archtype': <class 'Policies.Aggr.Aggr'>}]
plots is already a directory here...
Number of policies in this comparaison: 11
Time horizon: 30000
Number of repetitions: 50
Creating a new MAB problem ...
  Reading arms of this MAB problem from a dictionnary 'configuration' = {'params': [0.005, 0.01, 0.015, 0.02, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.78, 0.8, 0.82, 0.83, 0.84, 0.85], 'arm_type': <class 'Arms.Bernoulli.Bernoulli'>} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = [0.005, 0.01, 0.015, 0.02, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.78, 0.8, 0.82, 0.83, 0.84, 0.85]
 - with 'arms' = [B(0.005), B(0.01), B(0.015), B(0.02), B(0.3), B(0.35), B(0.4), B(0.45), B(0.5), B(0.55), B(0.6), B(0.78), B(0.8), B(0.82), B(0.83), B(0.84), B(0.85)]
 - with 'nbArms' = 17
 - with 'maxArm' = 0.85
Number of environments to try: 1

Evaluating environment: <MAB{'nbArms': 17, 'maxArm': 0.85, 'arms': [B(0.005), B(0.01), B(0.015), B(0.02), B(0.3), B(0.35), B(0.4), B(0.45), B(0.5), B(0.55), B(0.6), B(0.78), B(0.8), B(0.82), B(0.83), B(0.84), B(0.85)]}>
- Adding policy #1 = {'params': {}, 'archtype': <class 'Policies.UCB.UCB'>} ...
- Adding policy #2 = {'params': {'alpha': 4}, 'archtype': <class 'Policies.UCBalpha.UCBalpha'>} ...
- Adding policy #3 = {'params': {'alpha': 0.5}, 'archtype': <class 'Policies.UCBalpha.UCBalpha'>} ...
- Adding policy #4 = {'params': {'alpha': 0.1}, 'archtype': <class 'Policies.UCBalpha.UCBalpha'>} ...
- Adding policy #5 = {'params': {'temperature': 0.05}, 'archtype': <class 'Policies.Softmax.Softmax'>} ...
- Adding policy #6 = {'params': {}, 'archtype': <class 'Policies.Thompson.Thompson'>} ...
- Adding policy #7 = {'params': {}, 'archtype': <class 'Policies.klUCB.klUCB'>} ...
- Adding policy #8 = {'params': {}, 'archtype': <class 'Policies.BayesUCB.BayesUCB'>} ...
- Adding policy #9 = {'params': {'alpha': 0.5, 'horizon': 30000}, 'archtype': <class 'Policies.AdBandits.AdBandit'>} ...
- Adding policy #10 = {'params': {'alpha': 0.125, 'horizon': 30000}, 'archtype': <class 'Policies.AdBandits.AdBandit'>} ...
- Adding policy #11 = {'params': {'learningRate': 0.01, 'decreaseRate': 15000.0, 'update_all_children': True, 'children': [{'params': {}, 'archtype': <class 'Policies.UCB.UCB'>}, {'params': {'alpha': 4}, 'archtype': <class 'Policies.UCBalpha.UCBalpha'>}, {'params': {'alpha': 0.5}, 'archtype': <class 'Policies.UCBalpha.UCBalpha'>}, {'params': {'alpha': 0.1}, 'archtype': <class 'Policies.UCBalpha.UCBalpha'>}, {'params': {'temperature': 0.05}, 'archtype': <class 'Policies.Softmax.Softmax'>}, {'params': {}, 'archtype': <class 'Policies.Thompson.Thompson'>}, {'params': {}, 'archtype': <class 'Policies.klUCB.klUCB'>}, {'params': {}, 'archtype': <class 'Policies.BayesUCB.BayesUCB'>}, {'params': {'alpha': 0.5, 'horizon': 30000}, 'archtype': <class 'Policies.AdBandits.AdBandit'>}, {'params': {'alpha': 0.125, 'horizon': 30000}, 'archtype': <class 'Policies.AdBandits.AdBandit'>}]}, 'archtype': <class 'Policies.Aggr.Aggr'>} ...
  Creating this child player from a dictionnary 'children[0]' = {'params': {}, 'archtype': <class 'Policies.UCB.UCB'>} ...
  Creating this child player from a dictionnary 'children[1]' = {'params': {'alpha': 4}, 'archtype': <class 'Policies.UCBalpha.UCBalpha'>} ...
  Creating this child player from a dictionnary 'children[2]' = {'params': {'alpha': 0.5}, 'archtype': <class 'Policies.UCBalpha.UCBalpha'>} ...
  Creating this child player from a dictionnary 'children[3]' = {'params': {'alpha': 0.1}, 'archtype': <class 'Policies.UCBalpha.UCBalpha'>} ...
  Creating this child player from a dictionnary 'children[4]' = {'params': {'temperature': 0.05}, 'archtype': <class 'Policies.Softmax.Softmax'>} ...
  Creating this child player from a dictionnary 'children[5]' = {'params': {}, 'archtype': <class 'Policies.Thompson.Thompson'>} ...
  Creating this child player from a dictionnary 'children[6]' = {'params': {}, 'archtype': <class 'Policies.klUCB.klUCB'>} ...
  Creating this child player from a dictionnary 'children[7]' = {'params': {}, 'archtype': <class 'Policies.BayesUCB.BayesUCB'>} ...
  Creating this child player from a dictionnary 'children[8]' = {'params': {'alpha': 0.5, 'horizon': 30000}, 'archtype': <class 'Policies.AdBandits.AdBandit'>} ...
  Creating this child player from a dictionnary 'children[9]' = {'params': {'alpha': 0.125, 'horizon': 30000}, 'archtype': <class 'Policies.AdBandits.AdBandit'>} ...

- Evaluating policy #1/11: UCB ...

- Evaluating policy #2/11: UCB1 (alpha: 4) ...

- Evaluating policy #3/11: UCB1 (alpha: 0.5) ...

- Evaluating policy #4/11: UCB1 (alpha: 0.1) ...

- Evaluating policy #5/11: Softmax (temperature:0.05) ...

- Evaluating policy #6/11: Thompson ...

- Evaluating policy #7/11: klUCB ...

- Evaluating policy #8/11: BayesUCB ...

- Evaluating policy #9/11: AdBandit (alpha:0.5) ...

- Evaluating policy #10/11: AdBandit (alpha:0.125) ...

- Evaluating policy #11/11: Aggr (nb:10, rate:0.01) ...
Giving the final ranks ...

Final ranking for this environment #0 :
- Policy 'AdBandit (alpha:0.5)'	was ranked	1 / 11 for this simulation (last regret = 174).
- Policy 'UCB1 (alpha: 0.1)'	was ranked	2 / 11 for this simulation (last regret = 186).
- Policy 'AdBandit (alpha:0.125)'	was ranked	3 / 11 for this simulation (last regret = 196).
- Policy 'Thompson'	was ranked	4 / 11 for this simulation (last regret = 208).
- Policy 'BayesUCB'	was ranked	5 / 11 for this simulation (last regret = 248).
- Policy 'UCB1 (alpha: 0.5)'	was ranked	6 / 11 for this simulation (last regret = 268).
- Policy 'klUCB'	was ranked	7 / 11 for this simulation (last regret = 302).
- Policy 'Aggr (nb:10, rate:0.01)'	was ranked	8 / 11 for this simulation (last regret = 591).
- Policy 'Softmax (temperature:0.05)'	was ranked	9 / 11 for this simulation (last regret = 607).
- Policy 'UCB'	was ranked	10 / 11 for this simulation (last regret = 872).
- Policy 'UCB1 (alpha: 4)'	was ranked	11 / 11 for this simulation (last regret = 884).
 - Plotting the results, and saving the plot to plots/T30000_N50__11_algos/main____env1-1_7576589196148231609.png ...
Saving to plots/T30000_N50__11_algos/main____env1-1_7576589196148231609.png ...
 - Plotting the results, and saving the plot to plots/T30000_N50__11_algos/main_BestArmPulls____env1-1_7576589196148231609.png ...
Saving to plots/T30000_N50__11_algos/main_BestArmPulls____env1-1_7576589196148231609.png ...
