Loaded experiments configuration from 'configuration.py' :
configuration['policies'] = [{'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 4}}, {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 3}}, {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 2}}, {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 1}}, {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.5}}, {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.25}}, {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.1}}, {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.05}}, {'archtype': <class 'Policies.Aggr.Aggr'>, 'params': {'learningRate': 0.01, 'update_all_children': False, 'unbiased': True, 'update_like_exp4': False, 'children': [{'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 4}}, {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 3}}, {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 2}}, {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 1}}, {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.5}}, {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.25}}, {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.1}}, {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.05}}], 'decreaseRate': 'auto'}}]
plots/ is already a directory here...
Number of policies in this comparaison: 9
Time horizon: 10000
Number of repetitions: 4
Sampling rate DELTA_T_SAVE: 1
Creating a new MAB problem ...
  Reading arms of this MAB problem from a dictionnary 'configuration' = {'params': [0.01, 0.02, 0.3, 0.4, 0.5, 0.6, 0.78, 0.8, 0.82], 'arm_type': <class 'Arms.Bernoulli.Bernoulli'>} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = [0.01, 0.02, 0.3, 0.4, 0.5, 0.6, 0.78, 0.8, 0.82]
 - with 'arms' = [B(0.01), B(0.02), B(0.3), B(0.4), B(0.5), B(0.6), B(0.78), B(0.8), B(0.82)]
 - with 'nbArms' = 9
 - with 'maxArm' = 0.82
Number of environments to try: 1

Evaluating environment: <MAB{'arms': [B(0.01), B(0.02), B(0.3), B(0.4), B(0.5), B(0.6), B(0.78), B(0.8), B(0.82)], 'maxArm': 0.81999999999999995, 'nbArms': 9}>
- Adding policy #1 = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 4}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][0]' = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 4}} ...
- Adding policy #2 = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 3}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][1]' = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 3}} ...
- Adding policy #3 = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 2}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][2]' = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 2}} ...
- Adding policy #4 = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 1}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][3]' = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 1}} ...
- Adding policy #5 = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.5}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][4]' = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.5}} ...
- Adding policy #6 = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.25}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][5]' = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.25}} ...
- Adding policy #7 = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.1}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][6]' = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.1}} ...
- Adding policy #8 = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.05}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][7]' = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.05}} ...
- Adding policy #9 = {'archtype': <class 'Policies.Aggr.Aggr'>, 'params': {'learningRate': 0.01, 'update_all_children': False, 'unbiased': True, 'update_like_exp4': False, 'children': [{'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 4}}, {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 3}}, {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 2}}, {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 1}}, {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.5}}, {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.25}}, {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.1}}, {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.05}}], 'decreaseRate': 'auto'}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][8]' = {'archtype': <class 'Policies.Aggr.Aggr'>, 'params': {'learningRate': 0.01, 'update_all_children': False, 'unbiased': True, 'update_like_exp4': False, 'children': [{'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 4}}, {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 3}}, {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 2}}, {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 1}}, {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.5}}, {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.25}}, {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.1}}, {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.05}}], 'decreaseRate': 'auto'}} ...
  Creating this child player from a dictionnary 'children[0]' = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 4}} ...
  Creating this child player from a dictionnary 'children[1]' = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 3}} ...
  Creating this child player from a dictionnary 'children[2]' = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 2}} ...
  Creating this child player from a dictionnary 'children[3]' = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 1}} ...
  Creating this child player from a dictionnary 'children[4]' = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.5}} ...
  Creating this child player from a dictionnary 'children[5]' = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.25}} ...
  Creating this child player from a dictionnary 'children[6]' = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.1}} ...
  Creating this child player from a dictionnary 'children[7]' = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.05}} ...

===> Pre-computing the rewards ... Of shape (9, 10000) ...
    In order for all simulated algorithms to face the same random rewards (robust comparaison of A1,..,An vs Aggr(A1,..,An)) ...


- Evaluating policy #1/9: UCB(alpha: 4) ...

- Evaluating policy #2/9: UCB(alpha: 3) ...

- Evaluating policy #3/9: UCB(alpha: 2) ...

- Evaluating policy #4/9: UCB(alpha: 1) ...

- Evaluating policy #5/9: UCB(alpha: 0.5) ...

- Evaluating policy #6/9: UCB(alpha: 0.25) ...

- Evaluating policy #7/9: UCB(alpha: 0.1) ...

- Evaluating policy #8/9: UCB(alpha: 0.05) ...

- Evaluating policy #9/9: Aggr(nb: 8, dRate: auto) ...
   => t % 2000 == 0 : reinitializing the trust proba ...
   => t % 2000 == 0 : reinitializing the trust proba ...
   => t % 2000 == 0 : reinitializing the trust proba ...
   => t % 2000 == 0 : reinitializing the trust proba ...
   => t % 2000 == 0 : reinitializing the trust proba ...
   => t % 2000 == 0 : reinitializing the trust proba ...
   => t % 2000 == 0 : reinitializing the trust proba ...
   => t % 2000 == 0 : reinitializing the trust proba ...
   => t % 2000 == 0 : reinitializing the trust proba ...
   => t % 2000 == 0 : reinitializing the trust proba ...
   => t % 2000 == 0 : reinitializing the trust proba ...
   => t % 2000 == 0 : reinitializing the trust proba ...
   => t % 2000 == 0 : reinitializing the trust proba ...
   => t % 2000 == 0 : reinitializing the trust proba ...
   => t % 2000 == 0 : reinitializing the trust proba ...
   => t % 2000 == 0 : reinitializing the trust proba ...
   => t % 2000 == 0 : reinitializing the trust proba ...
   => t % 2000 == 0 : reinitializing the trust proba ...
   => t % 2000 == 0 : reinitializing the trust proba ...
   => t % 2000 == 0 : reinitializing the trust proba ...
Giving the final ranks ...

Final ranking for this environment #0 :
- Policy 'UCB(alpha: 0.5)'	was ranked	1 / 9 for this simulation (last regret = 341.57).
- Policy 'UCB(alpha: 2)'	was ranked	2 / 9 for this simulation (last regret = 360.32).
- Policy 'UCB(alpha: 1)'	was ranked	3 / 9 for this simulation (last regret = 363.82).
- Policy 'UCB(alpha: 3)'	was ranked	4 / 9 for this simulation (last regret = 445.82).
- Policy 'UCB(alpha: 0.25)'	was ranked	5 / 9 for this simulation (last regret = 461.32).
- Policy 'UCB(alpha: 4)'	was ranked	6 / 9 for this simulation (last regret = 472.07).
- Policy 'Aggr(nb: 8, dRate: auto)'	was ranked	7 / 9 for this simulation (last regret = 473.57).
- Policy 'UCB(alpha: 0.1)'	was ranked	8 / 9 for this simulation (last regret = 799.82).
- Policy 'UCB(alpha: 0.05)'	was ranked	9 / 9 for this simulation (last regret = 1192.57).
plots/T10000_N4__9_algos is already a directory here...
 - Plotting the cumulative rewards, and saving the plot to plots/T10000_N4__9_algos/main____env1-1_1109130098530808315.png ...
Saving to plots/T10000_N4__9_algos/main____env1-1_1109130098530808315.png ...
 - Plotting the mean rewards, and saving the plot to plots/T10000_N4__9_algos/main_MeanRewards____env1-1_1109130098530808315.png ...
Saving to plots/T10000_N4__9_algos/main_MeanRewards____env1-1_1109130098530808315.png ...
 - Plotting the mean rewards, and saving the plot to plots/T10000_N4__9_algos/main_Normalized_MeanRewards____env1-1_1109130098530808315.png ...
Saving to plots/T10000_N4__9_algos/main_Normalized_MeanRewards____env1-1_1109130098530808315.png ...
 - Not plotting probability of picking the best arm as we used random events ...
   ==> FIXME correct this bug
