 - Setting dpi of all figures to 110 ...
 - Setting 'figsize' of all figures to (19.8, 10.8) ...
Info: Using the regular tqdm() decorator ...
Info: numba.jit seems to be available.
Info: numba.jit seems to be available.
Loaded experiments configuration from 'configuration.py' :
configuration['policies'] = [{'params': {'alpha': 0.5}, 'archtype': <class 'Policies.UCBalpha.UCBalpha'>}, {'params': {'genuine': True}, 'archtype': <class 'Policies.DMED.DMED'>}, {'params': {'genuine': False}, 'archtype': <class 'Policies.DMED.DMED'>}, {'params': {}, 'archtype': <class 'Policies.Thompson.Thompson'>}, {'params': {'klucb': <built-in function klucbBern>}, 'archtype': <class 'Policies.klUCB.klUCB'>}, {'params': {'klucb': <built-in function klucbBern>}, 'archtype': <class 'Policies.klUCBPlus.klUCBPlus'>}, {'params': {}, 'archtype': <class 'Policies.BayesUCB.BayesUCB'>}]
====> TURNING DEBUG MODE ON <=====
plots/ is already a directory here...
Number of policies in this comparison: 7
Time horizon: 30000
Number of repetitions: 20
Sampling rate for saving, delta_t_save: 1
Sampling rate for plotting, delta_t_plot: 50
Number of jobs for parallelization: 4
Creating a new MAB problem ...
  Reading arms of this MAB problem from a dictionnary 'configuration' = {'params': [0.005, 0.01, 0.015, 0.02, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.78, 0.8, 0.82, 0.83, 0.84, 0.85], 'arm_type': <class 'Arms.Bernoulli.Bernoulli'>} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = [0.005, 0.01, 0.015, 0.02, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.78, 0.8, 0.82, 0.83, 0.84, 0.85]
 - with 'arms' = [B(0.005), B(0.01), B(0.015), B(0.02), B(0.3), B(0.35), B(0.4), B(0.45), B(0.5), B(0.55), B(0.6), B(0.78), B(0.8), B(0.82), B(0.83), B(0.84), B(0.85)]
 - with 'means' = [ 0.005  0.01   0.015  0.02   0.3    0.35   0.4    0.45   0.5    0.55   0.6
  0.78   0.8    0.82   0.83   0.84   0.85 ]
 - with 'nbArms' = 17
 - with 'maxArm' = 0.85
 - with 'minArm' = 0.005

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 66.4 ... 
 - a Optimal Arm Identification factor H_OI(mu) = 56.88% ...
Number of environments to try: 1

Evaluating environment: MAB(nbArms: 17, arms: [B(0.005), B(0.01), B(0.015), B(0.02), B(0.3), B(0.35), B(0.4), B(0.45), B(0.5), B(0.55), B(0.6), B(0.78), B(0.8), B(0.82), B(0.83), B(0.84), B(0.85)], minArm: 0.005, maxArm: 0.85)
- Adding policy #1 = {'params': {'alpha': 0.5}, 'archtype': <class 'Policies.UCBalpha.UCBalpha'>} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][0]' = {'params': {'alpha': 0.5}, 'archtype': <class 'Policies.UCBalpha.UCBalpha'>} ...
- Adding policy #2 = {'params': {'genuine': True}, 'archtype': <class 'Policies.DMED.DMED'>} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][1]' = {'params': {'genuine': True}, 'archtype': <class 'Policies.DMED.DMED'>} ...
- Adding policy #3 = {'params': {'genuine': False}, 'archtype': <class 'Policies.DMED.DMED'>} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][2]' = {'params': {'genuine': False}, 'archtype': <class 'Policies.DMED.DMED'>} ...
- Adding policy #4 = {'params': {}, 'archtype': <class 'Policies.Thompson.Thompson'>} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][3]' = {'params': {}, 'archtype': <class 'Policies.Thompson.Thompson'>} ...
- Adding policy #5 = {'params': {'klucb': <built-in function klucbBern>}, 'archtype': <class 'Policies.klUCB.klUCB'>} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][4]' = {'params': {'klucb': <built-in function klucbBern>}, 'archtype': <class 'Policies.klUCB.klUCB'>} ...
- Adding policy #6 = {'params': {'klucb': <built-in function klucbBern>}, 'archtype': <class 'Policies.klUCBPlus.klUCBPlus'>} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][5]' = {'params': {'klucb': <built-in function klucbBern>}, 'archtype': <class 'Policies.klUCBPlus.klUCBPlus'>} ...
- Adding policy #7 = {'params': {}, 'archtype': <class 'Policies.BayesUCB.BayesUCB'>} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][6]' = {'params': {}, 'archtype': <class 'Policies.BayesUCB.BayesUCB'>} ...

- Evaluating policy #1/7: UCB($\alpha=0.5$) ...

Estimated order by the policy UCB($\alpha=0.5$) after 30000 steps: [ 0  3  8  1  2  5  7  9 10  4  6 11 12 13 15 14 16] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 80.62% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 100.00% (relative success)...
  ==> Spearman    distance from optimal ordering: 100.00% (relative success)...
  ==> Gestalt     distance from optimal ordering: 70.59% (relative success)...
  ==> Mean distance from optimal ordering: 87.80% (relative success)...

- Evaluating policy #2/7: DMED+(n) ...

- Evaluating policy #3/7: DMED(n) ...

- Evaluating policy #4/7: Thompson ...

Estimated order by the policy Thompson after 30000 steps: [ 3  2  5  8  0  1  4  7  9  6 10 11 12 15 14 13 16] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 79.24% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 100.00% (relative success)...
  ==> Spearman    distance from optimal ordering: 100.00% (relative success)...
  ==> Gestalt     distance from optimal ordering: 58.82% (relative success)...
  ==> Mean distance from optimal ordering: 84.51% (relative success)...

- Evaluating policy #5/7: KL-UCB(Bern) ...

Estimated order by the policy KL-UCB(Bern) after 30000 steps: [ 0  1  2  7  3  4  5  8  6 10  9 13 14 12 11 16 15] ...
  ==> Optimal arm identification: 98.82% (relative success)...
  ==> Manhattan   distance from optimal ordering: 84.78% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 100.00% (relative success)...
  ==> Spearman    distance from optimal ordering: 100.00% (relative success)...
  ==> Gestalt     distance from optimal ordering: 64.71% (relative success)...
  ==> Mean distance from optimal ordering: 87.37% (relative success)...

- Evaluating policy #6/7: KL-UCB+(Bern) ...

Estimated order by the policy KL-UCB+(Bern) after 30000 steps: [ 0  1  2  3  5  6  4  7 10 12  8 13  9 14 16 15 11] ...
  ==> Optimal arm identification: 91.76% (relative success)...
  ==> Manhattan   distance from optimal ordering: 83.39% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 100.00% (relative success)...
  ==> Spearman    distance from optimal ordering: 100.00% (relative success)...
  ==> Gestalt     distance from optimal ordering: 70.59% (relative success)...
  ==> Mean distance from optimal ordering: 88.49% (relative success)...

- Evaluating policy #7/7: BayesUCB ...

Estimated order by the policy BayesUCB after 30000 steps: [ 0  1  2  3  4  6  9  8  5  7 10 12 13 14 15 11 16] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 87.54% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 100.00% (relative success)...
  ==> Spearman    distance from optimal ordering: 100.00% (relative success)...
  ==> Gestalt     distance from optimal ordering: 76.47% (relative success)...
  ==> Mean distance from optimal ordering: 91.00% (relative success)...
Giving the final ranks ...

Final ranking for this environment #0 :
- Policy 'KL-UCB+(Bern)'	was ranked	1 / 7 for this simulation (last regret = 206.05).
- Policy 'DMED+(n)'	was ranked	2 / 7 for this simulation (last regret = 213.9).
- Policy 'Thompson'	was ranked	3 / 7 for this simulation (last regret = 236.65).
- Policy 'UCB($\alpha=0.5$)'	was ranked	4 / 7 for this simulation (last regret = 260.75).
- Policy 'BayesUCB'	was ranked	5 / 7 for this simulation (last regret = 264.1).
- Policy 'KL-UCB(Bern)'	was ranked	6 / 7 for this simulation (last regret = 290.7).
- Policy 'DMED(n)'	was ranked	7 / 7 for this simulation (last regret = 525.4).

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 66.4 for 1-player problem... 
 - a Optimal Arm Identification factor H_OI(mu) = 56.88% ...

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 66.4 for 1-player problem... 
 - a Optimal Arm Identification factor H_OI(mu) = 56.88% ...

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 66.4 for 1-player problem... 
 - a Optimal Arm Identification factor H_OI(mu) = 56.88% ...

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 66.4 for 1-player problem... 
 - a Optimal Arm Identification factor H_OI(mu) = 56.88% ...

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 66.4 for 1-player problem... 
 - a Optimal Arm Identification factor H_OI(mu) = 56.88% ...
Done for simulations main.py ...
