Loaded experiments configuration from 'configuration.py' :
configuration['policies'] = [<Policies.TakeFixedArm.TakeFixedArm object at 0x7f98b6802160>, <Policies.TakeFixedArm.TakeFixedArm object at 0x7f98ca0408d0>, {'params': {}, 'archtype': <class 'Policies.Thompson.Thompson'>}, {'params': {}, 'archtype': <class 'Policies.klUCB.klUCB'>}, {'params': {}, 'archtype': <class 'Policies.BayesUCB.BayesUCB'>}, {'params': {'learningRate': 0.01, 'decreaseRate': 5000.0, 'update_all_children': True, 'children': [<Policies.TakeFixedArm.TakeFixedArm object at 0x7f98b6802160>, <Policies.TakeFixedArm.TakeFixedArm object at 0x7f98ca0408d0>, {'params': {}, 'archtype': <class 'Policies.Thompson.Thompson'>}, {'params': {}, 'archtype': <class 'Policies.klUCB.klUCB'>}, {'params': {}, 'archtype': <class 'Policies.BayesUCB.BayesUCB'>}]}, 'archtype': <class 'Policies.Aggr.Aggr'>}]
plots is already a directory here...
Number of policies in this comparaison: 6
Time horizon: 10000
Number of repetitions: 20
Creating a new MAB problem ...
  Reading arms of this MAB problem from a dictionnary 'configuration' = {'params': [0.005, 0.01, 0.015, 0.02, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.78, 0.8, 0.82, 0.83, 0.84, 0.85], 'arm_type': <class 'Arms.Bernoulli.Bernoulli'>} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = [0.005, 0.01, 0.015, 0.02, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.78, 0.8, 0.82, 0.83, 0.84, 0.85]
 - with 'arms' = [B(0.005), B(0.01), B(0.015), B(0.02), B(0.3), B(0.35), B(0.4), B(0.45), B(0.5), B(0.55), B(0.6), B(0.78), B(0.8), B(0.82), B(0.83), B(0.84), B(0.85)]
 - with 'nbArms' = 17
 - with 'maxArm' = 0.85
Number of environments to try: 1

Evaluating environment: <MAB{'maxArm': 0.85, 'nbArms': 17, 'arms': [B(0.005), B(0.01), B(0.015), B(0.02), B(0.3), B(0.35), B(0.4), B(0.45), B(0.5), B(0.55), B(0.6), B(0.78), B(0.8), B(0.82), B(0.83), B(0.84), B(0.85)]}>
- Adding policy #1 = TakeFixedArm(16) ...
  Using this already created policy 'self.cfg['policies'][0]' = TakeFixedArm(16) ...
- Adding policy #2 = TakeFixedArm(15) ...
  Using this already created policy 'self.cfg['policies'][1]' = TakeFixedArm(15) ...
- Adding policy #3 = {'params': {}, 'archtype': <class 'Policies.Thompson.Thompson'>} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][2]' = {'params': {}, 'archtype': <class 'Policies.Thompson.Thompson'>} ...
- Adding policy #4 = {'params': {}, 'archtype': <class 'Policies.klUCB.klUCB'>} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][3]' = {'params': {}, 'archtype': <class 'Policies.klUCB.klUCB'>} ...
- Adding policy #5 = {'params': {}, 'archtype': <class 'Policies.BayesUCB.BayesUCB'>} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][4]' = {'params': {}, 'archtype': <class 'Policies.BayesUCB.BayesUCB'>} ...
- Adding policy #6 = {'params': {'learningRate': 0.01, 'decreaseRate': 5000.0, 'update_all_children': True, 'children': [<Policies.TakeFixedArm.TakeFixedArm object at 0x7f98b6802160>, <Policies.TakeFixedArm.TakeFixedArm object at 0x7f98ca0408d0>, {'params': {}, 'archtype': <class 'Policies.Thompson.Thompson'>}, {'params': {}, 'archtype': <class 'Policies.klUCB.klUCB'>}, {'params': {}, 'archtype': <class 'Policies.BayesUCB.BayesUCB'>}]}, 'archtype': <class 'Policies.Aggr.Aggr'>} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][5]' = {'params': {'learningRate': 0.01, 'decreaseRate': 5000.0, 'update_all_children': True, 'children': [<Policies.TakeFixedArm.TakeFixedArm object at 0x7f98b6802160>, <Policies.TakeFixedArm.TakeFixedArm object at 0x7f98ca0408d0>, {'params': {}, 'archtype': <class 'Policies.Thompson.Thompson'>}, {'params': {}, 'archtype': <class 'Policies.klUCB.klUCB'>}, {'params': {}, 'archtype': <class 'Policies.BayesUCB.BayesUCB'>}]}, 'archtype': <class 'Policies.Aggr.Aggr'>} ...
  Using this already created player 'children[0]' = TakeFixedArm(16) ...
  Using this already created player 'children[1]' = TakeFixedArm(15) ...
  Creating this child player from a dictionnary 'children[2]' = {'params': {}, 'archtype': <class 'Policies.Thompson.Thompson'>} ...
  Creating this child player from a dictionnary 'children[3]' = {'params': {}, 'archtype': <class 'Policies.klUCB.klUCB'>} ...
  Creating this child player from a dictionnary 'children[4]' = {'params': {}, 'archtype': <class 'Policies.BayesUCB.BayesUCB'>} ...

- Evaluating policy #1/6: TakeFixedArm(16) ...

- Evaluating policy #2/6: TakeFixedArm(15) ...

- Evaluating policy #3/6: Thompson ...

- Evaluating policy #4/6: klUCB ...

- Evaluating policy #5/6: BayesUCB ...

- Evaluating policy #6/6: Aggr (nb:5, rate:0.01) ...
Giving the final ranks ...

Final ranking for this environment #0 :
- Policy 'TakeFixedArm(16)'	was ranked	1 / 6 for this simulation (last regret = -7.5).
- Policy 'TakeFixedArm(15)'	was ranked	2 / 6 for this simulation (last regret = 83.05).
- Policy 'Aggr (nb:5, rate:0.01)'	was ranked	3 / 6 for this simulation (last regret = 119.2).
- Policy 'Thompson'	was ranked	4 / 6 for this simulation (last regret = 134.6).
- Policy 'BayesUCB'	was ranked	5 / 6 for this simulation (last regret = 165.25).
- Policy 'klUCB'	was ranked	6 / 6 for this simulation (last regret = 197.45).
plots/T10000_N20__6_algos is already a directory here...
  - stdY = [  2.17944947e-02   6.17944947e-02   1.05095765e-01 ...,   3.33060335e+02
   3.33096042e+02   3.33139344e+02]
  - np.shape(stdY) = (10000,)
  - np.min(stdY) = 0.0217944947177
  - np.max(stdY) = 333.139343534
  - stdY = [  3.00000000e-02   6.00000000e-02   1.03301270e-01 ...,   3.42664135e+02
   3.42709961e+02   3.42745668e+02]
  - np.shape(stdY) = (10000,)
  - np.min(stdY) = 0.03
  - np.max(stdY) = 342.745667845
  - stdY = [  4.97493719e-02   9.55751288e-02   1.43272089e-01 ...,   3.47541474e+02
   3.47581474e+02   3.47624776e+02]
  - np.shape(stdY) = (10000,)
  - np.min(stdY) = 0.0497493718553
  - np.max(stdY) = 347.624775737
  - stdY = [  4.89897949e-02   9.66867549e-02   1.45676550e-01 ...,   3.53596617e+02
   3.53626617e+02   3.53656617e+02]
  - np.shape(stdY) = (10000,)
  - np.min(stdY) = 0.0489897948557
  - np.max(stdY) = 353.656617109
  - stdY = [  5.00000000e-02   9.97493719e-02   1.49498744e-01 ...,   3.50732818e+02
   3.50754613e+02   3.50802310e+02]
  - np.shape(stdY) = (10000,)
  - np.min(stdY) = 0.05
  - np.max(stdY) = 350.802309863
  - stdY = [  4.76969601e-02   9.74463319e-02   1.43272089e-01 ...,   3.46025360e+02
   3.46047154e+02   3.46096144e+02]
  - np.shape(stdY) = (10000,)
  - np.min(stdY) = 0.0476969600708
  - np.max(stdY) = 346.096143881
  - stdY = [ 0.02179449  0.03221025  0.0362859  ...,  0.03473725  0.03473735
  0.03473831]
  - np.shape(stdY) = (10000,)
  - np.min(stdY) = 0.0217944947177
  - np.max(stdY) = 0.0362859017618
  - stdY = [ 0.03        0.03        0.035      ...,  0.03557577  0.03557695
  0.03557696]
  - np.shape(stdY) = (10000,)
  - np.min(stdY) = 0.03
  - np.max(stdY) = 0.0374026514212
  - stdY = [ 0.04974937  0.04782782  0.04778424 ...,  0.03600058  0.036001    0.03600181]
  - np.shape(stdY) = (10000,)
  - np.min(stdY) = 0.0359937659746
  - np.max(stdY) = 0.0497493718553
  - stdY = [ 0.04898979  0.0483477   0.04856267 ...,  0.03651817  0.03651758
  0.03651698]
  - np.shape(stdY) = (10000,)
  - np.min(stdY) = 0.0365169823507
  - np.max(stdY) = 0.0490917508345
  - stdY = [ 0.05        0.04987484  0.04983305 ...,  0.03628751  0.03628635
  0.03628767]
  - np.shape(stdY) = (10000,)
  - np.min(stdY) = 0.0362800108501
  - np.max(stdY) = 0.05
  - stdY = [ 0.04769696  0.04873397  0.04778424 ...,  0.03587962  0.03587849
  0.03588004]
  - np.shape(stdY) = (10000,)
  - np.min(stdY) = 0.0358724858327
  - np.max(stdY) = 0.048733971724
