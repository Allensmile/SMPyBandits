Loaded experiments configuration from 'configuration.py' :
configuration['policies'] = [{'archtype': <class 'Policies.Aggr.Aggr'>, 'params': {'children': [OneScenario1, OneScenario1, OneScenario1, OneScenario1, OneScenario1, OneScenario1, OneScenario1, OneScenario1, OneScenario1, OneScenario1], 'unbiased': False, 'decreaseRate': 'auto', 'learningRate': 0.01, 'update_like_exp4': True, 'update_all_children': False}}, {'archtype': <class 'Policies.Aggr.Aggr'>, 'params': {'children': [OneScenario1, OneScenario1, OneScenario1, OneScenario1, OneScenario1, OneScenario1, OneScenario1, OneScenario1, OneScenario1, OneScenario1], 'unbiased': False, 'decreaseRate': 'auto', 'learningRate': 0.01, 'update_like_exp4': False, 'update_all_children': False}}, OneScenario1, OneScenario1, OneScenario1, OneScenario1, OneScenario1, OneScenario1, OneScenario1, OneScenario1, OneScenario1, OneScenario1]
plots/ is already a directory here...
Number of policies in this comparaison: 12
Time horizon: 10000
Number of repetitions: 1000
Sampling rate DELTA_T_SAVE: 1
Creating a new MAB problem ...
  Reading arms of this MAB problem from a dictionnary 'configuration' = {'params': [0, 1], 'arm_type': <class 'Arms.Bernoulli.Bernoulli'>} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = [0, 1]
 - with 'arms' = [B(0), B(1)]
 - with 'nbArms' = 2
 - with 'maxArm' = 1
 - with 'minArm' = 0
Number of environments to try: 1

Evaluating environment: <MAB{'maxArm': 1, 'nbArms': 2, 'minArm': 0, 'arms': [B(0), B(1)]}>
- Adding policy #1 = {'archtype': <class 'Policies.Aggr.Aggr'>, 'params': {'children': [OneScenario1, OneScenario1, OneScenario1, OneScenario1, OneScenario1, OneScenario1, OneScenario1, OneScenario1, OneScenario1, OneScenario1], 'unbiased': False, 'decreaseRate': 'auto', 'learningRate': 0.01, 'update_like_exp4': True, 'update_all_children': False}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][0]' = {'archtype': <class 'Policies.Aggr.Aggr'>, 'params': {'children': [OneScenario1, OneScenario1, OneScenario1, OneScenario1, OneScenario1, OneScenario1, OneScenario1, OneScenario1, OneScenario1, OneScenario1], 'unbiased': False, 'decreaseRate': 'auto', 'learningRate': 0.01, 'update_like_exp4': True, 'update_all_children': False}} ...
  Using this already created player 'children[0]' = #1<OneScenario1> ...
  Using this already created player 'children[1]' = #2<OneScenario1> ...
  Using this already created player 'children[2]' = #3<OneScenario1> ...
  Using this already created player 'children[3]' = #4<OneScenario1> ...
  Using this already created player 'children[4]' = #5<OneScenario1> ...
  Using this already created player 'children[5]' = #6<OneScenario1> ...
  Using this already created player 'children[6]' = #7<OneScenario1> ...
  Using this already created player 'children[7]' = #8<OneScenario1> ...
  Using this already created player 'children[8]' = #9<OneScenario1> ...
  Using this already created player 'children[9]' = #10<OneScenario1> ...
- Adding policy #2 = {'archtype': <class 'Policies.Aggr.Aggr'>, 'params': {'children': [OneScenario1, OneScenario1, OneScenario1, OneScenario1, OneScenario1, OneScenario1, OneScenario1, OneScenario1, OneScenario1, OneScenario1], 'unbiased': False, 'decreaseRate': 'auto', 'learningRate': 0.01, 'update_like_exp4': False, 'update_all_children': False}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][1]' = {'archtype': <class 'Policies.Aggr.Aggr'>, 'params': {'children': [OneScenario1, OneScenario1, OneScenario1, OneScenario1, OneScenario1, OneScenario1, OneScenario1, OneScenario1, OneScenario1, OneScenario1], 'unbiased': False, 'decreaseRate': 'auto', 'learningRate': 0.01, 'update_like_exp4': False, 'update_all_children': False}} ...
  Using this already created player 'children[0]' = #1<OneScenario1> ...
  Using this already created player 'children[1]' = #2<OneScenario1> ...
  Using this already created player 'children[2]' = #3<OneScenario1> ...
  Using this already created player 'children[3]' = #4<OneScenario1> ...
  Using this already created player 'children[4]' = #5<OneScenario1> ...
  Using this already created player 'children[5]' = #6<OneScenario1> ...
  Using this already created player 'children[6]' = #7<OneScenario1> ...
  Using this already created player 'children[7]' = #8<OneScenario1> ...
  Using this already created player 'children[8]' = #9<OneScenario1> ...
  Using this already created player 'children[9]' = #10<OneScenario1> ...
- Adding policy #3 = #1<OneScenario1> ...
  Using this already created policy 'self.cfg['policies'][2]' = #1<OneScenario1> ...
- Adding policy #4 = #2<OneScenario1> ...
  Using this already created policy 'self.cfg['policies'][3]' = #2<OneScenario1> ...
- Adding policy #5 = #3<OneScenario1> ...
  Using this already created policy 'self.cfg['policies'][4]' = #3<OneScenario1> ...
- Adding policy #6 = #4<OneScenario1> ...
  Using this already created policy 'self.cfg['policies'][5]' = #4<OneScenario1> ...
- Adding policy #7 = #5<OneScenario1> ...
  Using this already created policy 'self.cfg['policies'][6]' = #5<OneScenario1> ...
- Adding policy #8 = #6<OneScenario1> ...
  Using this already created policy 'self.cfg['policies'][7]' = #6<OneScenario1> ...
- Adding policy #9 = #7<OneScenario1> ...
  Using this already created policy 'self.cfg['policies'][8]' = #7<OneScenario1> ...
- Adding policy #10 = #8<OneScenario1> ...
  Using this already created policy 'self.cfg['policies'][9]' = #8<OneScenario1> ...
- Adding policy #11 = #9<OneScenario1> ...
  Using this already created policy 'self.cfg['policies'][10]' = #9<OneScenario1> ...
- Adding policy #12 = #10<OneScenario1> ...
  Using this already created policy 'self.cfg['policies'][11]' = #10<OneScenario1> ...

===> Pre-computing the rewards ... Of shape (2, 10000) ...
    In order for all simulated algorithms to face the same random rewards (robust comparaison of A1,..,An vs Aggr(A1,..,An)) ...


- Evaluating policy #1/12: Aggr($N=10$, Exp4) ...

- Evaluating policy #2/12: Aggr($N=10$) ...

- Evaluating policy #3/12: #1<OneScenario1> ...

- Evaluating policy #4/12: #2<OneScenario1> ...

- Evaluating policy #5/12: #3<OneScenario1> ...

- Evaluating policy #6/12: #4<OneScenario1> ...

- Evaluating policy #7/12: #5<OneScenario1> ...

- Evaluating policy #8/12: #6<OneScenario1> ...

- Evaluating policy #9/12: #7<OneScenario1> ...

- Evaluating policy #10/12: #8<OneScenario1> ...

- Evaluating policy #11/12: #9<OneScenario1> ...

- Evaluating policy #12/12: #10<OneScenario1> ...
Giving the final ranks ...

Final ranking for this environment #0 :
- Policy '#4<OneScenario1>'	was ranked	1 / 12 for this simulation (last regret = 841.585).
- Policy '#6<OneScenario1>'	was ranked	2 / 12 for this simulation (last regret = 851.486).
- Policy '#3<OneScenario1>'	was ranked	3 / 12 for this simulation (last regret = 891.09).
- Policy '#2<OneScenario1>'	was ranked	4 / 12 for this simulation (last regret = 900.991).
- Policy '#5<OneScenario1>'	was ranked	5 / 12 for this simulation (last regret = 900.991).
- Policy '#8<OneScenario1>'	was ranked	6 / 12 for this simulation (last regret = 960.397).
- Policy '#9<OneScenario1>'	was ranked	7 / 12 for this simulation (last regret = 960.397).
- Policy 'Aggr($N=10$, Exp4)'	was ranked	8 / 12 for this simulation (last regret = 988.79).
- Policy 'Aggr($N=10$)'	was ranked	9 / 12 for this simulation (last regret = 989.816).
- Policy '#1<OneScenario1>'	was ranked	10 / 12 for this simulation (last regret = 990.467).
- Policy '#10<OneScenario1>'	was ranked	11 / 12 for this simulation (last regret = 1009.9).
- Policy '#7<OneScenario1>'	was ranked	12 / 12 for this simulation (last regret = 1158.42).
