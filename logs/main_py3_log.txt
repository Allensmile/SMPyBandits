 - Setting dpi of all figures to 110 ...
 - Setting 'figsize' of all figures to (19.8, 10.8) ...
Info: numba.jit seems to be available.
Info: numba.jit seems to be available.
Loaded experiments configuration from 'configuration.py' :
configuration['policies'] = [{'archtype': <class 'Policies.Aggr.Aggr'>, 'params': {'children': [{'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 1}}, {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.5}}, {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.1}}, {'archtype': <class 'Policies.MOSS.MOSS'>, 'params': {}}, {'archtype': <class 'Policies.Thompson.Thompson'>, 'params': {}}, {'archtype': <class 'Policies.klUCB.klUCB'>, 'params': {'klucb': <built-in function klucbBern>}}, {'archtype': <class 'Policies.klUCBPlus.klUCBPlus'>, 'params': {'klucb': <built-in function klucbBern>}}, {'archtype': <class 'Policies.klUCBHPlus.klUCBHPlus'>, 'params': {'horizon': 5000, 'klucb': <built-in function klucbBern>}}, {'archtype': <class 'Policies.klUCBPlusPlus.klUCBPlusPlus'>, 'params': {'horizon': 5000, 'klucb': <built-in function klucbBern>}}, {'archtype': <class 'Policies.BayesUCB.BayesUCB'>, 'params': {}}], 'update_all_children': False, 'update_like_exp4': True, 'learningRate': 0.01, 'decreaseRate': 'auto', 'unbiased': False}}, {'archtype': <class 'Policies.Aggr.Aggr'>, 'params': {'children': [{'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 1}}, {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.5}}, {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.1}}, {'archtype': <class 'Policies.MOSS.MOSS'>, 'params': {}}, {'archtype': <class 'Policies.Thompson.Thompson'>, 'params': {}}, {'archtype': <class 'Policies.klUCB.klUCB'>, 'params': {'klucb': <built-in function klucbBern>}}, {'archtype': <class 'Policies.klUCBPlus.klUCBPlus'>, 'params': {'klucb': <built-in function klucbBern>}}, {'archtype': <class 'Policies.klUCBHPlus.klUCBHPlus'>, 'params': {'horizon': 5000, 'klucb': <built-in function klucbBern>}}, {'archtype': <class 'Policies.klUCBPlusPlus.klUCBPlusPlus'>, 'params': {'horizon': 5000, 'klucb': <built-in function klucbBern>}}, {'archtype': <class 'Policies.BayesUCB.BayesUCB'>, 'params': {}}], 'update_all_children': False, 'update_like_exp4': False, 'learningRate': 0.01, 'decreaseRate': 'auto', 'unbiased': False}}, {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 1}}, {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.5}}, {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.1}}, {'archtype': <class 'Policies.MOSS.MOSS'>, 'params': {}}, {'archtype': <class 'Policies.Thompson.Thompson'>, 'params': {}}, {'archtype': <class 'Policies.klUCB.klUCB'>, 'params': {'klucb': <built-in function klucbBern>}}, {'archtype': <class 'Policies.klUCBPlus.klUCBPlus'>, 'params': {'klucb': <built-in function klucbBern>}}, {'archtype': <class 'Policies.klUCBHPlus.klUCBHPlus'>, 'params': {'horizon': 5000, 'klucb': <built-in function klucbBern>}}, {'archtype': <class 'Policies.klUCBPlusPlus.klUCBPlusPlus'>, 'params': {'horizon': 5000, 'klucb': <built-in function klucbBern>}}, {'archtype': <class 'Policies.BayesUCB.BayesUCB'>, 'params': {}}]
plots/ is already a directory here...
Number of policies in this comparaison: 12
Time horizon: 5000
Number of repetitions: 4
Sampling rate DELTA_T_SAVE: 1
Creating a new MAB problem ...
  Reading arms of this MAB problem from a dictionnary 'configuration' = {'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]
 - with 'arms' = [B(0.1), B(0.2), B(0.3), B(0.4), B(0.5), B(0.6), B(0.7), B(0.8), B(0.9)]
 - with 'nbArms' = 9
 - with 'maxArm' = 0.9
 - with 'minArm' = 0.1

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 7.52 ... 
 - a Optimal Arm Identification factor H_OI(mu) = 60.00% ...
Number of environments to try: 1

Evaluating environment: <MAB{'nbArms': 9, 'arms': [B(0.1), B(0.2), B(0.3), B(0.4), B(0.5), B(0.6), B(0.7), B(0.8), B(0.9)], 'maxArm': 0.90000000000000002, 'minArm': 0.10000000000000001}>
- Adding policy #1 = {'archtype': <class 'Policies.Aggr.Aggr'>, 'params': {'children': [{'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 1}}, {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.5}}, {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.1}}, {'archtype': <class 'Policies.MOSS.MOSS'>, 'params': {}}, {'archtype': <class 'Policies.Thompson.Thompson'>, 'params': {}}, {'archtype': <class 'Policies.klUCB.klUCB'>, 'params': {'klucb': <built-in function klucbBern>}}, {'archtype': <class 'Policies.klUCBPlus.klUCBPlus'>, 'params': {'klucb': <built-in function klucbBern>}}, {'archtype': <class 'Policies.klUCBHPlus.klUCBHPlus'>, 'params': {'horizon': 5000, 'klucb': <built-in function klucbBern>}}, {'archtype': <class 'Policies.klUCBPlusPlus.klUCBPlusPlus'>, 'params': {'horizon': 5000, 'klucb': <built-in function klucbBern>}}, {'archtype': <class 'Policies.BayesUCB.BayesUCB'>, 'params': {}}], 'update_all_children': False, 'update_like_exp4': True, 'learningRate': 0.01, 'decreaseRate': 'auto', 'unbiased': False}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][0]' = {'archtype': <class 'Policies.Aggr.Aggr'>, 'params': {'children': [{'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 1}}, {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.5}}, {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.1}}, {'archtype': <class 'Policies.MOSS.MOSS'>, 'params': {}}, {'archtype': <class 'Policies.Thompson.Thompson'>, 'params': {}}, {'archtype': <class 'Policies.klUCB.klUCB'>, 'params': {'klucb': <built-in function klucbBern>}}, {'archtype': <class 'Policies.klUCBPlus.klUCBPlus'>, 'params': {'klucb': <built-in function klucbBern>}}, {'archtype': <class 'Policies.klUCBHPlus.klUCBHPlus'>, 'params': {'horizon': 5000, 'klucb': <built-in function klucbBern>}}, {'archtype': <class 'Policies.klUCBPlusPlus.klUCBPlusPlus'>, 'params': {'horizon': 5000, 'klucb': <built-in function klucbBern>}}, {'archtype': <class 'Policies.BayesUCB.BayesUCB'>, 'params': {}}], 'update_all_children': False, 'update_like_exp4': True, 'learningRate': 0.01, 'decreaseRate': 'auto', 'unbiased': False}} ...
  Creating this child player from a dictionnary 'children[0]' = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 1}} ...
  Creating this child player from a dictionnary 'children[1]' = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.5}} ...
  Creating this child player from a dictionnary 'children[2]' = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.1}} ...
  Creating this child player from a dictionnary 'children[3]' = {'archtype': <class 'Policies.MOSS.MOSS'>, 'params': {}} ...
  Creating this child player from a dictionnary 'children[4]' = {'archtype': <class 'Policies.Thompson.Thompson'>, 'params': {}} ...
  Creating this child player from a dictionnary 'children[5]' = {'archtype': <class 'Policies.klUCB.klUCB'>, 'params': {'klucb': <built-in function klucbBern>}} ...
  Creating this child player from a dictionnary 'children[6]' = {'archtype': <class 'Policies.klUCBPlus.klUCBPlus'>, 'params': {'klucb': <built-in function klucbBern>}} ...
  Creating this child player from a dictionnary 'children[7]' = {'archtype': <class 'Policies.klUCBHPlus.klUCBHPlus'>, 'params': {'horizon': 5000, 'klucb': <built-in function klucbBern>}} ...
  Creating this child player from a dictionnary 'children[8]' = {'archtype': <class 'Policies.klUCBPlusPlus.klUCBPlusPlus'>, 'params': {'horizon': 5000, 'klucb': <built-in function klucbBern>}} ...
  Creating this child player from a dictionnary 'children[9]' = {'archtype': <class 'Policies.BayesUCB.BayesUCB'>, 'params': {}} ...
- Adding policy #2 = {'archtype': <class 'Policies.Aggr.Aggr'>, 'params': {'children': [{'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 1}}, {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.5}}, {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.1}}, {'archtype': <class 'Policies.MOSS.MOSS'>, 'params': {}}, {'archtype': <class 'Policies.Thompson.Thompson'>, 'params': {}}, {'archtype': <class 'Policies.klUCB.klUCB'>, 'params': {'klucb': <built-in function klucbBern>}}, {'archtype': <class 'Policies.klUCBPlus.klUCBPlus'>, 'params': {'klucb': <built-in function klucbBern>}}, {'archtype': <class 'Policies.klUCBHPlus.klUCBHPlus'>, 'params': {'horizon': 5000, 'klucb': <built-in function klucbBern>}}, {'archtype': <class 'Policies.klUCBPlusPlus.klUCBPlusPlus'>, 'params': {'horizon': 5000, 'klucb': <built-in function klucbBern>}}, {'archtype': <class 'Policies.BayesUCB.BayesUCB'>, 'params': {}}], 'update_all_children': False, 'update_like_exp4': False, 'learningRate': 0.01, 'decreaseRate': 'auto', 'unbiased': False}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][1]' = {'archtype': <class 'Policies.Aggr.Aggr'>, 'params': {'children': [{'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 1}}, {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.5}}, {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.1}}, {'archtype': <class 'Policies.MOSS.MOSS'>, 'params': {}}, {'archtype': <class 'Policies.Thompson.Thompson'>, 'params': {}}, {'archtype': <class 'Policies.klUCB.klUCB'>, 'params': {'klucb': <built-in function klucbBern>}}, {'archtype': <class 'Policies.klUCBPlus.klUCBPlus'>, 'params': {'klucb': <built-in function klucbBern>}}, {'archtype': <class 'Policies.klUCBHPlus.klUCBHPlus'>, 'params': {'horizon': 5000, 'klucb': <built-in function klucbBern>}}, {'archtype': <class 'Policies.klUCBPlusPlus.klUCBPlusPlus'>, 'params': {'horizon': 5000, 'klucb': <built-in function klucbBern>}}, {'archtype': <class 'Policies.BayesUCB.BayesUCB'>, 'params': {}}], 'update_all_children': False, 'update_like_exp4': False, 'learningRate': 0.01, 'decreaseRate': 'auto', 'unbiased': False}} ...
  Creating this child player from a dictionnary 'children[0]' = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 1}} ...
  Creating this child player from a dictionnary 'children[1]' = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.5}} ...
  Creating this child player from a dictionnary 'children[2]' = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.1}} ...
  Creating this child player from a dictionnary 'children[3]' = {'archtype': <class 'Policies.MOSS.MOSS'>, 'params': {}} ...
  Creating this child player from a dictionnary 'children[4]' = {'archtype': <class 'Policies.Thompson.Thompson'>, 'params': {}} ...
  Creating this child player from a dictionnary 'children[5]' = {'archtype': <class 'Policies.klUCB.klUCB'>, 'params': {'klucb': <built-in function klucbBern>}} ...
  Creating this child player from a dictionnary 'children[6]' = {'archtype': <class 'Policies.klUCBPlus.klUCBPlus'>, 'params': {'klucb': <built-in function klucbBern>}} ...
  Creating this child player from a dictionnary 'children[7]' = {'archtype': <class 'Policies.klUCBHPlus.klUCBHPlus'>, 'params': {'horizon': 5000, 'klucb': <built-in function klucbBern>}} ...
  Creating this child player from a dictionnary 'children[8]' = {'archtype': <class 'Policies.klUCBPlusPlus.klUCBPlusPlus'>, 'params': {'horizon': 5000, 'klucb': <built-in function klucbBern>}} ...
  Creating this child player from a dictionnary 'children[9]' = {'archtype': <class 'Policies.BayesUCB.BayesUCB'>, 'params': {}} ...
- Adding policy #3 = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 1}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][2]' = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 1}} ...
- Adding policy #4 = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.5}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][3]' = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.5}} ...
- Adding policy #5 = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.1}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][4]' = {'archtype': <class 'Policies.UCBalpha.UCBalpha'>, 'params': {'alpha': 0.1}} ...
- Adding policy #6 = {'archtype': <class 'Policies.MOSS.MOSS'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][5]' = {'archtype': <class 'Policies.MOSS.MOSS'>, 'params': {}} ...
- Adding policy #7 = {'archtype': <class 'Policies.Thompson.Thompson'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][6]' = {'archtype': <class 'Policies.Thompson.Thompson'>, 'params': {}} ...
- Adding policy #8 = {'archtype': <class 'Policies.klUCB.klUCB'>, 'params': {'klucb': <built-in function klucbBern>}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][7]' = {'archtype': <class 'Policies.klUCB.klUCB'>, 'params': {'klucb': <built-in function klucbBern>}} ...
- Adding policy #9 = {'archtype': <class 'Policies.klUCBPlus.klUCBPlus'>, 'params': {'klucb': <built-in function klucbBern>}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][8]' = {'archtype': <class 'Policies.klUCBPlus.klUCBPlus'>, 'params': {'klucb': <built-in function klucbBern>}} ...
- Adding policy #10 = {'archtype': <class 'Policies.klUCBHPlus.klUCBHPlus'>, 'params': {'horizon': 5000, 'klucb': <built-in function klucbBern>}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][9]' = {'archtype': <class 'Policies.klUCBHPlus.klUCBHPlus'>, 'params': {'horizon': 5000, 'klucb': <built-in function klucbBern>}} ...
- Adding policy #11 = {'archtype': <class 'Policies.klUCBPlusPlus.klUCBPlusPlus'>, 'params': {'horizon': 5000, 'klucb': <built-in function klucbBern>}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][10]' = {'archtype': <class 'Policies.klUCBPlusPlus.klUCBPlusPlus'>, 'params': {'horizon': 5000, 'klucb': <built-in function klucbBern>}} ...
- Adding policy #12 = {'archtype': <class 'Policies.BayesUCB.BayesUCB'>, 'params': {}} ...
  Creating this policy from a dictionnary 'self.cfg['policies'][11]' = {'archtype': <class 'Policies.BayesUCB.BayesUCB'>, 'params': {}} ...

===> Pre-computing the rewards ... Of shape (9, 4, 5000) ...
    In order for all simulated algorithms to face the same random rewards (robust comparaison of A1,..,An vs Aggr(A1,..,An)) ...


- Evaluating policy #1/12: Aggr($N=10$, Exp4) ...

- Evaluating policy #2/12: Aggr($N=10$) ...

- Evaluating policy #3/12: UCB($\alpha=1$) ...

Estimated order by the policy UCB($\alpha=1$) after 5000 steps: [3 4 2 5 0 1 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 60.49% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 90.47% (relative success)...
  ==> Spearman    distance from optimal ordering: 87.50% (relative success)...
  ==> Gestalt     distance from optimal ordering: 66.67% (relative success)...
  ==> Mean distance from optimal ordering: 76.28% (relative success)...

- Evaluating policy #4/12: UCB($\alpha=0.5$) ...

Estimated order by the policy UCB($\alpha=0.5$) after 5000 steps: [0 1 3 2 5 7 4 6 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 80.25% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 99.65% (relative success)...
  ==> Spearman    distance from optimal ordering: 99.91% (relative success)...
  ==> Gestalt     distance from optimal ordering: 66.67% (relative success)...
  ==> Mean distance from optimal ordering: 86.62% (relative success)...

- Evaluating policy #5/12: UCB($\alpha=0.1$) ...

Estimated order by the policy UCB($\alpha=0.1$) after 5000 steps: [0 2 3 5 6 1 7 4 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 65.43% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 97.82% (relative success)...
  ==> Spearman    distance from optimal ordering: 96.42% (relative success)...
  ==> Gestalt     distance from optimal ordering: 77.78% (relative success)...
  ==> Mean distance from optimal ordering: 84.36% (relative success)...

- Evaluating policy #6/12: MOSS ...

Estimated order by the policy MOSS after 5000 steps: [4 6 2 5 7 3 0 1 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 30.86% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 16.52% (relative success)...
  ==> Spearman    distance from optimal ordering: 16.88% (relative success)...
  ==> Gestalt     distance from optimal ordering: 33.33% (relative success)...
  ==> Mean distance from optimal ordering: 24.40% (relative success)...

- Evaluating policy #7/12: Thompson ...

Estimated order by the policy Thompson after 5000 steps: [2 0 1 3 5 4 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 85.19% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 99.82% (relative success)...
  ==> Spearman    distance from optimal ordering: 99.98% (relative success)...
  ==> Gestalt     distance from optimal ordering: 77.78% (relative success)...
  ==> Mean distance from optimal ordering: 90.69% (relative success)...

- Evaluating policy #8/12: KL-UCB(Bern) ...

Estimated order by the policy KL-UCB(Bern) after 5000 steps: [1 0 2 3 4 5 7 6 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 90.12% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 99.92% (relative success)...
  ==> Spearman    distance from optimal ordering: 100.00% (relative success)...
  ==> Gestalt     distance from optimal ordering: 77.78% (relative success)...
  ==> Mean distance from optimal ordering: 91.95% (relative success)...

- Evaluating policy #9/12: KL-UCB+(Bern) ...

Estimated order by the policy KL-UCB+(Bern) after 5000 steps: [3 0 2 6 7 5 1 4 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 55.56% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 78.91% (relative success)...
  ==> Spearman    distance from optimal ordering: 81.25% (relative success)...
  ==> Gestalt     distance from optimal ordering: 44.44% (relative success)...
  ==> Mean distance from optimal ordering: 65.04% (relative success)...

- Evaluating policy #10/12: KL-UCB-H+(Bern) ...

Estimated order by the policy KL-UCB-H+(Bern) after 5000 steps: [0 2 7 1 4 3 6 5 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 70.37% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 96.29% (relative success)...
  ==> Spearman    distance from optimal ordering: 95.76% (relative success)...
  ==> Gestalt     distance from optimal ordering: 44.44% (relative success)...
  ==> Mean distance from optimal ordering: 76.72% (relative success)...

- Evaluating policy #11/12: KL-UCB++(Bern) ...

Estimated order by the policy KL-UCB++(Bern) after 5000 steps: [1 3 4 2 5 6 0 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 65.43% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 96.29% (relative success)...
  ==> Spearman    distance from optimal ordering: 91.24% (relative success)...
  ==> Gestalt     distance from optimal ordering: 77.78% (relative success)...
  ==> Mean distance from optimal ordering: 82.69% (relative success)...

- Evaluating policy #12/12: BayesUCB ...

Estimated order by the policy BayesUCB after 5000 steps: [1 4 0 2 5 3 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 75.31% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 99.33% (relative success)...
  ==> Spearman    distance from optimal ordering: 99.47% (relative success)...
  ==> Gestalt     distance from optimal ordering: 66.67% (relative success)...
  ==> Mean distance from optimal ordering: 85.19% (relative success)...
Giving the final ranks ...

Final ranking for this environment #0 :
- Policy 'UCB($\alpha=0.1$)'	was ranked	1 / 12 for this simulation (last regret = 21.4).
- Policy 'BayesUCB'	was ranked	2 / 12 for this simulation (last regret = 41.4).
- Policy 'Thompson'	was ranked	3 / 12 for this simulation (last regret = 42.65).
- Policy 'KL-UCB+(Bern)'	was ranked	4 / 12 for this simulation (last regret = 47.15).
- Policy 'KL-UCB-H+(Bern)'	was ranked	5 / 12 for this simulation (last regret = 47.9).
- Policy 'KL-UCB++(Bern)'	was ranked	6 / 12 for this simulation (last regret = 48.4).
- Policy 'Aggr($N=10$)'	was ranked	7 / 12 for this simulation (last regret = 53.9).
- Policy 'UCB($\alpha=0.5$)'	was ranked	8 / 12 for this simulation (last regret = 56.15).
- Policy 'KL-UCB(Bern)'	was ranked	9 / 12 for this simulation (last regret = 56.15).
- Policy 'Aggr($N=10$, Exp4)'	was ranked	10 / 12 for this simulation (last regret = 68.15).
- Policy 'MOSS'	was ranked	11 / 12 for this simulation (last regret = 76.4).
- Policy 'UCB($\alpha=1$)'	was ranked	12 / 12 for this simulation (last regret = 87.4).

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 7.52 for 1-player problem... 
 - a Optimal Arm Identification factor H_OI(mu) = 60.00% ...

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 7.52 for 1-player problem... 
 - a Optimal Arm Identification factor H_OI(mu) = 60.00% ...

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 7.52 for 1-player problem... 
 - a Optimal Arm Identification factor H_OI(mu) = 60.00% ...


==> To see the figures, do :
eog plots/T5000_N4__12_algos/main*2536103956350324390.png
Done for simulations main.py ...
