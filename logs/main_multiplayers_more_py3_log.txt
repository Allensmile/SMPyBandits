 - Setting dpi of all figures to 110 ...
 - Setting 'figsize' of all figures to (19.8, 10.8) ...
Info: Using the regular tqdm() decorator ...
Info: numba.jit seems to be available.
Info: numba.jit seems to be available.
Loaded experiments configuration from 'configuration.py' :
configuration = {'horizon': 100000, 'successive_players': [[Selfish(BayesUCB), Selfish(BayesUCB), Selfish(BayesUCB)], [rhoRand(BayesUCB), rhoRand(BayesUCB), rhoRand(BayesUCB)], [rhoLearn(BayesUCB), rhoLearn(BayesUCB), rhoLearn(BayesUCB)], [rhoLearn(BayesUCB), rhoLearn(BayesUCB), rhoLearn(BayesUCB)], [rhoLearn(BayesUCB), rhoLearn(BayesUCB), rhoLearn(BayesUCB)], [rhoLearn(BayesUCB), rhoLearn(BayesUCB), rhoLearn(BayesUCB)]], 'n_jobs': 1, 'finalRanksOnAverage': True, 'players': [rhoRand(UCB), rhoRand(UCB), rhoRand(UCB)], 'environment': [{'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': array([ 0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9])}], 'collisionModel': <function onlyUniqUserGetsReward at 0x7fb3fd0d5378>, 'verbosity': 6, 'repetitions': 1, 'delta_t_save': 1, 'averageOn': 0.001}
plots/ is already a directory here...



Considering the list of players :
 [Selfish(BayesUCB), Selfish(BayesUCB), Selfish(BayesUCB)]
Number of players in the multi-players game: 3
Time horizon: 100000
Number of repetitions: 1
Sampling rate DELTA_T_SAVE: 1
Using collision model: onlyUniqUserGetsReward
  Detail:  Simple collision model where only the players alone on one arm sample it and receive the reward.

    - This is the default collision model, cf. https://arxiv.org/abs/0910.2065v3 collision model 1.
    - The numpy array 'choices' is increased according to the number of users who collided (it is NOT binary).
    
Creating a new MAB problem ...
  Reading arms of this MAB problem from a dictionnary 'configuration' = {'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': array([ 0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9])} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = [ 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9]
 - with 'arms' = [B(0.1), B(0.2), B(0.3), B(0.4), B(0.5), B(0.6), B(0.7), B(0.8), B(0.9)]
 - with 'nbArms' = 9
 - with 'maxArm' = 0.9
 - with 'minArm' = 0.1

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 7.52 ... 
 - a Optimal Arm Identification factor H_OI(mu) = 48.89% ...
Number of environments to try: 1

Evaluating environment: MAB(nbArms: 9, arms: [B(0.1), B(0.2), B(0.3), B(0.4), B(0.5), B(0.6), B(0.7), B(0.8), B(0.9)], minArm: 0.1, maxArm: 0.9)
- Adding player #1 = #1<Selfish, BayesUCB> ...
  Using this already created player 'player' = #1<Selfish, BayesUCB> ...
- Adding player #2 = #2<Selfish, BayesUCB> ...
  Using this already created player 'player' = #2<Selfish, BayesUCB> ...
- Adding player #3 = #3<Selfish, BayesUCB> ...
  Using this already created player 'player' = #3<Selfish, BayesUCB> ...

Estimated order by the policy #1<Selfish, BayesUCB> after 100000 steps: [0 1 8 2 6 3 5 4 7] ...
  ==> Optimal arm identification: 79.17% (relative success)...
  ==> Manhattan   distance from optimal ordering: 60.49% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 90.47% (relative success)...
  ==> Spearman    distance from optimal ordering: 86.08% (relative success)...
  ==> Gestalt     distance from optimal ordering: 33.33% (relative success)...
  ==> Mean distance from optimal ordering: 67.59% (relative success)...

Estimated order by the policy #2<Selfish, BayesUCB> after 100000 steps: [3 7 6 5 4 1 0 2 8] ...
  ==> Optimal arm identification: 54.17% (relative success)...
  ==> Manhattan   distance from optimal ordering: 25.93% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 59.58% (relative success)...
  ==> Spearman    distance from optimal ordering: 36.32% (relative success)...
  ==> Gestalt     distance from optimal ordering: 33.33% (relative success)...
  ==> Mean distance from optimal ordering: 38.79% (relative success)...

Estimated order by the policy #3<Selfish, BayesUCB> after 100000 steps: [0 1 7 2 8 4 3 5 6] ...
  ==> Optimal arm identification: 70.83% (relative success)...
  ==> Manhattan   distance from optimal ordering: 55.56% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 90.47% (relative success)...
  ==> Spearman    distance from optimal ordering: 82.95% (relative success)...
  ==> Gestalt     distance from optimal ordering: 66.67% (relative success)...
  ==> Mean distance from optimal ordering: 73.91% (relative success)...
Giving the final ranks ...

Final ranking for this environment #0 :
- Player #2, '#2<Selfish, BayesUCB>'	was ranked	1 / 3 for this simulation (last rewards = 89806).
- Player #1, '#1<Selfish, BayesUCB>'	was ranked	2 / 3 for this simulation (last rewards = 79813).
- Player #3, '#3<Selfish, BayesUCB>'	was ranked	3 / 3 for this simulation (last rewards = 69788).



Considering the list of players :
 [rhoRand(BayesUCB), rhoRand(BayesUCB), rhoRand(BayesUCB)]
Number of players in the multi-players game: 3
Time horizon: 100000
Number of repetitions: 1
Sampling rate DELTA_T_SAVE: 1
Using collision model: onlyUniqUserGetsReward
  Detail:  Simple collision model where only the players alone on one arm sample it and receive the reward.

    - This is the default collision model, cf. https://arxiv.org/abs/0910.2065v3 collision model 1.
    - The numpy array 'choices' is increased according to the number of users who collided (it is NOT binary).
    
Creating a new MAB problem ...
  Reading arms of this MAB problem from a dictionnary 'configuration' = {'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': array([ 0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9])} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = [ 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9]
 - with 'arms' = [B(0.1), B(0.2), B(0.3), B(0.4), B(0.5), B(0.6), B(0.7), B(0.8), B(0.9)]
 - with 'nbArms' = 9
 - with 'maxArm' = 0.9
 - with 'minArm' = 0.1

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 7.52 ... 
 - a Optimal Arm Identification factor H_OI(mu) = 48.89% ...
Number of environments to try: 1

Evaluating environment: MAB(nbArms: 9, arms: [B(0.1), B(0.2), B(0.3), B(0.4), B(0.5), B(0.6), B(0.7), B(0.8), B(0.9)], minArm: 0.1, maxArm: 0.9)
- Adding player #1 = #1<$\rho^{\mathrm{Rand}}$, BayesUCB> ...
  Using this already created player 'player' = #1<$\rho^{\mathrm{Rand}}$, BayesUCB> ...
- Adding player #2 = #2<$\rho^{\mathrm{Rand}}$, BayesUCB> ...
  Using this already created player 'player' = #2<$\rho^{\mathrm{Rand}}$, BayesUCB> ...
- Adding player #3 = #3<$\rho^{\mathrm{Rand}}$, BayesUCB> ...
  Using this already created player 'player' = #3<$\rho^{\mathrm{Rand}}$, BayesUCB> ...

Estimated order by the policy #1<$\rho^{\mathrm{Rand}}$, BayesUCB, rank:1> after 100000 steps: [7 5 4 0 6 1 2 3 8] ...
  ==> Optimal arm identification: 66.67% (relative success)...
  ==> Manhattan   distance from optimal ordering: 25.93% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 16.52% (relative success)...
  ==> Spearman    distance from optimal ordering: 16.88% (relative success)...
  ==> Gestalt     distance from optimal ordering: 55.56% (relative success)...
  ==> Mean distance from optimal ordering: 28.72% (relative success)...

Estimated order by the policy #2<$\rho^{\mathrm{Rand}}$, BayesUCB, rank:2> after 100000 steps: [5 2 4 1 0 6 3 7 8] ...
  ==> Optimal arm identification: 87.50% (relative success)...
  ==> Manhattan   distance from optimal ordering: 55.56% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 78.91% (relative success)...
  ==> Spearman    distance from optimal ordering: 82.95% (relative success)...
  ==> Gestalt     distance from optimal ordering: 44.44% (relative success)...
  ==> Mean distance from optimal ordering: 65.47% (relative success)...

Estimated order by the policy #3<$\rho^{\mathrm{Rand}}$, BayesUCB, rank:3> after 100000 steps: [0 3 1 4 2 5 6 8 7] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 80.25% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 99.65% (relative success)...
  ==> Spearman    distance from optimal ordering: 99.91% (relative success)...
  ==> Gestalt     distance from optimal ordering: 66.67% (relative success)...
  ==> Mean distance from optimal ordering: 86.62% (relative success)...
Giving the final ranks ...

Final ranking for this environment #0 :
- Player #1, '#1<$\rho^{\mathrm{Rand}}$, BayesUCB>'	was ranked	1 / 3 for this simulation (last rewards = 89547).
- Player #2, '#2<$\rho^{\mathrm{Rand}}$, BayesUCB>'	was ranked	2 / 3 for this simulation (last rewards = 80224).
- Player #3, '#3<$\rho^{\mathrm{Rand}}$, BayesUCB>'	was ranked	3 / 3 for this simulation (last rewards = 69836).
plots/MP__M3_T100000_N1__3_algos is already a directory here...



Considering the list of players :
 [rhoLearn(BayesUCB), rhoLearn(BayesUCB), rhoLearn(BayesUCB)]
Number of players in the multi-players game: 3
Time horizon: 100000
Number of repetitions: 1
Sampling rate DELTA_T_SAVE: 1
Using collision model: onlyUniqUserGetsReward
  Detail:  Simple collision model where only the players alone on one arm sample it and receive the reward.

    - This is the default collision model, cf. https://arxiv.org/abs/0910.2065v3 collision model 1.
    - The numpy array 'choices' is increased according to the number of users who collided (it is NOT binary).
    
Creating a new MAB problem ...
  Reading arms of this MAB problem from a dictionnary 'configuration' = {'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': array([ 0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9])} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = [ 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9]
 - with 'arms' = [B(0.1), B(0.2), B(0.3), B(0.4), B(0.5), B(0.6), B(0.7), B(0.8), B(0.9)]
 - with 'nbArms' = 9
 - with 'maxArm' = 0.9
 - with 'minArm' = 0.1

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 7.52 ... 
 - a Optimal Arm Identification factor H_OI(mu) = 48.89% ...
Number of environments to try: 1

Evaluating environment: MAB(nbArms: 9, arms: [B(0.1), B(0.2), B(0.3), B(0.4), B(0.5), B(0.6), B(0.7), B(0.8), B(0.9)], minArm: 0.1, maxArm: 0.9)
- Adding player #1 = #1<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ UCB($\alpha=4$)> ...
  Using this already created player 'player' = #1<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ UCB($\alpha=4$)> ...
- Adding player #2 = #2<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ UCB($\alpha=4$)> ...
  Using this already created player 'player' = #2<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ UCB($\alpha=4$)> ...
- Adding player #3 = #3<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ UCB($\alpha=4$)> ...
  Using this already created player 'player' = #3<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ UCB($\alpha=4$)> ...

Estimated order by the policy #1<$\rho^{\mathrm{Learn}}$, BayesUCB, rank: 3 ~ UCB($\alpha=4$)> after 100000 steps: [3 2 1 0 4 5 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 80.25% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 98.77% (relative success)...
  ==> Spearman    distance from optimal ordering: 99.47% (relative success)...
  ==> Gestalt     distance from optimal ordering: 66.67% (relative success)...
  ==> Mean distance from optimal ordering: 86.29% (relative success)...

Estimated order by the policy #2<$\rho^{\mathrm{Learn}}$, BayesUCB, rank: 1 ~ UCB($\alpha=4$)> after 100000 steps: [7 4 6 5 0 1 2 3 8] ...
  ==> Optimal arm identification: 66.67% (relative success)...
  ==> Manhattan   distance from optimal ordering: 20.99% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 32.33% (relative success)...
  ==> Spearman    distance from optimal ordering: 36.32% (relative success)...
  ==> Gestalt     distance from optimal ordering: 55.56% (relative success)...
  ==> Mean distance from optimal ordering: 36.30% (relative success)...

Estimated order by the policy #3<$\rho^{\mathrm{Learn}}$, BayesUCB, rank: 2 ~ UCB($\alpha=4$)> after 100000 steps: [5 0 6 4 1 2 3 7 8] ...
  ==> Optimal arm identification: 87.50% (relative success)...
  ==> Manhattan   distance from optimal ordering: 50.62% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 78.91% (relative success)...
  ==> Spearman    distance from optimal ordering: 73.54% (relative success)...
  ==> Gestalt     distance from optimal ordering: 66.67% (relative success)...
  ==> Mean distance from optimal ordering: 67.43% (relative success)...
Giving the final ranks ...

Final ranking for this environment #0 :
- Player #2, '#2<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ UCB($\alpha=4$)>'	was ranked	1 / 3 for this simulation (last rewards = 89881).
- Player #3, '#3<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ UCB($\alpha=4$)>'	was ranked	2 / 3 for this simulation (last rewards = 79772).
- Player #1, '#1<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ UCB($\alpha=4$)>'	was ranked	3 / 3 for this simulation (last rewards = 69946).
plots/MP__M3_T100000_N1__3_algos is already a directory here...



Considering the list of players :
 [rhoLearn(BayesUCB), rhoLearn(BayesUCB), rhoLearn(BayesUCB)]
Number of players in the multi-players game: 3
Time horizon: 100000
Number of repetitions: 1
Sampling rate DELTA_T_SAVE: 1
Using collision model: onlyUniqUserGetsReward
  Detail:  Simple collision model where only the players alone on one arm sample it and receive the reward.

    - This is the default collision model, cf. https://arxiv.org/abs/0910.2065v3 collision model 1.
    - The numpy array 'choices' is increased according to the number of users who collided (it is NOT binary).
    
Creating a new MAB problem ...
  Reading arms of this MAB problem from a dictionnary 'configuration' = {'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': array([ 0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9])} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = [ 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9]
 - with 'arms' = [B(0.1), B(0.2), B(0.3), B(0.4), B(0.5), B(0.6), B(0.7), B(0.8), B(0.9)]
 - with 'nbArms' = 9
 - with 'maxArm' = 0.9
 - with 'minArm' = 0.1

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 7.52 ... 
 - a Optimal Arm Identification factor H_OI(mu) = 48.89% ...
Number of environments to try: 1

Evaluating environment: MAB(nbArms: 9, arms: [B(0.1), B(0.2), B(0.3), B(0.4), B(0.5), B(0.6), B(0.7), B(0.8), B(0.9)], minArm: 0.1, maxArm: 0.9)
- Adding player #1 = #1<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ Thompson> ...
  Using this already created player 'player' = #1<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ Thompson> ...
- Adding player #2 = #2<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ Thompson> ...
  Using this already created player 'player' = #2<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ Thompson> ...
- Adding player #3 = #3<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ Thompson> ...
  Using this already created player 'player' = #3<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ Thompson> ...

Estimated order by the policy #1<$\rho^{\mathrm{Learn}}$, BayesUCB, rank: 3 ~ Thompson> after 100000 steps: [0 1 3 2 5 4 7 6 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 85.19% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 99.82% (relative success)...
  ==> Spearman    distance from optimal ordering: 99.99% (relative success)...
  ==> Gestalt     distance from optimal ordering: 66.67% (relative success)...
  ==> Mean distance from optimal ordering: 87.92% (relative success)...

Estimated order by the policy #2<$\rho^{\mathrm{Learn}}$, BayesUCB, rank: 3 ~ Thompson> after 100000 steps: [0 1 2 3 4 5 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 100.00% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 99.98% (relative success)...
  ==> Spearman    distance from optimal ordering: 100.00% (relative success)...
  ==> Gestalt     distance from optimal ordering: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 100.00% (relative success)...

Estimated order by the policy #3<$\rho^{\mathrm{Learn}}$, BayesUCB, rank: 3 ~ Thompson> after 100000 steps: [3 4 0 1 2 5 8 7 6] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 60.49% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 93.94% (relative success)...
  ==> Spearman    distance from optimal ordering: 95.76% (relative success)...
  ==> Gestalt     distance from optimal ordering: 55.56% (relative success)...
  ==> Mean distance from optimal ordering: 76.44% (relative success)...
Giving the final ranks ...

Final ranking for this environment #0 :
- Player #3, '#3<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ Thompson>'	was ranked	1 / 3 for this simulation (last rewards = 89713).
- Player #1, '#1<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ Thompson>'	was ranked	2 / 3 for this simulation (last rewards = 79539).
- Player #2, '#2<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ Thompson>'	was ranked	3 / 3 for this simulation (last rewards = 69610).
plots/MP__M3_T100000_N1__3_algos is already a directory here...



Considering the list of players :
 [rhoLearn(BayesUCB), rhoLearn(BayesUCB), rhoLearn(BayesUCB)]
Number of players in the multi-players game: 3
Time horizon: 100000
Number of repetitions: 1
Sampling rate DELTA_T_SAVE: 1
Using collision model: onlyUniqUserGetsReward
  Detail:  Simple collision model where only the players alone on one arm sample it and receive the reward.

    - This is the default collision model, cf. https://arxiv.org/abs/0910.2065v3 collision model 1.
    - The numpy array 'choices' is increased according to the number of users who collided (it is NOT binary).
    
Creating a new MAB problem ...
  Reading arms of this MAB problem from a dictionnary 'configuration' = {'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': array([ 0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9])} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = [ 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9]
 - with 'arms' = [B(0.1), B(0.2), B(0.3), B(0.4), B(0.5), B(0.6), B(0.7), B(0.8), B(0.9)]
 - with 'nbArms' = 9
 - with 'maxArm' = 0.9
 - with 'minArm' = 0.1

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 7.52 ... 
 - a Optimal Arm Identification factor H_OI(mu) = 48.89% ...
Number of environments to try: 1

Evaluating environment: MAB(nbArms: 9, arms: [B(0.1), B(0.2), B(0.3), B(0.4), B(0.5), B(0.6), B(0.7), B(0.8), B(0.9)], minArm: 0.1, maxArm: 0.9)
- Adding player #1 = #1<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ KL-UCB+(Bern)> ...
  Using this already created player 'player' = #1<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ KL-UCB+(Bern)> ...
- Adding player #2 = #2<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ KL-UCB+(Bern)> ...
  Using this already created player 'player' = #2<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ KL-UCB+(Bern)> ...
- Adding player #3 = #3<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ KL-UCB+(Bern)> ...
  Using this already created player 'player' = #3<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ KL-UCB+(Bern)> ...

Estimated order by the policy #1<$\rho^{\mathrm{Learn}}$, BayesUCB, rank: 3 ~ KL-UCB+(Bern)> after 100000 steps: [0 2 3 5 4 1 6 8 7] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 75.31% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 98.77% (relative success)...
  ==> Spearman    distance from optimal ordering: 99.04% (relative success)...
  ==> Gestalt     distance from optimal ordering: 66.67% (relative success)...
  ==> Mean distance from optimal ordering: 84.94% (relative success)...

Estimated order by the policy #2<$\rho^{\mathrm{Learn}}$, BayesUCB, rank: 3 ~ KL-UCB+(Bern)> after 100000 steps: [1 2 4 5 0 3 8 7 6] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 60.49% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 93.94% (relative success)...
  ==> Spearman    distance from optimal ordering: 95.76% (relative success)...
  ==> Gestalt     distance from optimal ordering: 55.56% (relative success)...
  ==> Mean distance from optimal ordering: 76.44% (relative success)...

Estimated order by the policy #3<$\rho^{\mathrm{Learn}}$, BayesUCB, rank: 3 ~ KL-UCB+(Bern)> after 100000 steps: [0 1 3 5 4 2 7 6 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 80.25% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 99.33% (relative success)...
  ==> Spearman    distance from optimal ordering: 99.75% (relative success)...
  ==> Gestalt     distance from optimal ordering: 66.67% (relative success)...
  ==> Mean distance from optimal ordering: 86.50% (relative success)...
Giving the final ranks ...

Final ranking for this environment #0 :
- Player #2, '#2<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ KL-UCB+(Bern)>'	was ranked	1 / 3 for this simulation (last rewards = 89811).
- Player #3, '#3<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ KL-UCB+(Bern)>'	was ranked	2 / 3 for this simulation (last rewards = 78879).
- Player #1, '#1<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ KL-UCB+(Bern)>'	was ranked	3 / 3 for this simulation (last rewards = 68849).
plots/MP__M3_T100000_N1__3_algos is already a directory here...



Considering the list of players :
 [rhoLearn(BayesUCB), rhoLearn(BayesUCB), rhoLearn(BayesUCB)]
Number of players in the multi-players game: 3
Time horizon: 100000
Number of repetitions: 1
Sampling rate DELTA_T_SAVE: 1
Using collision model: onlyUniqUserGetsReward
  Detail:  Simple collision model where only the players alone on one arm sample it and receive the reward.

    - This is the default collision model, cf. https://arxiv.org/abs/0910.2065v3 collision model 1.
    - The numpy array 'choices' is increased according to the number of users who collided (it is NOT binary).
    
Creating a new MAB problem ...
  Reading arms of this MAB problem from a dictionnary 'configuration' = {'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': array([ 0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9])} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = [ 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9]
 - with 'arms' = [B(0.1), B(0.2), B(0.3), B(0.4), B(0.5), B(0.6), B(0.7), B(0.8), B(0.9)]
 - with 'nbArms' = 9
 - with 'maxArm' = 0.9
 - with 'minArm' = 0.1

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 7.52 ... 
 - a Optimal Arm Identification factor H_OI(mu) = 48.89% ...
Number of environments to try: 1

Evaluating environment: MAB(nbArms: 9, arms: [B(0.1), B(0.2), B(0.3), B(0.4), B(0.5), B(0.6), B(0.7), B(0.8), B(0.9)], minArm: 0.1, maxArm: 0.9)
- Adding player #1 = #1<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ BayesUCB> ...
  Using this already created player 'player' = #1<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ BayesUCB> ...
- Adding player #2 = #2<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ BayesUCB> ...
  Using this already created player 'player' = #2<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ BayesUCB> ...
- Adding player #3 = #3<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ BayesUCB> ...
  Using this already created player 'player' = #3<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ BayesUCB> ...

Estimated order by the policy #1<$\rho^{\mathrm{Learn}}$, BayesUCB, rank: 3 ~ BayesUCB> after 100000 steps: [2 5 3 1 0 4 8 7 6] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 55.56% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 78.91% (relative success)...
  ==> Spearman    distance from optimal ordering: 90.08% (relative success)...
  ==> Gestalt     distance from optimal ordering: 33.33% (relative success)...
  ==> Mean distance from optimal ordering: 64.47% (relative success)...

Estimated order by the policy #2<$\rho^{\mathrm{Learn}}$, BayesUCB, rank: 3 ~ BayesUCB> after 100000 steps: [1 2 3 4 5 0 7 6 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 70.37% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 98.77% (relative success)...
  ==> Spearman    distance from optimal ordering: 97.54% (relative success)...
  ==> Gestalt     distance from optimal ordering: 77.78% (relative success)...
  ==> Mean distance from optimal ordering: 86.11% (relative success)...

Estimated order by the policy #3<$\rho^{\mathrm{Learn}}$, BayesUCB, rank: 3 ~ BayesUCB> after 100000 steps: [1 2 3 4 0 5 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 80.25% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 99.65% (relative success)...
  ==> Spearman    distance from optimal ordering: 99.47% (relative success)...
  ==> Gestalt     distance from optimal ordering: 88.89% (relative success)...
  ==> Mean distance from optimal ordering: 92.06% (relative success)...
Giving the final ranks ...

Final ranking for this environment #0 :
- Player #1, '#1<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ BayesUCB>'	was ranked	1 / 3 for this simulation (last rewards = 87313).
- Player #2, '#2<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ BayesUCB>'	was ranked	2 / 3 for this simulation (last rewards = 77545).
- Player #3, '#3<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ BayesUCB>'	was ranked	3 / 3 for this simulation (last rewards = 69785).
plots/MP__M3_T100000_N1__3_algos is already a directory here...


- Plotting the centralized regret for all 'players' values
  and saving the plot to plots/MP__M3_T100000_N1__3_algos/all_RegretCentralized____env1-1_416297610775185467 ...
  - For 3 players, our lower bound gave = 33.5 ...
  - For 3 players, the initial lower bound in Theorem 6 from [Anandkumar et al., 2010] gave = 17.8 ...

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 7.52 for 1-player problem ... 
 - a Optimal Arm Identification factor H_OI(mu) = 48.89% ...
 - Our lowerbound = 33.5,
 - [Anandkumar et al] lowerbound = 17.8
Saving figure with format png, to file 'plots/MP__M3_T100000_N1__3_algos/all_RegretCentralized____env1-1_416297610775185467.png'...
       Saved! 'plots/MP__M3_T100000_N1__3_algos/all_RegretCentralized____env1-1_416297610775185467.png' created of size '228996b', at 'Mon Mar 20 15:57:17 2017' ...
Saving figure with format pdf, to file 'plots/MP__M3_T100000_N1__3_algos/all_RegretCentralized____env1-1_416297610775185467.pdf'...
       Saved! 'plots/MP__M3_T100000_N1__3_algos/all_RegretCentralized____env1-1_416297610775185467.pdf' created of size '1084070b', at 'Mon Mar 20 15:57:20 2017' ...


- Plotting the centralized regret for all 'players' values, in semilogx scale
  and saving the plot to plots/MP__M3_T100000_N1__3_algos/all_RegretCentralized_semilogx____env1-1_416297610775185467 ...
  - For 3 players, our lower bound gave = 33.5 ...
  - For 3 players, the initial lower bound in Theorem 6 from [Anandkumar et al., 2010] gave = 17.8 ...

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 7.52 for 1-player problem ... 
 - a Optimal Arm Identification factor H_OI(mu) = 48.89% ...
 - Our lowerbound = 33.5,
 - [Anandkumar et al] lowerbound = 17.8
Saving figure with format png, to file 'plots/MP__M3_T100000_N1__3_algos/all_RegretCentralized_semilogx____env1-1_416297610775185467.png'...
       Saved! 'plots/MP__M3_T100000_N1__3_algos/all_RegretCentralized_semilogx____env1-1_416297610775185467.png' created of size '213599b', at 'Mon Mar 20 15:57:22 2017' ...
Saving figure with format pdf, to file 'plots/MP__M3_T100000_N1__3_algos/all_RegretCentralized_semilogx____env1-1_416297610775185467.pdf'...
       Saved! 'plots/MP__M3_T100000_N1__3_algos/all_RegretCentralized_semilogx____env1-1_416297610775185467.pdf' created of size '707622b', at 'Mon Mar 20 15:57:22 2017' ...


- Plotting the centralized regret for all 'players' values, in semilogy scale
  and saving the plot to plots/MP__M3_T100000_N1__3_algos/all_RegretCentralized_semilogy____env1-1_416297610775185467 ...
  - For 3 players, our lower bound gave = 33.5 ...
  - For 3 players, the initial lower bound in Theorem 6 from [Anandkumar et al., 2010] gave = 17.8 ...

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 7.52 for 1-player problem ... 
 - a Optimal Arm Identification factor H_OI(mu) = 48.89% ...
 - Our lowerbound = 33.5,
 - [Anandkumar et al] lowerbound = 17.8
Saving figure with format png, to file 'plots/MP__M3_T100000_N1__3_algos/all_RegretCentralized_semilogy____env1-1_416297610775185467.png'...
       Saved! 'plots/MP__M3_T100000_N1__3_algos/all_RegretCentralized_semilogy____env1-1_416297610775185467.png' created of size '232086b', at 'Mon Mar 20 15:57:25 2017' ...
Saving figure with format pdf, to file 'plots/MP__M3_T100000_N1__3_algos/all_RegretCentralized_semilogy____env1-1_416297610775185467.pdf'...
       Saved! 'plots/MP__M3_T100000_N1__3_algos/all_RegretCentralized_semilogy____env1-1_416297610775185467.pdf' created of size '901842b', at 'Mon Mar 20 15:57:25 2017' ...


- Plotting the centralized regret for all 'players' values, in loglog scale
  and saving the plot to plots/MP__M3_T100000_N1__3_algos/all_RegretCentralized_loglog____env1-1_416297610775185467 ...
  - For 3 players, our lower bound gave = 33.5 ...
  - For 3 players, the initial lower bound in Theorem 6 from [Anandkumar et al., 2010] gave = 17.8 ...

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 7.52 for 1-player problem ... 
 - a Optimal Arm Identification factor H_OI(mu) = 48.89% ...
 - Our lowerbound = 33.5,
 - [Anandkumar et al] lowerbound = 17.8
Saving figure with format png, to file 'plots/MP__M3_T100000_N1__3_algos/all_RegretCentralized_loglog____env1-1_416297610775185467.png'...
       Saved! 'plots/MP__M3_T100000_N1__3_algos/all_RegretCentralized_loglog____env1-1_416297610775185467.png' created of size '296100b', at 'Mon Mar 20 15:57:28 2017' ...
Saving figure with format pdf, to file 'plots/MP__M3_T100000_N1__3_algos/all_RegretCentralized_loglog____env1-1_416297610775185467.pdf'...
       Saved! 'plots/MP__M3_T100000_N1__3_algos/all_RegretCentralized_loglog____env1-1_416297610775185467.pdf' created of size '744717b', at 'Mon Mar 20 15:57:28 2017' ...


- Plotting the centralized fairness (RajJain)
  and saving the plot to plots/MP__M3_T100000_N1__3_algos/all____env1-1_416297610775185467 ...
Saving figure with format png, to file 'plots/MP__M3_T100000_N1__3_algos/all____env1-1_416297610775185467.png'...
       Saved! 'plots/MP__M3_T100000_N1__3_algos/all____env1-1_416297610775185467.png' created of size '148951b', at 'Mon Mar 20 15:57:30 2017' ...
Saving figure with format pdf, to file 'plots/MP__M3_T100000_N1__3_algos/all____env1-1_416297610775185467.pdf'...
       Saved! 'plots/MP__M3_T100000_N1__3_algos/all____env1-1_416297610775185467.pdf' created of size '110448b', at 'Mon Mar 20 15:57:30 2017' ...
 - Plotting the total nb of collision as a function of time for all 'players' values
  and saving the plot to plots/MP__M3_T100000_N1__3_algos/all_NbCollisions____env1-1_416297610775185467 ...
No upper bound for the non-cumulated number of collisions...
Saving figure with format png, to file 'plots/MP__M3_T100000_N1__3_algos/all_NbCollisions____env1-1_416297610775185467.png'...
       Saved! 'plots/MP__M3_T100000_N1__3_algos/all_NbCollisions____env1-1_416297610775185467.png' created of size '119734b', at 'Mon Mar 20 15:57:31 2017' ...
Saving figure with format pdf, to file 'plots/MP__M3_T100000_N1__3_algos/all_NbCollisions____env1-1_416297610775185467.pdf'...
       Saved! 'plots/MP__M3_T100000_N1__3_algos/all_NbCollisions____env1-1_416297610775185467.pdf' created of size '103473b', at 'Mon Mar 20 15:57:39 2017' ...
 - Plotting the cumulated total nb of collision as a function of time for all 'players' values
  and saving the plot to plots/MP__M3_T100000_N1__3_algos/all_CumNbCollisions____env1-1_416297610775185467 ...
  - For 3 players, Upsilon(M,M) = (2M-1 choose M) = 10 ...
  - For 3 players, the bound with (1 + pi^2 / 3) = 270 ...
  - For 3 players, the bound with (8 / (mu_b^* - mu_a^*)^2) = 1.09e+04 ...
  - For 3 players, Anandkumar et al. upper bound for the total cumulated number of collisions is 4.14e+06 here ...
Anandkumar et al. upper bound for the non-cumulated number of collisions is 4.14e+06 * log(t) here ...
Saving figure with format png, to file 'plots/MP__M3_T100000_N1__3_algos/all_CumNbCollisions____env1-1_416297610775185467.png'...
       Saved! 'plots/MP__M3_T100000_N1__3_algos/all_CumNbCollisions____env1-1_416297610775185467.png' created of size '145390b', at 'Mon Mar 20 15:57:53 2017' ...
Saving figure with format pdf, to file 'plots/MP__M3_T100000_N1__3_algos/all_CumNbCollisions____env1-1_416297610775185467.pdf'...
       Saved! 'plots/MP__M3_T100000_N1__3_algos/all_CumNbCollisions____env1-1_416297610775185467.pdf' created of size '43228b', at 'Mon Mar 20 15:57:53 2017' ...


- Plotting the number of switches as a function of time for all 'players' values
  and saving the plot to plots/MP__M3_T100000_N1__3_algos/all_CumNbSwitchs____env1-1_416297610775185467 ...
Saving figure with format png, to file 'plots/MP__M3_T100000_N1__3_algos/all_CumNbSwitchs____env1-1_416297610775185467.png'...
       Saved! 'plots/MP__M3_T100000_N1__3_algos/all_CumNbSwitchs____env1-1_416297610775185467.png' created of size '180091b', at 'Mon Mar 20 15:57:55 2017' ...
Saving figure with format pdf, to file 'plots/MP__M3_T100000_N1__3_algos/all_CumNbSwitchs____env1-1_416297610775185467.pdf'...
       Saved! 'plots/MP__M3_T100000_N1__3_algos/all_CumNbSwitchs____env1-1_416297610775185467.pdf' created of size '56116b', at 'Mon Mar 20 15:57:55 2017' ...


==> To see the figures, do :
eog plots/MP__M3_T100000_N1__3_algos/all*416297610775185467.png
Done for simulations main_multiplayers.py ...
