 - Setting dpi of all figures to 110 ...
 - Setting 'figsize' of all figures to (19.8, 10.8) ...
Info: Using the regular tqdm() decorator ...
Info: numba.jit seems to be available.
Info: numba.jit seems to be available.
Loaded experiments configuration from 'configuration.py' :
configuration = {'collisionModel': <function onlyUniqUserGetsReward at 0x7f81f4dd22f0>, 'players': [rhoRand(BayesUCB), rhoRand(BayesUCB), rhoRand(BayesUCB), rhoRand(BayesUCB), rhoRand(BayesUCB), rhoRand(BayesUCB)], 'repetitions': 1, 'successive_players': [[Selfish(BayesUCB), Selfish(BayesUCB), Selfish(BayesUCB), Selfish(BayesUCB), Selfish(BayesUCB), Selfish(BayesUCB)], [rhoRand(BayesUCB), rhoRand(BayesUCB), rhoRand(BayesUCB), rhoRand(BayesUCB), rhoRand(BayesUCB), rhoRand(BayesUCB)], [rhoLearn(BayesUCB), rhoLearn(BayesUCB), rhoLearn(BayesUCB), rhoLearn(BayesUCB), rhoLearn(BayesUCB), rhoLearn(BayesUCB)]], 'averageOn': 0.001, 'n_jobs': 2, 'finalRanksOnAverage': True, 'verbosity': 6, 'delta_t_save': 1, 'environment': [{'params': array([ 0.125,  0.25 ,  0.375,  0.5  ,  0.625,  0.75 ,  0.875]), 'arm_type': <class 'Arms.Bernoulli.Bernoulli'>}], 'horizon': 10000}
plots/ is already a directory here...



Considering the list of players :
 [Selfish(BayesUCB), Selfish(BayesUCB), Selfish(BayesUCB), Selfish(BayesUCB), Selfish(BayesUCB), Selfish(BayesUCB)]
Number of players in the multi-players game: 6
Time horizon: 10000
Number of repetitions: 1
Sampling rate for saving, delta_t_save: 1
Sampling rate for plotting, delta_t_plot: 1
Number of jobs for parallelization: 2
Using collision model: onlyUniqUserGetsReward
  Detail:  Simple collision model where only the players alone on one arm sample it and receive the reward.

    - This is the default collision model, cf. https://arxiv.org/abs/0910.2065v3 collision model 1.
    - The numpy array 'choices' is increased according to the number of users who collided (it is NOT binary).
    
Creating a new MAB problem ...
  Reading arms of this MAB problem from a dictionnary 'configuration' = {'params': array([ 0.125,  0.25 ,  0.375,  0.5  ,  0.625,  0.75 ,  0.875]), 'arm_type': <class 'Arms.Bernoulli.Bernoulli'>} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = [ 0.125  0.25   0.375  0.5    0.625  0.75   0.875]
 - with 'arms' = [B(0.125), B(0.25), B(0.375), B(0.5), B(0.625), B(0.75), B(0.875)]
 - with 'nbArms' = 7
 - with 'maxArm' = 0.875
 - with 'minArm' = 0.125

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 6.16 ... 
 - a Optimal Arm Identification factor H_OI(mu) = 48.21% ...
Number of environments to try: 1

Evaluating environment: MAB(nbArms: 7, arms: [B(0.125), B(0.25), B(0.375), B(0.5), B(0.625), B(0.75), B(0.875)], minArm: 0.125, maxArm: 0.875)
- Adding player #1 = #1<Selfish, BayesUCB> ...
  Using this already created player 'player' = #1<Selfish, BayesUCB> ...
- Adding player #2 = #2<Selfish, BayesUCB> ...
  Using this already created player 'player' = #2<Selfish, BayesUCB> ...
- Adding player #3 = #3<Selfish, BayesUCB> ...
  Using this already created player 'player' = #3<Selfish, BayesUCB> ...
- Adding player #4 = #4<Selfish, BayesUCB> ...
  Using this already created player 'player' = #4<Selfish, BayesUCB> ...
- Adding player #5 = #5<Selfish, BayesUCB> ...
  Using this already created player 'player' = #5<Selfish, BayesUCB> ...
- Adding player #6 = #6<Selfish, BayesUCB> ...
  Using this already created player 'player' = #6<Selfish, BayesUCB> ...

Estimated order by the policy #1<Selfish, BayesUCB> after 10000 steps: [2 5 6 3 0 4 1] ...
  ==> Optimal arm identification: 92.59% (relative success)...
  ==> Manhattan   distance from optimal ordering: 18.37% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 54.73% (relative success)...
  ==> Spearman    distance from optimal ordering: 61.67% (relative success)...
  ==> Gestalt     distance from optimal ordering: 42.86% (relative success)...
  ==> Mean distance from optimal ordering: 44.41% (relative success)...

Estimated order by the policy #2<Selfish, BayesUCB> after 10000 steps: [0 1 4 6 3 2 5] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 59.18% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 82.35% (relative success)...
  ==> Spearman    distance from optimal ordering: 81.98% (relative success)...
  ==> Gestalt     distance from optimal ordering: 57.14% (relative success)...
  ==> Mean distance from optimal ordering: 70.16% (relative success)...

Estimated order by the policy #3<Selfish, BayesUCB> after 10000 steps: [4 0 1 3 5 2 6] ...
  ==> Optimal arm identification: 85.19% (relative success)...
  ==> Manhattan   distance from optimal ordering: 59.18% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 82.35% (relative success)...
  ==> Spearman    distance from optimal ordering: 74.68% (relative success)...
  ==> Gestalt     distance from optimal ordering: 71.43% (relative success)...
  ==> Mean distance from optimal ordering: 71.91% (relative success)...

Estimated order by the policy #4<Selfish, BayesUCB> after 10000 steps: [5 6 0 1 2 4 3] ...
  ==> Optimal arm identification: 81.48% (relative success)...
  ==> Manhattan   distance from optimal ordering: 18.37% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 11.94% (relative success)...
  ==> Spearman    distance from optimal ordering: 46.55% (relative success)...
  ==> Gestalt     distance from optimal ordering: 57.14% (relative success)...
  ==> Mean distance from optimal ordering: 33.50% (relative success)...

Estimated order by the policy #5<Selfish, BayesUCB> after 10000 steps: [2 0 3 1 6 5 4] ...
  ==> Optimal arm identification: 92.59% (relative success)...
  ==> Manhattan   distance from optimal ordering: 59.18% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 82.35% (relative success)...
  ==> Spearman    distance from optimal ordering: 90.62% (relative success)...
  ==> Gestalt     distance from optimal ordering: 42.86% (relative success)...
  ==> Mean distance from optimal ordering: 68.75% (relative success)...

Estimated order by the policy #6<Selfish, BayesUCB> after 10000 steps: [6 5 1 3 0 4 2] ...
  ==> Optimal arm identification: 77.78% (relative success)...
  ==> Manhattan   distance from optimal ordering: 18.37% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 82.35% (relative success)...
  ==> Spearman    distance from optimal ordering: 78.48% (relative success)...
  ==> Gestalt     distance from optimal ordering: 14.29% (relative success)...
  ==> Mean distance from optimal ordering: 48.37% (relative success)...
Giving the final ranks ...

Final ranking for this environment #0 :
- Player #3, '#3<Selfish, BayesUCB>'	was ranked	1 / 6 for this simulation (last rewards = 8664).
- Player #2, '#2<Selfish, BayesUCB>'	was ranked	2 / 6 for this simulation (last rewards = 7384).
- Player #5, '#5<Selfish, BayesUCB>'	was ranked	3 / 6 for this simulation (last rewards = 5976).
- Player #4, '#4<Selfish, BayesUCB>'	was ranked	4 / 6 for this simulation (last rewards = 4914).
- Player #6, '#6<Selfish, BayesUCB>'	was ranked	5 / 6 for this simulation (last rewards = 3652).
- Player #1, '#1<Selfish, BayesUCB>'	was ranked	6 / 6 for this simulation (last rewards = 2349).



Considering the list of players :
 [rhoRand(BayesUCB), rhoRand(BayesUCB), rhoRand(BayesUCB), rhoRand(BayesUCB), rhoRand(BayesUCB), rhoRand(BayesUCB)]
Number of players in the multi-players game: 6
Time horizon: 10000
Number of repetitions: 1
Sampling rate for saving, delta_t_save: 1
Sampling rate for plotting, delta_t_plot: 1
Number of jobs for parallelization: 2
Using collision model: onlyUniqUserGetsReward
  Detail:  Simple collision model where only the players alone on one arm sample it and receive the reward.

    - This is the default collision model, cf. https://arxiv.org/abs/0910.2065v3 collision model 1.
    - The numpy array 'choices' is increased according to the number of users who collided (it is NOT binary).
    
Creating a new MAB problem ...
  Reading arms of this MAB problem from a dictionnary 'configuration' = {'params': array([ 0.125,  0.25 ,  0.375,  0.5  ,  0.625,  0.75 ,  0.875]), 'arm_type': <class 'Arms.Bernoulli.Bernoulli'>} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = [ 0.125  0.25   0.375  0.5    0.625  0.75   0.875]
 - with 'arms' = [B(0.125), B(0.25), B(0.375), B(0.5), B(0.625), B(0.75), B(0.875)]
 - with 'nbArms' = 7
 - with 'maxArm' = 0.875
 - with 'minArm' = 0.125

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 6.16 ... 
 - a Optimal Arm Identification factor H_OI(mu) = 48.21% ...
Number of environments to try: 1

Evaluating environment: MAB(nbArms: 7, arms: [B(0.125), B(0.25), B(0.375), B(0.5), B(0.625), B(0.75), B(0.875)], minArm: 0.125, maxArm: 0.875)
- Adding player #1 = #1<$\rho^{\mathrm{Rand}}$, BayesUCB> ...
  Using this already created player 'player' = #1<$\rho^{\mathrm{Rand}}$, BayesUCB> ...
- Adding player #2 = #2<$\rho^{\mathrm{Rand}}$, BayesUCB> ...
  Using this already created player 'player' = #2<$\rho^{\mathrm{Rand}}$, BayesUCB> ...
- Adding player #3 = #3<$\rho^{\mathrm{Rand}}$, BayesUCB> ...
  Using this already created player 'player' = #3<$\rho^{\mathrm{Rand}}$, BayesUCB> ...
- Adding player #4 = #4<$\rho^{\mathrm{Rand}}$, BayesUCB> ...
  Using this already created player 'player' = #4<$\rho^{\mathrm{Rand}}$, BayesUCB> ...
- Adding player #5 = #5<$\rho^{\mathrm{Rand}}$, BayesUCB> ...
  Using this already created player 'player' = #5<$\rho^{\mathrm{Rand}}$, BayesUCB> ...
- Adding player #6 = #6<$\rho^{\mathrm{Rand}}$, BayesUCB> ...
  Using this already created player 'player' = #6<$\rho^{\mathrm{Rand}}$, BayesUCB> ...

Estimated order by the policy #1<$\rho^{\mathrm{Rand}}$, BayesUCB, rank:6> after 10000 steps: [0 1 2 4 3 5 6] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 91.84% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 99.57% (relative success)...
  ==> Spearman    distance from optimal ordering: 99.95% (relative success)...
  ==> Gestalt     distance from optimal ordering: 85.71% (relative success)...
  ==> Mean distance from optimal ordering: 94.27% (relative success)...

Estimated order by the policy #2<$\rho^{\mathrm{Rand}}$, BayesUCB, rank:2> after 10000 steps: [0 1 2 3 4 5 6] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 100.00% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 99.84% (relative success)...
  ==> Spearman    distance from optimal ordering: 100.00% (relative success)...
  ==> Gestalt     distance from optimal ordering: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 99.96% (relative success)...

Estimated order by the policy #3<$\rho^{\mathrm{Rand}}$, BayesUCB, rank:4> after 10000 steps: [1 0 2 3 4 5 6] ...
  ==> Optimal arm identification: 96.30% (relative success)...
  ==> Manhattan   distance from optimal ordering: 91.84% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 99.57% (relative success)...
  ==> Spearman    distance from optimal ordering: 99.95% (relative success)...
  ==> Gestalt     distance from optimal ordering: 85.71% (relative success)...
  ==> Mean distance from optimal ordering: 94.27% (relative success)...

Estimated order by the policy #4<$\rho^{\mathrm{Rand}}$, BayesUCB, rank:5> after 10000 steps: [1 0 2 4 3 5 6] ...
  ==> Optimal arm identification: 96.30% (relative success)...
  ==> Manhattan   distance from optimal ordering: 83.67% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 98.93% (relative success)...
  ==> Spearman    distance from optimal ordering: 99.75% (relative success)...
  ==> Gestalt     distance from optimal ordering: 71.43% (relative success)...
  ==> Mean distance from optimal ordering: 88.45% (relative success)...

Estimated order by the policy #5<$\rho^{\mathrm{Rand}}$, BayesUCB, rank:1> after 10000 steps: [1 0 2 3 4 5 6] ...
  ==> Optimal arm identification: 96.30% (relative success)...
  ==> Manhattan   distance from optimal ordering: 91.84% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 99.57% (relative success)...
  ==> Spearman    distance from optimal ordering: 99.95% (relative success)...
  ==> Gestalt     distance from optimal ordering: 85.71% (relative success)...
  ==> Mean distance from optimal ordering: 94.27% (relative success)...

Estimated order by the policy #6<$\rho^{\mathrm{Rand}}$, BayesUCB, rank:3> after 10000 steps: [0 2 1 3 4 5 6] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 91.84% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 99.57% (relative success)...
  ==> Spearman    distance from optimal ordering: 99.95% (relative success)...
  ==> Gestalt     distance from optimal ordering: 85.71% (relative success)...
  ==> Mean distance from optimal ordering: 94.27% (relative success)...
Giving the final ranks ...

Final ranking for this environment #0 :
- Player #5, '#5<$\rho^{\mathrm{Rand}}$, BayesUCB>'	was ranked	1 / 6 for this simulation (last rewards = 7118).
- Player #2, '#2<$\rho^{\mathrm{Rand}}$, BayesUCB>'	was ranked	2 / 6 for this simulation (last rewards = 5491).
- Player #3, '#3<$\rho^{\mathrm{Rand}}$, BayesUCB>'	was ranked	3 / 6 for this simulation (last rewards = 5100).
- Player #4, '#4<$\rho^{\mathrm{Rand}}$, BayesUCB>'	was ranked	4 / 6 for this simulation (last rewards = 4988).
- Player #6, '#6<$\rho^{\mathrm{Rand}}$, BayesUCB>'	was ranked	5 / 6 for this simulation (last rewards = 4974).
- Player #1, '#1<$\rho^{\mathrm{Rand}}$, BayesUCB>'	was ranked	6 / 6 for this simulation (last rewards = 3736).



Considering the list of players :
 [rhoLearn(BayesUCB), rhoLearn(BayesUCB), rhoLearn(BayesUCB), rhoLearn(BayesUCB), rhoLearn(BayesUCB), rhoLearn(BayesUCB)]
Number of players in the multi-players game: 6
Time horizon: 10000
Number of repetitions: 1
Sampling rate for saving, delta_t_save: 1
Sampling rate for plotting, delta_t_plot: 1
Number of jobs for parallelization: 2
Using collision model: onlyUniqUserGetsReward
  Detail:  Simple collision model where only the players alone on one arm sample it and receive the reward.

    - This is the default collision model, cf. https://arxiv.org/abs/0910.2065v3 collision model 1.
    - The numpy array 'choices' is increased according to the number of users who collided (it is NOT binary).
    
Creating a new MAB problem ...
  Reading arms of this MAB problem from a dictionnary 'configuration' = {'params': array([ 0.125,  0.25 ,  0.375,  0.5  ,  0.625,  0.75 ,  0.875]), 'arm_type': <class 'Arms.Bernoulli.Bernoulli'>} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = [ 0.125  0.25   0.375  0.5    0.625  0.75   0.875]
 - with 'arms' = [B(0.125), B(0.25), B(0.375), B(0.5), B(0.625), B(0.75), B(0.875)]
 - with 'nbArms' = 7
 - with 'maxArm' = 0.875
 - with 'minArm' = 0.125

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 6.16 ... 
 - a Optimal Arm Identification factor H_OI(mu) = 48.21% ...
Number of environments to try: 1

Evaluating environment: MAB(nbArms: 7, arms: [B(0.125), B(0.25), B(0.375), B(0.5), B(0.625), B(0.75), B(0.875)], minArm: 0.125, maxArm: 0.875)
- Adding player #1 = #1<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ BayesUCB> ...
  Using this already created player 'player' = #1<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ BayesUCB> ...
- Adding player #2 = #2<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ BayesUCB> ...
  Using this already created player 'player' = #2<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ BayesUCB> ...
- Adding player #3 = #3<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ BayesUCB> ...
  Using this already created player 'player' = #3<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ BayesUCB> ...
- Adding player #4 = #4<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ BayesUCB> ...
  Using this already created player 'player' = #4<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ BayesUCB> ...
- Adding player #5 = #5<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ BayesUCB> ...
  Using this already created player 'player' = #5<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ BayesUCB> ...
- Adding player #6 = #6<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ BayesUCB> ...
  Using this already created player 'player' = #6<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ BayesUCB> ...

Estimated order by the policy #1<$\rho^{\mathrm{Learn}}$, BayesUCB, rank: 3 ~ BayesUCB> after 10000 steps: [0 1 2 6 4 3 5] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 75.51% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 94.91% (relative success)...
  ==> Spearman    distance from optimal ordering: 94.78% (relative success)...
  ==> Gestalt     distance from optimal ordering: 57.14% (relative success)...
  ==> Mean distance from optimal ordering: 80.59% (relative success)...

Estimated order by the policy #2<$\rho^{\mathrm{Learn}}$, BayesUCB, rank: 5 ~ BayesUCB> after 10000 steps: [0 1 4 3 5 2 6] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 75.51% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 94.91% (relative success)...
  ==> Spearman    distance from optimal ordering: 94.78% (relative success)...
  ==> Gestalt     distance from optimal ordering: 71.43% (relative success)...
  ==> Mean distance from optimal ordering: 84.16% (relative success)...

Estimated order by the policy #3<$\rho^{\mathrm{Learn}}$, BayesUCB, rank: 4 ~ BayesUCB> after 10000 steps: [2 0 1 5 4 3 6] ...
  ==> Optimal arm identification: 92.59% (relative success)...
  ==> Manhattan   distance from optimal ordering: 67.35% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 90.15% (relative success)...
  ==> Spearman    distance from optimal ordering: 94.78% (relative success)...
  ==> Gestalt     distance from optimal ordering: 57.14% (relative success)...
  ==> Mean distance from optimal ordering: 77.35% (relative success)...

Estimated order by the policy #4<$\rho^{\mathrm{Learn}}$, BayesUCB, rank: 6 ~ BayesUCB> after 10000 steps: [1 3 0 5 4 2 6] ...
  ==> Optimal arm identification: 96.30% (relative success)...
  ==> Manhattan   distance from optimal ordering: 59.18% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 82.35% (relative success)...
  ==> Spearman    distance from optimal ordering: 85.18% (relative success)...
  ==> Gestalt     distance from optimal ordering: 57.14% (relative success)...
  ==> Mean distance from optimal ordering: 70.96% (relative success)...

Estimated order by the policy #5<$\rho^{\mathrm{Learn}}$, BayesUCB, rank: 6 ~ BayesUCB> after 10000 steps: [0 2 1 3 6 5 4] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 75.51% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 94.91% (relative success)...
  ==> Spearman    distance from optimal ordering: 97.66% (relative success)...
  ==> Gestalt     distance from optimal ordering: 57.14% (relative success)...
  ==> Mean distance from optimal ordering: 81.30% (relative success)...

Estimated order by the policy #6<$\rho^{\mathrm{Learn}}$, BayesUCB, rank: 6 ~ BayesUCB> after 10000 steps: [0 1 2 3 4 6 5] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 91.84% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 99.57% (relative success)...
  ==> Spearman    distance from optimal ordering: 99.95% (relative success)...
  ==> Gestalt     distance from optimal ordering: 85.71% (relative success)...
  ==> Mean distance from optimal ordering: 94.27% (relative success)...
Giving the final ranks ...

Final ranking for this environment #0 :
- Player #3, '#3<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ BayesUCB>'	was ranked	1 / 6 for this simulation (last rewards = 6798).
- Player #1, '#1<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ BayesUCB>'	was ranked	2 / 6 for this simulation (last rewards = 6552).
- Player #2, '#2<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ BayesUCB>'	was ranked	3 / 6 for this simulation (last rewards = 5083).
- Player #4, '#4<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ BayesUCB>'	was ranked	4 / 6 for this simulation (last rewards = 4080).
- Player #5, '#5<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ BayesUCB>'	was ranked	5 / 6 for this simulation (last rewards = 3662).
- Player #6, '#6<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ BayesUCB>'	was ranked	6 / 6 for this simulation (last rewards = 1998).


- Plotting the centralized regret for all 'players' values
  - For 6 players, Anandtharam et al. centralized lower-bound gave = 2.59 ...
  - For 6 players, our lower bound gave = 15.5 ...
  - For 6 players, the initial lower bound in Theorem 6 from [Anandkumar et al., 2010] gave = 4.24 ...

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 6.16 for 1-player problem ... 
 - a Optimal Arm Identification factor H_OI(mu) = 48.21% ...
 - [Anandtharam et al] centralized lowerbound = 15.5,
 - Our decentralized lowerbound = 4.24,
 - [Anandkumar et al] decentralized lowerbound = 2.59


- Plotting the centralized regret for all 'players' values, in semilogx scale
  - For 6 players, Anandtharam et al. centralized lower-bound gave = 2.59 ...
  - For 6 players, our lower bound gave = 15.5 ...
  - For 6 players, the initial lower bound in Theorem 6 from [Anandkumar et al., 2010] gave = 4.24 ...

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 6.16 for 1-player problem ... 
 - a Optimal Arm Identification factor H_OI(mu) = 48.21% ...
 - [Anandtharam et al] centralized lowerbound = 15.5,
 - Our decentralized lowerbound = 4.24,
 - [Anandkumar et al] decentralized lowerbound = 2.59


- Plotting the centralized regret for all 'players' values, in semilogy scale
  - For 6 players, Anandtharam et al. centralized lower-bound gave = 2.59 ...
  - For 6 players, our lower bound gave = 15.5 ...
  - For 6 players, the initial lower bound in Theorem 6 from [Anandkumar et al., 2010] gave = 4.24 ...

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 6.16 for 1-player problem ... 
 - a Optimal Arm Identification factor H_OI(mu) = 48.21% ...
 - [Anandtharam et al] centralized lowerbound = 15.5,
 - Our decentralized lowerbound = 4.24,
 - [Anandkumar et al] decentralized lowerbound = 2.59


- Plotting the centralized regret for all 'players' values, in loglog scale
  - For 6 players, Anandtharam et al. centralized lower-bound gave = 2.59 ...
  - For 6 players, our lower bound gave = 15.5 ...
  - For 6 players, the initial lower bound in Theorem 6 from [Anandkumar et al., 2010] gave = 4.24 ...

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 6.16 for 1-player problem ... 
 - a Optimal Arm Identification factor H_OI(mu) = 48.21% ...
 - [Anandtharam et al] centralized lowerbound = 15.5,
 - Our decentralized lowerbound = 4.24,
 - [Anandkumar et al] decentralized lowerbound = 2.59


- Plotting the centralized fairness (STD)
 - Plotting the total nb of collision as a function of time for all 'players' values
No upper bound for the non-cumulated number of collisions...
 - Plotting the cumulated total nb of collision as a function of time for all 'players' values
No upper bound for the non-cumulated number of collisions...


- Plotting the number of switches as a function of time for all 'players' values
Done for simulations main_multiplayers.py ...
