 - Setting dpi of all figures to 110 ...
 - Setting 'figsize' of all figures to (19.8, 10.8) ...
Info: Using the regular tqdm() decorator ...
Info: numba.jit seems to be available.
Info: numba.jit seems to be available.
Loaded experiments configuration from 'configuration.py' :
configuration = {'verbosity': 6, 'n_jobs': 2, 'players': [rhoRand(UCB), rhoRand(UCB), rhoRand(UCB), rhoRand(UCB), rhoRand(UCB), rhoRand(UCB)], 'repetitions': 1, 'delta_t_save': 1, 'horizon': 10000, 'averageOn': 0.001, 'successive_players': [[Selfish(BayesUCB), Selfish(BayesUCB), Selfish(BayesUCB), Selfish(BayesUCB), Selfish(BayesUCB), Selfish(BayesUCB)], [rhoRand(BayesUCB), rhoRand(BayesUCB), rhoRand(BayesUCB), rhoRand(BayesUCB), rhoRand(BayesUCB), rhoRand(BayesUCB)], [rhoEst(BayesUCB), rhoEst(BayesUCB), rhoEst(BayesUCB), rhoEst(BayesUCB), rhoEst(BayesUCB), rhoEst(BayesUCB)], [rhoLearn(BayesUCB), rhoLearn(BayesUCB), rhoLearn(BayesUCB), rhoLearn(BayesUCB), rhoLearn(BayesUCB), rhoLearn(BayesUCB)], [rhoLearn(BayesUCB), rhoLearn(BayesUCB), rhoLearn(BayesUCB), rhoLearn(BayesUCB), rhoLearn(BayesUCB), rhoLearn(BayesUCB)], [rhoLearn(BayesUCB), rhoLearn(BayesUCB), rhoLearn(BayesUCB), rhoLearn(BayesUCB), rhoLearn(BayesUCB), rhoLearn(BayesUCB)], [rhoLearn(BayesUCB), rhoLearn(BayesUCB), rhoLearn(BayesUCB), rhoLearn(BayesUCB), rhoLearn(BayesUCB), rhoLearn(BayesUCB)], [rhoLearn(BayesUCB), rhoLearn(BayesUCB), rhoLearn(BayesUCB), rhoLearn(BayesUCB), rhoLearn(BayesUCB), rhoLearn(BayesUCB)]], 'collisionModel': <function onlyUniqUserGetsReward at 0x7fac11c4c158>, 'finalRanksOnAverage': True, 'environment': [{'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': array([ 0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9])}]}
plots/ is already a directory here...



Considering the list of players :
 [Selfish(BayesUCB), Selfish(BayesUCB), Selfish(BayesUCB), Selfish(BayesUCB), Selfish(BayesUCB), Selfish(BayesUCB)]
Number of players in the multi-players game: 6
Time horizon: 10000
Number of repetitions: 1
Sampling rate for saving, delta_t_save: 1
Sampling rate for plotting, delta_t_plot: 1
Number of jobs for parallelization: 2
Using collision model: onlyUniqUserGetsReward
  Detail:  Simple collision model where only the players alone on one arm sample it and receive the reward.

    - This is the default collision model, cf. https://arxiv.org/abs/0910.2065v3 collision model 1.
    - The numpy array 'choices' is increased according to the number of users who collided (it is NOT binary).
    
Creating a new MAB problem ...
  Reading arms of this MAB problem from a dictionnary 'configuration' = {'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': array([ 0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9])} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = [ 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9]
 - with 'arms' = [B(0.1), B(0.2), B(0.3), B(0.4), B(0.5), B(0.6), B(0.7), B(0.8), B(0.9)]
 - with 'nbArms' = 9
 - with 'maxArm' = 0.9
 - with 'minArm' = 0.1

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 7.52 ... 
 - a Optimal Arm Identification factor H_OI(mu) = 48.89% ...
Number of environments to try: 1

Evaluating environment: MAB(nbArms: 9, arms: [B(0.1), B(0.2), B(0.3), B(0.4), B(0.5), B(0.6), B(0.7), B(0.8), B(0.9)], minArm: 0.1, maxArm: 0.9)
- Adding player #1 = #1<Selfish, BayesUCB> ...
  Using this already created player 'player' = #1<Selfish, BayesUCB> ...
- Adding player #2 = #2<Selfish, BayesUCB> ...
  Using this already created player 'player' = #2<Selfish, BayesUCB> ...
- Adding player #3 = #3<Selfish, BayesUCB> ...
  Using this already created player 'player' = #3<Selfish, BayesUCB> ...
- Adding player #4 = #4<Selfish, BayesUCB> ...
  Using this already created player 'player' = #4<Selfish, BayesUCB> ...
- Adding player #5 = #5<Selfish, BayesUCB> ...
  Using this already created player 'player' = #5<Selfish, BayesUCB> ...
- Adding player #6 = #6<Selfish, BayesUCB> ...
  Using this already created player 'player' = #6<Selfish, BayesUCB> ...

Estimated order by the policy #1<Selfish, BayesUCB> after 10000 steps: [1 5 4 7 0 2 3 6 8] ...
  ==> Optimal arm identification: 82.05% (relative success)...
  ==> Manhattan   distance from optimal ordering: 45.68% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 78.91% (relative success)...
  ==> Spearman    distance from optimal ordering: 71.39% (relative success)...
  ==> Gestalt     distance from optimal ordering: 55.56% (relative success)...
  ==> Mean distance from optimal ordering: 62.88% (relative success)...

Estimated order by the policy #2<Selfish, BayesUCB> after 10000 steps: [0 4 6 8 1 2 3 5 7] ...
  ==> Optimal arm identification: 82.05% (relative success)...
  ==> Manhattan   distance from optimal ordering: 40.74% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 78.91% (relative success)...
  ==> Spearman    distance from optimal ordering: 59.36% (relative success)...
  ==> Gestalt     distance from optimal ordering: 66.67% (relative success)...
  ==> Mean distance from optimal ordering: 61.42% (relative success)...

Estimated order by the policy #3<Selfish, BayesUCB> after 10000 steps: [5 6 8 0 7 2 1 4 3] ...
  ==> Optimal arm identification: 58.97% (relative success)...
  ==> Manhattan   distance from optimal ordering: 6.17% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 59.58% (relative success)...
  ==> Spearman    distance from optimal ordering: 75.60% (relative success)...
  ==> Gestalt     distance from optimal ordering: 33.33% (relative success)...
  ==> Mean distance from optimal ordering: 43.67% (relative success)...

Estimated order by the policy #4<Selfish, BayesUCB> after 10000 steps: [1 5 2 3 4 8 7 0 6] ...
  ==> Optimal arm identification: 87.18% (relative success)...
  ==> Manhattan   distance from optimal ordering: 55.56% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 70.29% (relative success)...
  ==> Spearman    distance from optimal ordering: 61.93% (relative success)...
  ==> Gestalt     distance from optimal ordering: 55.56% (relative success)...
  ==> Mean distance from optimal ordering: 60.83% (relative success)...

Estimated order by the policy #5<Selfish, BayesUCB> after 10000 steps: [0 3 5 6 7 8 1 2 4] ...
  ==> Optimal arm identification: 87.18% (relative success)...
  ==> Manhattan   distance from optimal ordering: 30.86% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 59.58% (relative success)...
  ==> Spearman    distance from optimal ordering: 23.50% (relative success)...
  ==> Gestalt     distance from optimal ordering: 66.67% (relative success)...
  ==> Mean distance from optimal ordering: 45.15% (relative success)...

Estimated order by the policy #6<Selfish, BayesUCB> after 10000 steps: [1 4 7 8 6 0 2 3 5] ...
  ==> Optimal arm identification: 76.92% (relative success)...
  ==> Manhattan   distance from optimal ordering: 20.99% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 0.00% (relative success)...
  ==> Spearman    distance from optimal ordering: 16.88% (relative success)...
  ==> Gestalt     distance from optimal ordering: 44.44% (relative success)...
  ==> Mean distance from optimal ordering: 20.58% (relative success)...
Giving the final ranks ...

Final ranking for this environment #0 :
- Player #1, '#1<Selfish, BayesUCB>'	was ranked	1 / 6 for this simulation (last rewards = 8942).
- Player #2, '#2<Selfish, BayesUCB>'	was ranked	2 / 6 for this simulation (last rewards = 7967).
- Player #4, '#4<Selfish, BayesUCB>'	was ranked	3 / 6 for this simulation (last rewards = 6886).
- Player #6, '#6<Selfish, BayesUCB>'	was ranked	4 / 6 for this simulation (last rewards = 5928).
- Player #5, '#5<Selfish, BayesUCB>'	was ranked	5 / 6 for this simulation (last rewards = 4908).
- Player #3, '#3<Selfish, BayesUCB>'	was ranked	6 / 6 for this simulation (last rewards = 3882).



Considering the list of players :
 [rhoRand(BayesUCB), rhoRand(BayesUCB), rhoRand(BayesUCB), rhoRand(BayesUCB), rhoRand(BayesUCB), rhoRand(BayesUCB)]
Number of players in the multi-players game: 6
Time horizon: 10000
Number of repetitions: 1
Sampling rate for saving, delta_t_save: 1
Sampling rate for plotting, delta_t_plot: 1
Number of jobs for parallelization: 2
Using collision model: onlyUniqUserGetsReward
  Detail:  Simple collision model where only the players alone on one arm sample it and receive the reward.

    - This is the default collision model, cf. https://arxiv.org/abs/0910.2065v3 collision model 1.
    - The numpy array 'choices' is increased according to the number of users who collided (it is NOT binary).
    
Creating a new MAB problem ...
  Reading arms of this MAB problem from a dictionnary 'configuration' = {'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': array([ 0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9])} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = [ 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9]
 - with 'arms' = [B(0.1), B(0.2), B(0.3), B(0.4), B(0.5), B(0.6), B(0.7), B(0.8), B(0.9)]
 - with 'nbArms' = 9
 - with 'maxArm' = 0.9
 - with 'minArm' = 0.1

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 7.52 ... 
 - a Optimal Arm Identification factor H_OI(mu) = 48.89% ...
Number of environments to try: 1

Evaluating environment: MAB(nbArms: 9, arms: [B(0.1), B(0.2), B(0.3), B(0.4), B(0.5), B(0.6), B(0.7), B(0.8), B(0.9)], minArm: 0.1, maxArm: 0.9)
- Adding player #1 = #1<$\rho^{\mathrm{Rand}}$, BayesUCB> ...
  Using this already created player 'player' = #1<$\rho^{\mathrm{Rand}}$, BayesUCB> ...
- Adding player #2 = #2<$\rho^{\mathrm{Rand}}$, BayesUCB> ...
  Using this already created player 'player' = #2<$\rho^{\mathrm{Rand}}$, BayesUCB> ...
- Adding player #3 = #3<$\rho^{\mathrm{Rand}}$, BayesUCB> ...
  Using this already created player 'player' = #3<$\rho^{\mathrm{Rand}}$, BayesUCB> ...
- Adding player #4 = #4<$\rho^{\mathrm{Rand}}$, BayesUCB> ...
  Using this already created player 'player' = #4<$\rho^{\mathrm{Rand}}$, BayesUCB> ...
- Adding player #5 = #5<$\rho^{\mathrm{Rand}}$, BayesUCB> ...
  Using this already created player 'player' = #5<$\rho^{\mathrm{Rand}}$, BayesUCB> ...
- Adding player #6 = #6<$\rho^{\mathrm{Rand}}$, BayesUCB> ...
  Using this already created player 'player' = #6<$\rho^{\mathrm{Rand}}$, BayesUCB> ...

Estimated order by the policy #1<$\rho^{\mathrm{Rand}}$, BayesUCB, rank:5> after 10000 steps: [0 1 3 2 4 5 7 6 8] ...
  ==> Optimal arm identification: 97.44% (relative success)...
  ==> Manhattan   distance from optimal ordering: 90.12% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 99.92% (relative success)...
  ==> Spearman    distance from optimal ordering: 100.00% (relative success)...
  ==> Gestalt     distance from optimal ordering: 77.78% (relative success)...
  ==> Mean distance from optimal ordering: 91.95% (relative success)...

Estimated order by the policy #2<$\rho^{\mathrm{Rand}}$, BayesUCB, rank:1> after 10000 steps: [2 1 0 3 4 5 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 90.12% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 99.82% (relative success)...
  ==> Spearman    distance from optimal ordering: 99.98% (relative success)...
  ==> Gestalt     distance from optimal ordering: 77.78% (relative success)...
  ==> Mean distance from optimal ordering: 91.93% (relative success)...

Estimated order by the policy #3<$\rho^{\mathrm{Rand}}$, BayesUCB, rank:4> after 10000 steps: [0 3 2 1 4 5 6 7 8] ...
  ==> Optimal arm identification: 94.87% (relative success)...
  ==> Manhattan   distance from optimal ordering: 90.12% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 99.82% (relative success)...
  ==> Spearman    distance from optimal ordering: 99.98% (relative success)...
  ==> Gestalt     distance from optimal ordering: 77.78% (relative success)...
  ==> Mean distance from optimal ordering: 91.93% (relative success)...

Estimated order by the policy #4<$\rho^{\mathrm{Rand}}$, BayesUCB, rank:6> after 10000 steps: [1 2 0 3 4 5 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 90.12% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 99.92% (relative success)...
  ==> Spearman    distance from optimal ordering: 99.99% (relative success)...
  ==> Gestalt     distance from optimal ordering: 88.89% (relative success)...
  ==> Mean distance from optimal ordering: 94.73% (relative success)...

Estimated order by the policy #5<$\rho^{\mathrm{Rand}}$, BayesUCB, rank:3> after 10000 steps: [2 3 1 0 4 5 6 8 7] ...
  ==> Optimal arm identification: 92.31% (relative success)...
  ==> Manhattan   distance from optimal ordering: 75.31% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 98.77% (relative success)...
  ==> Spearman    distance from optimal ordering: 99.47% (relative success)...
  ==> Gestalt     distance from optimal ordering: 66.67% (relative success)...
  ==> Mean distance from optimal ordering: 85.05% (relative success)...

Estimated order by the policy #6<$\rho^{\mathrm{Rand}}$, BayesUCB, rank:2> after 10000 steps: [1 0 3 2 4 6 5 7 8] ...
  ==> Optimal arm identification: 97.44% (relative success)...
  ==> Manhattan   distance from optimal ordering: 85.19% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 99.82% (relative success)...
  ==> Spearman    distance from optimal ordering: 99.99% (relative success)...
  ==> Gestalt     distance from optimal ordering: 66.67% (relative success)...
  ==> Mean distance from optimal ordering: 87.92% (relative success)...
Giving the final ranks ...

Final ranking for this environment #0 :
- Player #6, '#6<$\rho^{\mathrm{Rand}}$, BayesUCB>'	was ranked	1 / 6 for this simulation (last rewards = 7349).
- Player #2, '#2<$\rho^{\mathrm{Rand}}$, BayesUCB>'	was ranked	2 / 6 for this simulation (last rewards = 6635).
- Player #5, '#5<$\rho^{\mathrm{Rand}}$, BayesUCB>'	was ranked	3 / 6 for this simulation (last rewards = 6150).
- Player #3, '#3<$\rho^{\mathrm{Rand}}$, BayesUCB>'	was ranked	4 / 6 for this simulation (last rewards = 5592).
- Player #4, '#4<$\rho^{\mathrm{Rand}}$, BayesUCB>'	was ranked	5 / 6 for this simulation (last rewards = 5148).
- Player #1, '#1<$\rho^{\mathrm{Rand}}$, BayesUCB>'	was ranked	6 / 6 for this simulation (last rewards = 4758).



Considering the list of players :
 [rhoEst(BayesUCB), rhoEst(BayesUCB), rhoEst(BayesUCB), rhoEst(BayesUCB), rhoEst(BayesUCB), rhoEst(BayesUCB)]
Number of players in the multi-players game: 6
Time horizon: 10000
Number of repetitions: 1
Sampling rate for saving, delta_t_save: 1
Sampling rate for plotting, delta_t_plot: 1
Number of jobs for parallelization: 2
Using collision model: onlyUniqUserGetsReward
  Detail:  Simple collision model where only the players alone on one arm sample it and receive the reward.

    - This is the default collision model, cf. https://arxiv.org/abs/0910.2065v3 collision model 1.
    - The numpy array 'choices' is increased according to the number of users who collided (it is NOT binary).
    
Creating a new MAB problem ...
  Reading arms of this MAB problem from a dictionnary 'configuration' = {'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': array([ 0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9])} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = [ 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9]
 - with 'arms' = [B(0.1), B(0.2), B(0.3), B(0.4), B(0.5), B(0.6), B(0.7), B(0.8), B(0.9)]
 - with 'nbArms' = 9
 - with 'maxArm' = 0.9
 - with 'minArm' = 0.1

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 7.52 ... 
 - a Optimal Arm Identification factor H_OI(mu) = 48.89% ...
Number of environments to try: 1

Evaluating environment: MAB(nbArms: 9, arms: [B(0.1), B(0.2), B(0.3), B(0.4), B(0.5), B(0.6), B(0.7), B(0.8), B(0.9)], minArm: 0.1, maxArm: 0.9)
- Adding player #1 = #1<$\rho^{\mathrm{Est}}$, BayesUCB> ...
  Using this already created player 'player' = #1<$\rho^{\mathrm{Est}}$, BayesUCB> ...
- Adding player #2 = #2<$\rho^{\mathrm{Est}}$, BayesUCB> ...
  Using this already created player 'player' = #2<$\rho^{\mathrm{Est}}$, BayesUCB> ...
- Adding player #3 = #3<$\rho^{\mathrm{Est}}$, BayesUCB> ...
  Using this already created player 'player' = #3<$\rho^{\mathrm{Est}}$, BayesUCB> ...
- Adding player #4 = #4<$\rho^{\mathrm{Est}}$, BayesUCB> ...
  Using this already created player 'player' = #4<$\rho^{\mathrm{Est}}$, BayesUCB> ...
- Adding player #5 = #5<$\rho^{\mathrm{Est}}$, BayesUCB> ...
  Using this already created player 'player' = #5<$\rho^{\mathrm{Est}}$, BayesUCB> ...
- Adding player #6 = #6<$\rho^{\mathrm{Est}}$, BayesUCB> ...
  Using this already created player 'player' = #6<$\rho^{\mathrm{Est}}$, BayesUCB> ...

Estimated order by the policy #1<$\rho^{\mathrm{Est}}$, BayesUCB, rank:2> after 10000 steps: [0 3 1 4 5 6 2 7 8] ...
  ==> Optimal arm identification: 97.44% (relative success)...
  ==> Manhattan   distance from optimal ordering: 75.31% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 99.33% (relative success)...
  ==> Spearman    distance from optimal ordering: 99.04% (relative success)...
  ==> Gestalt     distance from optimal ordering: 77.78% (relative success)...
  ==> Mean distance from optimal ordering: 87.86% (relative success)...

Estimated order by the policy #2<$\rho^{\mathrm{Est}}$, BayesUCB, rank:1> after 10000 steps: [0 1 3 4 6 5 2 7 8] ...
  ==> Optimal arm identification: 97.44% (relative success)...
  ==> Manhattan   distance from optimal ordering: 80.25% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 99.33% (relative success)...
  ==> Spearman    distance from optimal ordering: 99.28% (relative success)...
  ==> Gestalt     distance from optimal ordering: 77.78% (relative success)...
  ==> Mean distance from optimal ordering: 89.16% (relative success)...

Estimated order by the policy #3<$\rho^{\mathrm{Est}}$, BayesUCB, rank:1> after 10000 steps: [0 1 2 4 6 5 3 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 85.19% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 99.65% (relative success)...
  ==> Spearman    distance from optimal ordering: 99.84% (relative success)...
  ==> Gestalt     distance from optimal ordering: 77.78% (relative success)...
  ==> Mean distance from optimal ordering: 90.61% (relative success)...

Estimated order by the policy #4<$\rho^{\mathrm{Est}}$, BayesUCB, rank:2> after 10000 steps: [0 4 5 1 2 3 6 7 8] ...
  ==> Optimal arm identification: 84.62% (relative success)...
  ==> Manhattan   distance from optimal ordering: 70.37% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 98.77% (relative success)...
  ==> Spearman    distance from optimal ordering: 98.01% (relative success)...
  ==> Gestalt     distance from optimal ordering: 77.78% (relative success)...
  ==> Mean distance from optimal ordering: 86.23% (relative success)...

Estimated order by the policy #5<$\rho^{\mathrm{Est}}$, BayesUCB, rank:2> after 10000 steps: [2 5 0 1 3 4 6 7 8] ...
  ==> Optimal arm identification: 89.74% (relative success)...
  ==> Manhattan   distance from optimal ordering: 70.37% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 98.77% (relative success)...
  ==> Spearman    distance from optimal ordering: 98.01% (relative success)...
  ==> Gestalt     distance from optimal ordering: 77.78% (relative success)...
  ==> Mean distance from optimal ordering: 86.23% (relative success)...

Estimated order by the policy #6<$\rho^{\mathrm{Est}}$, BayesUCB, rank:1> after 10000 steps: [0 4 3 1 2 6 5 7 8] ...
  ==> Optimal arm identification: 89.74% (relative success)...
  ==> Manhattan   distance from optimal ordering: 75.31% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 98.77% (relative success)...
  ==> Spearman    distance from optimal ordering: 99.47% (relative success)...
  ==> Gestalt     distance from optimal ordering: 66.67% (relative success)...
  ==> Mean distance from optimal ordering: 85.05% (relative success)...
Giving the final ranks ...

Final ranking for this environment #0 :
- Player #4, '#4<$\rho^{\mathrm{Est}}$, BayesUCB>'	was ranked	1 / 6 for this simulation (last rewards = 349).
- Player #1, '#1<$\rho^{\mathrm{Est}}$, BayesUCB>'	was ranked	2 / 6 for this simulation (last rewards = 345).
- Player #5, '#5<$\rho^{\mathrm{Est}}$, BayesUCB>'	was ranked	3 / 6 for this simulation (last rewards = 337).
- Player #6, '#6<$\rho^{\mathrm{Est}}$, BayesUCB>'	was ranked	4 / 6 for this simulation (last rewards = 321).
- Player #3, '#3<$\rho^{\mathrm{Est}}$, BayesUCB>'	was ranked	5 / 6 for this simulation (last rewards = 320).
- Player #2, '#2<$\rho^{\mathrm{Est}}$, BayesUCB>'	was ranked	6 / 6 for this simulation (last rewards = 319).



Considering the list of players :
 [rhoLearn(BayesUCB), rhoLearn(BayesUCB), rhoLearn(BayesUCB), rhoLearn(BayesUCB), rhoLearn(BayesUCB), rhoLearn(BayesUCB)]
Number of players in the multi-players game: 6
Time horizon: 10000
Number of repetitions: 1
Sampling rate for saving, delta_t_save: 1
Sampling rate for plotting, delta_t_plot: 1
Number of jobs for parallelization: 2
Using collision model: onlyUniqUserGetsReward
  Detail:  Simple collision model where only the players alone on one arm sample it and receive the reward.

    - This is the default collision model, cf. https://arxiv.org/abs/0910.2065v3 collision model 1.
    - The numpy array 'choices' is increased according to the number of users who collided (it is NOT binary).
    
Creating a new MAB problem ...
  Reading arms of this MAB problem from a dictionnary 'configuration' = {'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': array([ 0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9])} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = [ 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9]
 - with 'arms' = [B(0.1), B(0.2), B(0.3), B(0.4), B(0.5), B(0.6), B(0.7), B(0.8), B(0.9)]
 - with 'nbArms' = 9
 - with 'maxArm' = 0.9
 - with 'minArm' = 0.1

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 7.52 ... 
 - a Optimal Arm Identification factor H_OI(mu) = 48.89% ...
Number of environments to try: 1

Evaluating environment: MAB(nbArms: 9, arms: [B(0.1), B(0.2), B(0.3), B(0.4), B(0.5), B(0.6), B(0.7), B(0.8), B(0.9)], minArm: 0.1, maxArm: 0.9)
- Adding player #1 = #1<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ Softmax(decreasing)> ...
  Using this already created player 'player' = #1<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ Softmax(decreasing)> ...
- Adding player #2 = #2<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ Softmax(decreasing)> ...
  Using this already created player 'player' = #2<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ Softmax(decreasing)> ...
- Adding player #3 = #3<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ Softmax(decreasing)> ...
  Using this already created player 'player' = #3<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ Softmax(decreasing)> ...
- Adding player #4 = #4<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ Softmax(decreasing)> ...
  Using this already created player 'player' = #4<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ Softmax(decreasing)> ...
- Adding player #5 = #5<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ Softmax(decreasing)> ...
  Using this already created player 'player' = #5<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ Softmax(decreasing)> ...
- Adding player #6 = #6<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ Softmax(decreasing)> ...
  Using this already created player 'player' = #6<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ Softmax(decreasing)> ...

Estimated order by the policy #1<$\rho^{\mathrm{Learn}}$, BayesUCB, rank: 3 ~ Softmax(decreasing)> after 10000 steps: [0 5 3 1 2 6 8 7 4] ...
  ==> Optimal arm identification: 87.18% (relative success)...
  ==> Manhattan   distance from optimal ordering: 60.49% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 90.47% (relative success)...
  ==> Spearman    distance from optimal ordering: 92.31% (relative success)...
  ==> Gestalt     distance from optimal ordering: 55.56% (relative success)...
  ==> Mean distance from optimal ordering: 74.71% (relative success)...

Estimated order by the policy #2<$\rho^{\mathrm{Learn}}$, BayesUCB, rank: 5 ~ Softmax(decreasing)> after 10000 steps: [0 1 2 3 4 5 7 6 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 95.06% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 99.96% (relative success)...
  ==> Spearman    distance from optimal ordering: 100.00% (relative success)...
  ==> Gestalt     distance from optimal ordering: 88.89% (relative success)...
  ==> Mean distance from optimal ordering: 95.98% (relative success)...

Estimated order by the policy #3<$\rho^{\mathrm{Learn}}$, BayesUCB, rank: 5 ~ Softmax(decreasing)> after 10000 steps: [2 0 1 5 3 4 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 80.25% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 99.65% (relative success)...
  ==> Spearman    distance from optimal ordering: 99.91% (relative success)...
  ==> Gestalt     distance from optimal ordering: 77.78% (relative success)...
  ==> Mean distance from optimal ordering: 89.39% (relative success)...

Estimated order by the policy #4<$\rho^{\mathrm{Learn}}$, BayesUCB, rank: 5 ~ Softmax(decreasing)> after 10000 steps: [0 1 2 3 4 7 6 5 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 90.12% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 99.82% (relative success)...
  ==> Spearman    distance from optimal ordering: 99.98% (relative success)...
  ==> Gestalt     distance from optimal ordering: 77.78% (relative success)...
  ==> Mean distance from optimal ordering: 91.93% (relative success)...

Estimated order by the policy #5<$\rho^{\mathrm{Learn}}$, BayesUCB, rank: 4 ~ Softmax(decreasing)> after 10000 steps: [0 2 1 3 5 6 4 8 7] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 80.25% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 99.65% (relative success)...
  ==> Spearman    distance from optimal ordering: 99.95% (relative success)...
  ==> Gestalt     distance from optimal ordering: 66.67% (relative success)...
  ==> Mean distance from optimal ordering: 86.63% (relative success)...

Estimated order by the policy #6<$\rho^{\mathrm{Learn}}$, BayesUCB, rank: 4 ~ Softmax(decreasing)> after 10000 steps: [0 1 2 3 6 7 5 4 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 80.25% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 99.33% (relative success)...
  ==> Spearman    distance from optimal ordering: 99.63% (relative success)...
  ==> Gestalt     distance from optimal ordering: 77.78% (relative success)...
  ==> Mean distance from optimal ordering: 89.25% (relative success)...
Giving the final ranks ...

Final ranking for this environment #0 :
- Player #6, '#6<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ Softmax(decreasing)>'	was ranked	1 / 6 for this simulation (last rewards = 5937).
- Player #1, '#1<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ Softmax(decreasing)>'	was ranked	2 / 6 for this simulation (last rewards = 5503).
- Player #4, '#4<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ Softmax(decreasing)>'	was ranked	3 / 6 for this simulation (last rewards = 2660).
- Player #3, '#3<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ Softmax(decreasing)>'	was ranked	4 / 6 for this simulation (last rewards = 2557).
- Player #2, '#2<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ Softmax(decreasing)>'	was ranked	5 / 6 for this simulation (last rewards = 1254).
- Player #5, '#5<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ Softmax(decreasing)>'	was ranked	6 / 6 for this simulation (last rewards = 1142).



Considering the list of players :
 [rhoLearn(BayesUCB), rhoLearn(BayesUCB), rhoLearn(BayesUCB), rhoLearn(BayesUCB), rhoLearn(BayesUCB), rhoLearn(BayesUCB)]
Number of players in the multi-players game: 6
Time horizon: 10000
Number of repetitions: 1
Sampling rate for saving, delta_t_save: 1
Sampling rate for plotting, delta_t_plot: 1
Number of jobs for parallelization: 2
Using collision model: onlyUniqUserGetsReward
  Detail:  Simple collision model where only the players alone on one arm sample it and receive the reward.

    - This is the default collision model, cf. https://arxiv.org/abs/0910.2065v3 collision model 1.
    - The numpy array 'choices' is increased according to the number of users who collided (it is NOT binary).
    
Creating a new MAB problem ...
  Reading arms of this MAB problem from a dictionnary 'configuration' = {'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': array([ 0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9])} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = [ 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9]
 - with 'arms' = [B(0.1), B(0.2), B(0.3), B(0.4), B(0.5), B(0.6), B(0.7), B(0.8), B(0.9)]
 - with 'nbArms' = 9
 - with 'maxArm' = 0.9
 - with 'minArm' = 0.1

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 7.52 ... 
 - a Optimal Arm Identification factor H_OI(mu) = 48.89% ...
Number of environments to try: 1

Evaluating environment: MAB(nbArms: 9, arms: [B(0.1), B(0.2), B(0.3), B(0.4), B(0.5), B(0.6), B(0.7), B(0.8), B(0.9)], minArm: 0.1, maxArm: 0.9)
- Adding player #1 = #1<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ UCB($\alpha=1$)> ...
  Using this already created player 'player' = #1<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ UCB($\alpha=1$)> ...
- Adding player #2 = #2<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ UCB($\alpha=1$)> ...
  Using this already created player 'player' = #2<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ UCB($\alpha=1$)> ...
- Adding player #3 = #3<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ UCB($\alpha=1$)> ...
  Using this already created player 'player' = #3<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ UCB($\alpha=1$)> ...
- Adding player #4 = #4<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ UCB($\alpha=1$)> ...
  Using this already created player 'player' = #4<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ UCB($\alpha=1$)> ...
- Adding player #5 = #5<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ UCB($\alpha=1$)> ...
  Using this already created player 'player' = #5<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ UCB($\alpha=1$)> ...
- Adding player #6 = #6<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ UCB($\alpha=1$)> ...
  Using this already created player 'player' = #6<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ UCB($\alpha=1$)> ...

Estimated order by the policy #1<$\rho^{\mathrm{Learn}}$, BayesUCB, rank: 4 ~ UCB($\alpha=1$)> after 10000 steps: [0 5 2 3 1 6 4 7 8] ...
  ==> Optimal arm identification: 89.74% (relative success)...
  ==> Manhattan   distance from optimal ordering: 75.31% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 97.82% (relative success)...
  ==> Spearman    distance from optimal ordering: 98.01% (relative success)...
  ==> Gestalt     distance from optimal ordering: 66.67% (relative success)...
  ==> Mean distance from optimal ordering: 84.45% (relative success)...

Estimated order by the policy #2<$\rho^{\mathrm{Learn}}$, BayesUCB, rank: 6 ~ UCB($\alpha=1$)> after 10000 steps: [2 0 1 5 4 3 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 80.25% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 99.33% (relative success)...
  ==> Spearman    distance from optimal ordering: 99.84% (relative success)...
  ==> Gestalt     distance from optimal ordering: 66.67% (relative success)...
  ==> Mean distance from optimal ordering: 86.52% (relative success)...

Estimated order by the policy #3<$\rho^{\mathrm{Learn}}$, BayesUCB, rank: 6 ~ UCB($\alpha=1$)> after 10000 steps: [0 1 2 3 6 4 7 5 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 85.19% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 99.82% (relative success)...
  ==> Spearman    distance from optimal ordering: 99.95% (relative success)...
  ==> Gestalt     distance from optimal ordering: 77.78% (relative success)...
  ==> Mean distance from optimal ordering: 90.68% (relative success)...

Estimated order by the policy #4<$\rho^{\mathrm{Learn}}$, BayesUCB, rank: 4 ~ UCB($\alpha=1$)> after 10000 steps: [1 3 0 2 5 7 6 4 8] ...
  ==> Optimal arm identification: 97.44% (relative success)...
  ==> Manhattan   distance from optimal ordering: 70.37% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 97.82% (relative success)...
  ==> Spearman    distance from optimal ordering: 99.04% (relative success)...
  ==> Gestalt     distance from optimal ordering: 55.56% (relative success)...
  ==> Mean distance from optimal ordering: 80.70% (relative success)...

Estimated order by the policy #5<$\rho^{\mathrm{Learn}}$, BayesUCB, rank: 6 ~ UCB($\alpha=1$)> after 10000 steps: [1 0 2 4 6 3 5 8 7] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 75.31% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 99.33% (relative success)...
  ==> Spearman    distance from optimal ordering: 99.84% (relative success)...
  ==> Gestalt     distance from optimal ordering: 55.56% (relative success)...
  ==> Mean distance from optimal ordering: 82.51% (relative success)...

Estimated order by the policy #6<$\rho^{\mathrm{Learn}}$, BayesUCB, rank: 5 ~ UCB($\alpha=1$)> after 10000 steps: [1 0 2 3 8 6 7 5 4] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 65.43% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 90.47% (relative success)...
  ==> Spearman    distance from optimal ordering: 95.01% (relative success)...
  ==> Gestalt     distance from optimal ordering: 55.56% (relative success)...
  ==> Mean distance from optimal ordering: 76.62% (relative success)...
Giving the final ranks ...

Final ranking for this environment #0 :
- Player #6, '#6<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ UCB($\alpha=1$)>'	was ranked	1 / 6 for this simulation (last rewards = 7992).
- Player #4, '#4<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ UCB($\alpha=1$)>'	was ranked	2 / 6 for this simulation (last rewards = 7373).
- Player #1, '#1<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ UCB($\alpha=1$)>'	was ranked	3 / 6 for this simulation (last rewards = 5706).
- Player #2, '#2<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ UCB($\alpha=1$)>'	was ranked	4 / 6 for this simulation (last rewards = 5121).
- Player #5, '#5<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ UCB($\alpha=1$)>'	was ranked	5 / 6 for this simulation (last rewards = 4544).
- Player #3, '#3<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ UCB($\alpha=1$)>'	was ranked	6 / 6 for this simulation (last rewards = 3375).



Considering the list of players :
 [rhoLearn(BayesUCB), rhoLearn(BayesUCB), rhoLearn(BayesUCB), rhoLearn(BayesUCB), rhoLearn(BayesUCB), rhoLearn(BayesUCB)]
Number of players in the multi-players game: 6
Time horizon: 10000
Number of repetitions: 1
Sampling rate for saving, delta_t_save: 1
Sampling rate for plotting, delta_t_plot: 1
Number of jobs for parallelization: 2
Using collision model: onlyUniqUserGetsReward
  Detail:  Simple collision model where only the players alone on one arm sample it and receive the reward.

    - This is the default collision model, cf. https://arxiv.org/abs/0910.2065v3 collision model 1.
    - The numpy array 'choices' is increased according to the number of users who collided (it is NOT binary).
    
Creating a new MAB problem ...
  Reading arms of this MAB problem from a dictionnary 'configuration' = {'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': array([ 0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9])} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = [ 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9]
 - with 'arms' = [B(0.1), B(0.2), B(0.3), B(0.4), B(0.5), B(0.6), B(0.7), B(0.8), B(0.9)]
 - with 'nbArms' = 9
 - with 'maxArm' = 0.9
 - with 'minArm' = 0.1

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 7.52 ... 
 - a Optimal Arm Identification factor H_OI(mu) = 48.89% ...
Number of environments to try: 1

Evaluating environment: MAB(nbArms: 9, arms: [B(0.1), B(0.2), B(0.3), B(0.4), B(0.5), B(0.6), B(0.7), B(0.8), B(0.9)], minArm: 0.1, maxArm: 0.9)
- Adding player #1 = #1<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ Thompson> ...
  Using this already created player 'player' = #1<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ Thompson> ...
- Adding player #2 = #2<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ Thompson> ...
  Using this already created player 'player' = #2<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ Thompson> ...
- Adding player #3 = #3<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ Thompson> ...
  Using this already created player 'player' = #3<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ Thompson> ...
- Adding player #4 = #4<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ Thompson> ...
  Using this already created player 'player' = #4<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ Thompson> ...
- Adding player #5 = #5<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ Thompson> ...
  Using this already created player 'player' = #5<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ Thompson> ...
- Adding player #6 = #6<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ Thompson> ...
  Using this already created player 'player' = #6<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ Thompson> ...

Estimated order by the policy #1<$\rho^{\mathrm{Learn}}$, BayesUCB, rank: 6 ~ Thompson> after 10000 steps: [2 0 1 4 3 5 7 6 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 80.25% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 99.65% (relative success)...
  ==> Spearman    distance from optimal ordering: 99.95% (relative success)...
  ==> Gestalt     distance from optimal ordering: 66.67% (relative success)...
  ==> Mean distance from optimal ordering: 86.63% (relative success)...

Estimated order by the policy #2<$\rho^{\mathrm{Learn}}$, BayesUCB, rank: 5 ~ Thompson> after 10000 steps: [0 2 1 3 6 4 8 5 7] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 75.31% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 99.33% (relative success)...
  ==> Spearman    distance from optimal ordering: 99.75% (relative success)...
  ==> Gestalt     distance from optimal ordering: 55.56% (relative success)...
  ==> Mean distance from optimal ordering: 82.49% (relative success)...

Estimated order by the policy #3<$\rho^{\mathrm{Learn}}$, BayesUCB, rank: 5 ~ Thompson> after 10000 steps: [0 2 1 6 8 5 3 4 7] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 60.49% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 90.47% (relative success)...
  ==> Spearman    distance from optimal ordering: 92.31% (relative success)...
  ==> Gestalt     distance from optimal ordering: 55.56% (relative success)...
  ==> Mean distance from optimal ordering: 74.71% (relative success)...

Estimated order by the policy #4<$\rho^{\mathrm{Learn}}$, BayesUCB, rank: 6 ~ Thompson> after 10000 steps: [0 1 2 3 4 5 7 8 6] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 90.12% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 99.92% (relative success)...
  ==> Spearman    distance from optimal ordering: 99.99% (relative success)...
  ==> Gestalt     distance from optimal ordering: 88.89% (relative success)...
  ==> Mean distance from optimal ordering: 94.73% (relative success)...

Estimated order by the policy #5<$\rho^{\mathrm{Learn}}$, BayesUCB, rank: 5 ~ Thompson> after 10000 steps: [0 1 2 3 5 4 8 7 6] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 85.19% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 99.65% (relative success)...
  ==> Spearman    distance from optimal ordering: 99.95% (relative success)...
  ==> Gestalt     distance from optimal ordering: 66.67% (relative success)...
  ==> Mean distance from optimal ordering: 87.86% (relative success)...

Estimated order by the policy #6<$\rho^{\mathrm{Learn}}$, BayesUCB, rank: 4 ~ Thompson> after 10000 steps: [2 5 1 0 3 7 6 4 8] ...
  ==> Optimal arm identification: 87.18% (relative success)...
  ==> Manhattan   distance from optimal ordering: 60.49% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 90.47% (relative success)...
  ==> Spearman    distance from optimal ordering: 93.29% (relative success)...
  ==> Gestalt     distance from optimal ordering: 44.44% (relative success)...
  ==> Mean distance from optimal ordering: 72.18% (relative success)...
Giving the final ranks ...

Final ranking for this environment #0 :
- Player #3, '#3<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ Thompson>'	was ranked	1 / 6 for this simulation (last rewards = 7805).
- Player #6, '#6<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ Thompson>'	was ranked	2 / 6 for this simulation (last rewards = 7527).
- Player #2, '#2<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ Thompson>'	was ranked	3 / 6 for this simulation (last rewards = 5927).
- Player #5, '#5<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ Thompson>'	was ranked	4 / 6 for this simulation (last rewards = 4724).
- Player #1, '#1<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ Thompson>'	was ranked	5 / 6 for this simulation (last rewards = 4613).
- Player #4, '#4<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ Thompson>'	was ranked	6 / 6 for this simulation (last rewards = 2966).



Considering the list of players :
 [rhoLearn(BayesUCB), rhoLearn(BayesUCB), rhoLearn(BayesUCB), rhoLearn(BayesUCB), rhoLearn(BayesUCB), rhoLearn(BayesUCB)]
Number of players in the multi-players game: 6
Time horizon: 10000
Number of repetitions: 1
Sampling rate for saving, delta_t_save: 1
Sampling rate for plotting, delta_t_plot: 1
Number of jobs for parallelization: 2
Using collision model: onlyUniqUserGetsReward
  Detail:  Simple collision model where only the players alone on one arm sample it and receive the reward.

    - This is the default collision model, cf. https://arxiv.org/abs/0910.2065v3 collision model 1.
    - The numpy array 'choices' is increased according to the number of users who collided (it is NOT binary).
    
Creating a new MAB problem ...
  Reading arms of this MAB problem from a dictionnary 'configuration' = {'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': array([ 0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9])} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = [ 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9]
 - with 'arms' = [B(0.1), B(0.2), B(0.3), B(0.4), B(0.5), B(0.6), B(0.7), B(0.8), B(0.9)]
 - with 'nbArms' = 9
 - with 'maxArm' = 0.9
 - with 'minArm' = 0.1

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 7.52 ... 
 - a Optimal Arm Identification factor H_OI(mu) = 48.89% ...
Number of environments to try: 1

Evaluating environment: MAB(nbArms: 9, arms: [B(0.1), B(0.2), B(0.3), B(0.4), B(0.5), B(0.6), B(0.7), B(0.8), B(0.9)], minArm: 0.1, maxArm: 0.9)
- Adding player #1 = #1<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ KL-UCB+(Bern)> ...
  Using this already created player 'player' = #1<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ KL-UCB+(Bern)> ...
- Adding player #2 = #2<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ KL-UCB+(Bern)> ...
  Using this already created player 'player' = #2<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ KL-UCB+(Bern)> ...
- Adding player #3 = #3<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ KL-UCB+(Bern)> ...
  Using this already created player 'player' = #3<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ KL-UCB+(Bern)> ...
- Adding player #4 = #4<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ KL-UCB+(Bern)> ...
  Using this already created player 'player' = #4<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ KL-UCB+(Bern)> ...
- Adding player #5 = #5<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ KL-UCB+(Bern)> ...
  Using this already created player 'player' = #5<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ KL-UCB+(Bern)> ...
- Adding player #6 = #6<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ KL-UCB+(Bern)> ...
  Using this already created player 'player' = #6<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ KL-UCB+(Bern)> ...

Estimated order by the policy #1<$\rho^{\mathrm{Learn}}$, BayesUCB, rank: 6 ~ KL-UCB+(Bern)> after 10000 steps: [0 2 1 7 5 3 4 6 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 70.37% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 97.82% (relative success)...
  ==> Spearman    distance from optimal ordering: 98.41% (relative success)...
  ==> Gestalt     distance from optimal ordering: 66.67% (relative success)...
  ==> Mean distance from optimal ordering: 83.32% (relative success)...

Estimated order by the policy #2<$\rho^{\mathrm{Learn}}$, BayesUCB, rank: 5 ~ KL-UCB+(Bern)> after 10000 steps: [0 2 3 1 8 5 6 7 4] ...
  ==> Optimal arm identification: 94.87% (relative success)...
  ==> Manhattan   distance from optimal ordering: 70.37% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 93.94% (relative success)...
  ==> Spearman    distance from optimal ordering: 95.76% (relative success)...
  ==> Gestalt     distance from optimal ordering: 66.67% (relative success)...
  ==> Mean distance from optimal ordering: 81.68% (relative success)...

Estimated order by the policy #3<$\rho^{\mathrm{Learn}}$, BayesUCB, rank: 6 ~ KL-UCB+(Bern)> after 10000 steps: [1 0 2 3 5 4 6 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 90.12% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 99.92% (relative success)...
  ==> Spearman    distance from optimal ordering: 100.00% (relative success)...
  ==> Gestalt     distance from optimal ordering: 77.78% (relative success)...
  ==> Mean distance from optimal ordering: 91.95% (relative success)...

Estimated order by the policy #4<$\rho^{\mathrm{Learn}}$, BayesUCB, rank: 6 ~ KL-UCB+(Bern)> after 10000 steps: [0 1 2 5 3 4 6 8 7] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 85.19% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 99.82% (relative success)...
  ==> Spearman    distance from optimal ordering: 99.98% (relative success)...
  ==> Gestalt     distance from optimal ordering: 77.78% (relative success)...
  ==> Mean distance from optimal ordering: 90.69% (relative success)...

Estimated order by the policy #5<$\rho^{\mathrm{Learn}}$, BayesUCB, rank: 6 ~ KL-UCB+(Bern)> after 10000 steps: [0 1 2 4 5 6 3 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 85.19% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 99.82% (relative success)...
  ==> Spearman    distance from optimal ordering: 99.91% (relative success)...
  ==> Gestalt     distance from optimal ordering: 88.89% (relative success)...
  ==> Mean distance from optimal ordering: 93.45% (relative success)...

Estimated order by the policy #6<$\rho^{\mathrm{Learn}}$, BayesUCB, rank: 6 ~ KL-UCB+(Bern)> after 10000 steps: [0 1 2 6 3 4 7 5 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 80.25% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 99.65% (relative success)...
  ==> Spearman    distance from optimal ordering: 99.75% (relative success)...
  ==> Gestalt     distance from optimal ordering: 77.78% (relative success)...
  ==> Mean distance from optimal ordering: 89.36% (relative success)...
Giving the final ranks ...

Final ranking for this environment #0 :
- Player #2, '#2<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ KL-UCB+(Bern)>'	was ranked	1 / 6 for this simulation (last rewards = 8652).
- Player #1, '#1<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ KL-UCB+(Bern)>'	was ranked	2 / 6 for this simulation (last rewards = 7885).
- Player #6, '#6<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ KL-UCB+(Bern)>'	was ranked	3 / 6 for this simulation (last rewards = 6318).
- Player #4, '#4<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ KL-UCB+(Bern)>'	was ranked	4 / 6 for this simulation (last rewards = 5457).
- Player #5, '#5<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ KL-UCB+(Bern)>'	was ranked	5 / 6 for this simulation (last rewards = 4612).
- Player #3, '#3<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ KL-UCB+(Bern)>'	was ranked	6 / 6 for this simulation (last rewards = 3525).



Considering the list of players :
 [rhoLearn(BayesUCB), rhoLearn(BayesUCB), rhoLearn(BayesUCB), rhoLearn(BayesUCB), rhoLearn(BayesUCB), rhoLearn(BayesUCB)]
Number of players in the multi-players game: 6
Time horizon: 10000
Number of repetitions: 1
Sampling rate for saving, delta_t_save: 1
Sampling rate for plotting, delta_t_plot: 1
Number of jobs for parallelization: 2
Using collision model: onlyUniqUserGetsReward
  Detail:  Simple collision model where only the players alone on one arm sample it and receive the reward.

    - This is the default collision model, cf. https://arxiv.org/abs/0910.2065v3 collision model 1.
    - The numpy array 'choices' is increased according to the number of users who collided (it is NOT binary).
    
Creating a new MAB problem ...
  Reading arms of this MAB problem from a dictionnary 'configuration' = {'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': array([ 0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9])} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = [ 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9]
 - with 'arms' = [B(0.1), B(0.2), B(0.3), B(0.4), B(0.5), B(0.6), B(0.7), B(0.8), B(0.9)]
 - with 'nbArms' = 9
 - with 'maxArm' = 0.9
 - with 'minArm' = 0.1

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 7.52 ... 
 - a Optimal Arm Identification factor H_OI(mu) = 48.89% ...
Number of environments to try: 1

Evaluating environment: MAB(nbArms: 9, arms: [B(0.1), B(0.2), B(0.3), B(0.4), B(0.5), B(0.6), B(0.7), B(0.8), B(0.9)], minArm: 0.1, maxArm: 0.9)
- Adding player #1 = #1<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ BayesUCB> ...
  Using this already created player 'player' = #1<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ BayesUCB> ...
- Adding player #2 = #2<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ BayesUCB> ...
  Using this already created player 'player' = #2<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ BayesUCB> ...
- Adding player #3 = #3<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ BayesUCB> ...
  Using this already created player 'player' = #3<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ BayesUCB> ...
- Adding player #4 = #4<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ BayesUCB> ...
  Using this already created player 'player' = #4<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ BayesUCB> ...
- Adding player #5 = #5<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ BayesUCB> ...
  Using this already created player 'player' = #5<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ BayesUCB> ...
- Adding player #6 = #6<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ BayesUCB> ...
  Using this already created player 'player' = #6<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ BayesUCB> ...

Estimated order by the policy #1<$\rho^{\mathrm{Learn}}$, BayesUCB, rank: 6 ~ BayesUCB> after 10000 steps: [0 2 1 5 6 4 3 7 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 75.31% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 98.77% (relative success)...
  ==> Spearman    distance from optimal ordering: 99.47% (relative success)...
  ==> Gestalt     distance from optimal ordering: 66.67% (relative success)...
  ==> Mean distance from optimal ordering: 85.05% (relative success)...

Estimated order by the policy #2<$\rho^{\mathrm{Learn}}$, BayesUCB, rank: 6 ~ BayesUCB> after 10000 steps: [3 2 0 6 1 5 4 7 8] ...
  ==> Optimal arm identification: 94.87% (relative success)...
  ==> Manhattan   distance from optimal ordering: 65.43% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 93.94% (relative success)...
  ==> Spearman    distance from optimal ordering: 96.42% (relative success)...
  ==> Gestalt     distance from optimal ordering: 44.44% (relative success)...
  ==> Mean distance from optimal ordering: 75.06% (relative success)...

Estimated order by the policy #3<$\rho^{\mathrm{Learn}}$, BayesUCB, rank: 6 ~ BayesUCB> after 10000 steps: [1 0 2 4 6 7 3 5 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 70.37% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 98.77% (relative success)...
  ==> Spearman    distance from optimal ordering: 99.04% (relative success)...
  ==> Gestalt     distance from optimal ordering: 66.67% (relative success)...
  ==> Mean distance from optimal ordering: 83.71% (relative success)...

Estimated order by the policy #4<$\rho^{\mathrm{Learn}}$, BayesUCB, rank: 6 ~ BayesUCB> after 10000 steps: [0 1 2 3 6 5 7 4 8] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 85.19% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 99.65% (relative success)...
  ==> Spearman    distance from optimal ordering: 99.84% (relative success)...
  ==> Gestalt     distance from optimal ordering: 77.78% (relative success)...
  ==> Mean distance from optimal ordering: 90.61% (relative success)...

Estimated order by the policy #5<$\rho^{\mathrm{Learn}}$, BayesUCB, rank: 4 ~ BayesUCB> after 10000 steps: [4 0 1 2 3 8 5 7 6] ...
  ==> Optimal arm identification: 94.87% (relative success)...
  ==> Manhattan   distance from optimal ordering: 65.43% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 96.29% (relative success)...
  ==> Spearman    distance from optimal ordering: 97.02% (relative success)...
  ==> Gestalt     distance from optimal ordering: 55.56% (relative success)...
  ==> Mean distance from optimal ordering: 78.58% (relative success)...

Estimated order by the policy #6<$\rho^{\mathrm{Learn}}$, BayesUCB, rank: 6 ~ BayesUCB> after 10000 steps: [0 2 1 7 6 5 3 8 4] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 60.49% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 85.56% (relative success)...
  ==> Spearman    distance from optimal ordering: 91.24% (relative success)...
  ==> Gestalt     distance from optimal ordering: 44.44% (relative success)...
  ==> Mean distance from optimal ordering: 70.43% (relative success)...
Giving the final ranks ...

Final ranking for this environment #0 :
- Player #5, '#5<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ BayesUCB>'	was ranked	1 / 6 for this simulation (last rewards = 8479).
- Player #6, '#6<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ BayesUCB>'	was ranked	2 / 6 for this simulation (last rewards = 5895).
- Player #2, '#2<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ BayesUCB>'	was ranked	3 / 6 for this simulation (last rewards = 5484).
- Player #1, '#1<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ BayesUCB>'	was ranked	4 / 6 for this simulation (last rewards = 5371).
- Player #3, '#3<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ BayesUCB>'	was ranked	5 / 6 for this simulation (last rewards = 4912).
- Player #4, '#4<$\rho^{\mathrm{Learn}}$, BayesUCB, rank ~ BayesUCB>'	was ranked	6 / 6 for this simulation (last rewards = 3270).


- Plotting the centralized regret for all 'players' values
markerlmbda(evaId) = (0.0, 0.1)
markerlmbda(evaId) = (0.02, 0.1)
markerlmbda(evaId) = (0.04, 0.1)
markerlmbda(evaId) = (0.06, 0.1)
markerlmbda(evaId) = (0.08, 0.1)
markerlmbda(evaId) = (0.1, 0.1)
markerlmbda(evaId) = (0.12, 0.1)
markerlmbda(evaId) = (0.14, 0.1)
  - For 6 players, our lower bound gave = 48.8 ...
  - For 6 players, the initial lower bound in Theorem 6 from [Anandkumar et al., 2010] gave = 15 ...

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 7.52 for 1-player problem ... 
 - a Optimal Arm Identification factor H_OI(mu) = 48.89% ...
 - Our lowerbound = 48.8,
 - [Anandkumar et al] lowerbound = 15


- Plotting the centralized regret for all 'players' values, in semilogx scale
markerlmbda(evaId) = (0.0, 0.1)
markerlmbda(evaId) = (0.02, 0.1)
markerlmbda(evaId) = (0.04, 0.1)
markerlmbda(evaId) = (0.06, 0.1)
markerlmbda(evaId) = (0.08, 0.1)
markerlmbda(evaId) = (0.1, 0.1)
markerlmbda(evaId) = (0.12, 0.1)
markerlmbda(evaId) = (0.14, 0.1)
  - For 6 players, our lower bound gave = 48.8 ...
  - For 6 players, the initial lower bound in Theorem 6 from [Anandkumar et al., 2010] gave = 15 ...

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 7.52 for 1-player problem ... 
 - a Optimal Arm Identification factor H_OI(mu) = 48.89% ...
 - Our lowerbound = 48.8,
 - [Anandkumar et al] lowerbound = 15


- Plotting the centralized regret for all 'players' values, in semilogy scale
markerlmbda(evaId) = (0.0, 0.1)
markerlmbda(evaId) = (0.02, 0.1)
markerlmbda(evaId) = (0.04, 0.1)
markerlmbda(evaId) = (0.06, 0.1)
markerlmbda(evaId) = (0.08, 0.1)
markerlmbda(evaId) = (0.1, 0.1)
markerlmbda(evaId) = (0.12, 0.1)
markerlmbda(evaId) = (0.14, 0.1)
  - For 6 players, our lower bound gave = 48.8 ...
  - For 6 players, the initial lower bound in Theorem 6 from [Anandkumar et al., 2010] gave = 15 ...

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 7.52 for 1-player problem ... 
 - a Optimal Arm Identification factor H_OI(mu) = 48.89% ...
 - Our lowerbound = 48.8,
 - [Anandkumar et al] lowerbound = 15
markerlmbda(evaId) = (0.0, 0.01)
markerlmbda(evaId) = (0.02, 0.01)
markerlmbda(evaId) = (0.04, 0.01)
markerlmbda(evaId) = (0.06, 0.01)
markerlmbda(evaId) = (0.08, 0.01)
markerlmbda(evaId) = (0.1, 0.01)
markerlmbda(evaId) = (0.12, 0.01)
markerlmbda(evaId) = (0.14, 0.01)
  - For 6 players, our lower bound gave = 48.8 ...
  - For 6 players, the initial lower bound in Theorem 6 from [Anandkumar et al., 2010] gave = 15 ...

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 7.52 for 1-player problem ... 
 - a Optimal Arm Identification factor H_OI(mu) = 48.89% ...
 - Our lowerbound = 48.8,
 - [Anandkumar et al] lowerbound = 15
markerlmbda(evaId) = (0.0, 0.05)
markerlmbda(evaId) = (0.02, 0.05)
markerlmbda(evaId) = (0.04, 0.05)
markerlmbda(evaId) = (0.06, 0.05)
markerlmbda(evaId) = (0.08, 0.05)
markerlmbda(evaId) = (0.1, 0.05)
markerlmbda(evaId) = (0.12, 0.05)
markerlmbda(evaId) = (0.14, 0.05)
  - For 6 players, our lower bound gave = 48.8 ...
  - For 6 players, the initial lower bound in Theorem 6 from [Anandkumar et al., 2010] gave = 15 ...

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 7.52 for 1-player problem ... 
 - a Optimal Arm Identification factor H_OI(mu) = 48.89% ...
 - Our lowerbound = 48.8,
 - [Anandkumar et al] lowerbound = 15
markerlmbda(evaId) = (0.0, 0.1)
markerlmbda(evaId) = (0.02, 0.1)
markerlmbda(evaId) = (0.04, 0.1)
markerlmbda(evaId) = (0.06, 0.1)
markerlmbda(evaId) = (0.08, 0.1)
markerlmbda(evaId) = (0.1, 0.1)
markerlmbda(evaId) = (0.12, 0.1)
markerlmbda(evaId) = (0.14, 0.1)
  - For 6 players, our lower bound gave = 48.8 ...
  - For 6 players, the initial lower bound in Theorem 6 from [Anandkumar et al., 2010] gave = 15 ...

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 7.52 for 1-player problem ... 
 - a Optimal Arm Identification factor H_OI(mu) = 48.89% ...
 - Our lowerbound = 48.8,
 - [Anandkumar et al] lowerbound = 15
markerlmbda(evaId) = (0.0, 0.2)
markerlmbda(evaId) = (0.02, 0.2)
markerlmbda(evaId) = (0.04, 0.2)
markerlmbda(evaId) = (0.06, 0.2)
markerlmbda(evaId) = (0.08, 0.2)
markerlmbda(evaId) = (0.1, 0.2)
markerlmbda(evaId) = (0.12, 0.2)
markerlmbda(evaId) = (0.14, 0.2)
  - For 6 players, our lower bound gave = 48.8 ...
  - For 6 players, the initial lower bound in Theorem 6 from [Anandkumar et al., 2010] gave = 15 ...

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 7.52 for 1-player problem ... 
 - a Optimal Arm Identification factor H_OI(mu) = 48.89% ...
 - Our lowerbound = 48.8,
 - [Anandkumar et al] lowerbound = 15
markerlmbda(evaId) = (0.0, 0.3)
markerlmbda(evaId) = (0.02, 0.3)
markerlmbda(evaId) = (0.04, 0.3)
markerlmbda(evaId) = (0.06, 0.3)
markerlmbda(evaId) = (0.08, 0.3)
markerlmbda(evaId) = (0.1, 0.3)
markerlmbda(evaId) = (0.12, 0.3)
markerlmbda(evaId) = (0.14, 0.3)
  - For 6 players, our lower bound gave = 48.8 ...
  - For 6 players, the initial lower bound in Theorem 6 from [Anandkumar et al., 2010] gave = 15 ...

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 7.52 for 1-player problem ... 
 - a Optimal Arm Identification factor H_OI(mu) = 48.89% ...
 - Our lowerbound = 48.8,
 - [Anandkumar et al] lowerbound = 15
markerlmbda(evaId) = (0.0, 0.5)
markerlmbda(evaId) = (0.02, 0.5)
markerlmbda(evaId) = (0.04, 0.5)
markerlmbda(evaId) = (0.06, 0.5)
markerlmbda(evaId) = (0.08, 0.5)
markerlmbda(evaId) = (0.1, 0.5)
markerlmbda(evaId) = (0.12, 0.5)
markerlmbda(evaId) = (0.14, 0.5)
  - For 6 players, our lower bound gave = 48.8 ...
  - For 6 players, the initial lower bound in Theorem 6 from [Anandkumar et al., 2010] gave = 15 ...

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 7.52 for 1-player problem ... 
 - a Optimal Arm Identification factor H_OI(mu) = 48.89% ...
 - Our lowerbound = 48.8,
 - [Anandkumar et al] lowerbound = 15
markerlmbda(evaId) = (0.0, 0.7)
markerlmbda(evaId) = (0.02, 0.7)
markerlmbda(evaId) = (0.04, 0.7)
markerlmbda(evaId) = (0.06, 0.7)
markerlmbda(evaId) = (0.08, 0.7)
markerlmbda(evaId) = (0.1, 0.7)
markerlmbda(evaId) = (0.12, 0.7)
markerlmbda(evaId) = (0.14, 0.7)
  - For 6 players, our lower bound gave = 48.8 ...
  - For 6 players, the initial lower bound in Theorem 6 from [Anandkumar et al., 2010] gave = 15 ...

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 7.52 for 1-player problem ... 
 - a Optimal Arm Identification factor H_OI(mu) = 48.89% ...
 - Our lowerbound = 48.8,
 - [Anandkumar et al] lowerbound = 15
markerlmbda(evaId) = (0.0, 1.0)
markerlmbda(evaId) = (0.02, 1.0)
markerlmbda(evaId) = (0.04, 1.0)
markerlmbda(evaId) = (0.06, 1.0)
markerlmbda(evaId) = (0.08, 1.0)
markerlmbda(evaId) = (0.1, 1.0)
markerlmbda(evaId) = (0.12, 1.0)
markerlmbda(evaId) = (0.14, 1.0)
  - For 6 players, our lower bound gave = 48.8 ...
  - For 6 players, the initial lower bound in Theorem 6 from [Anandkumar et al., 2010] gave = 15 ...

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 7.52 for 1-player problem ... 
 - a Optimal Arm Identification factor H_OI(mu) = 48.89% ...
 - Our lowerbound = 48.8,
 - [Anandkumar et al] lowerbound = 15
markerlmbda(evaId) = (0.0, 1.1)
markerlmbda(evaId) = (0.02, 1.1)
markerlmbda(evaId) = (0.04, 1.1)
markerlmbda(evaId) = (0.06, 1.1)
markerlmbda(evaId) = (0.08, 1.1)
markerlmbda(evaId) = (0.1, 1.1)
markerlmbda(evaId) = (0.12, 1.1)
markerlmbda(evaId) = (0.14, 1.1)
  - For 6 players, our lower bound gave = 48.8 ...
  - For 6 players, the initial lower bound in Theorem 6 from [Anandkumar et al., 2010] gave = 15 ...

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 7.52 for 1-player problem ... 
 - a Optimal Arm Identification factor H_OI(mu) = 48.89% ...
 - Our lowerbound = 48.8,
 - [Anandkumar et al] lowerbound = 15
markerlmbda(evaId) = (0.0, 1.5)
markerlmbda(evaId) = (0.02, 1.5)
markerlmbda(evaId) = (0.04, 1.5)
markerlmbda(evaId) = (0.06, 1.5)
markerlmbda(evaId) = (0.08, 1.5)
markerlmbda(evaId) = (0.1, 1.5)
markerlmbda(evaId) = (0.12, 1.5)
markerlmbda(evaId) = (0.14, 1.5)
  - For 6 players, our lower bound gave = 48.8 ...
  - For 6 players, the initial lower bound in Theorem 6 from [Anandkumar et al., 2010] gave = 15 ...

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 7.52 for 1-player problem ... 
 - a Optimal Arm Identification factor H_OI(mu) = 48.89% ...
 - Our lowerbound = 48.8,
 - [Anandkumar et al] lowerbound = 15
markerlmbda(evaId) = (0.0, 2.0)
markerlmbda(evaId) = (0.02, 2.0)
markerlmbda(evaId) = (0.04, 2.0)
markerlmbda(evaId) = (0.06, 2.0)
markerlmbda(evaId) = (0.08, 2.0)
markerlmbda(evaId) = (0.1, 2.0)
markerlmbda(evaId) = (0.12, 2.0)
markerlmbda(evaId) = (0.14, 2.0)
  - For 6 players, our lower bound gave = 48.8 ...
  - For 6 players, the initial lower bound in Theorem 6 from [Anandkumar et al., 2010] gave = 15 ...

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 7.52 for 1-player problem ... 
 - a Optimal Arm Identification factor H_OI(mu) = 48.89% ...
 - Our lowerbound = 48.8,
 - [Anandkumar et al] lowerbound = 15
markerlmbda(evaId) = (0.0, 5.0)
markerlmbda(evaId) = (0.02, 5.0)
markerlmbda(evaId) = (0.04, 5.0)
markerlmbda(evaId) = (0.06, 5.0)
markerlmbda(evaId) = (0.08, 5.0)
markerlmbda(evaId) = (0.1, 5.0)
markerlmbda(evaId) = (0.12, 5.0)
markerlmbda(evaId) = (0.14, 5.0)
  - For 6 players, our lower bound gave = 48.8 ...
  - For 6 players, the initial lower bound in Theorem 6 from [Anandkumar et al., 2010] gave = 15 ...

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 7.52 for 1-player problem ... 
 - a Optimal Arm Identification factor H_OI(mu) = 48.89% ...
 - Our lowerbound = 48.8,
 - [Anandkumar et al] lowerbound = 15
markerlmbda(evaId) = (0.0, 10.0)
markerlmbda(evaId) = (0.02, 10.0)
markerlmbda(evaId) = (0.04, 10.0)
markerlmbda(evaId) = (0.06, 10.0)
markerlmbda(evaId) = (0.08, 10.0)
markerlmbda(evaId) = (0.1, 10.0)
markerlmbda(evaId) = (0.12, 10.0)
markerlmbda(evaId) = (0.14, 10.0)
  - For 6 players, our lower bound gave = 48.8 ...
  - For 6 players, the initial lower bound in Theorem 6 from [Anandkumar et al., 2010] gave = 15 ...

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 7.52 for 1-player problem ... 
 - a Optimal Arm Identification factor H_OI(mu) = 48.89% ...
 - Our lowerbound = 48.8,
 - [Anandkumar et al] lowerbound = 15


- Plotting the centralized regret for all 'players' values, in loglog scale
markerlmbda(evaId) = (0.0, 0.1)
markerlmbda(evaId) = (0.02, 0.1)
markerlmbda(evaId) = (0.04, 0.1)
markerlmbda(evaId) = (0.06, 0.1)
markerlmbda(evaId) = (0.08, 0.1)
markerlmbda(evaId) = (0.1, 0.1)
markerlmbda(evaId) = (0.12, 0.1)
markerlmbda(evaId) = (0.14, 0.1)
  - For 6 players, our lower bound gave = 48.8 ...
  - For 6 players, the initial lower bound in Theorem 6 from [Anandkumar et al., 2010] gave = 15 ...

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 7.52 for 1-player problem ... 
 - a Optimal Arm Identification factor H_OI(mu) = 48.89% ...
 - Our lowerbound = 48.8,
 - [Anandkumar et al] lowerbound = 15


==> To see the figures, do :
eog plots/MP__M6_T10000_N1__6_algos/all*5946072657933300008.png
Done for simulations main_multiplayers.py ...
