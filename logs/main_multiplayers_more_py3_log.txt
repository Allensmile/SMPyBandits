 - Setting dpi of all figures to 110 ...
 - Setting 'figsize' of all figures to (19.8, 10.8) ...
Info: numba.jit seems to be available.
Info: numba.jit seems to be available.
 - One new child, of index 0, and class #1<CentralizedMultiplePlay(UCB)> ...
 - One new child, of index 1, and class #2<CentralizedMultiplePlay(UCB)> ...
 - One new child, of index 2, and class #3<CentralizedMultiplePlay(UCB)> ...
 - One new child, of index 3, and class #4<CentralizedMultiplePlay(UCB)> ...
 - One new child, of index 4, and class #5<CentralizedMultiplePlay(UCB)> ...
 - One new child, of index 5, and class #6<CentralizedMultiplePlay(UCB)> ...
Loaded experiments configuration from 'configuration.py' :
configuration = {'players': [CentralizedMultiplePlay(UCB), CentralizedMultiplePlay(UCB), CentralizedMultiplePlay(UCB), CentralizedMultiplePlay(UCB), CentralizedMultiplePlay(UCB), CentralizedMultiplePlay(UCB)], 'verbosity': 6, 'environment': [{'params': [0.005, 0.01, 0.015, 0.02, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.78, 0.8, 0.82, 0.83, 0.84, 0.85], 'arm_type': <class 'Arms.Bernoulli.Bernoulli'>}], 'finalRanksOnAverage': True, 'delta_t_save': 1, 'averageOn': 0.001, 'collisionModel': <function onlyUniqUserGetsReward at 0x7f2e5b38ca60>, 'repetitions': 4, 'n_jobs': -1, 'successive_players': [[Selfish(UCB($\alpha=1$)), Selfish(UCB($\alpha=1$)), Selfish(UCB($\alpha=1$)), Selfish(UCB($\alpha=1$)), Selfish(UCB($\alpha=1$)), Selfish(UCB($\alpha=1$))], [Selfish(UCB($\alpha=0.25$)), Selfish(UCB($\alpha=0.25$)), Selfish(UCB($\alpha=0.25$)), Selfish(UCB($\alpha=0.25$)), Selfish(UCB($\alpha=0.25$)), Selfish(UCB($\alpha=0.25$))], [Selfish(KL-UCB(Bern)), Selfish(KL-UCB(Bern)), Selfish(KL-UCB(Bern)), Selfish(KL-UCB(Bern)), Selfish(KL-UCB(Bern)), Selfish(KL-UCB(Bern))], [Selfish(KL-UCB+(Bern)), Selfish(KL-UCB+(Bern)), Selfish(KL-UCB+(Bern)), Selfish(KL-UCB+(Bern)), Selfish(KL-UCB+(Bern)), Selfish(KL-UCB+(Bern))], [Selfish(Thompson), Selfish(Thompson), Selfish(Thompson), Selfish(Thompson), Selfish(Thompson), Selfish(Thompson)], [Selfish(Softmax(decreasing)), Selfish(Softmax(decreasing)), Selfish(Softmax(decreasing)), Selfish(Softmax(decreasing)), Selfish(Softmax(decreasing)), Selfish(Softmax(decreasing))], [Selfish(BayesUCB), Selfish(BayesUCB), Selfish(BayesUCB), Selfish(BayesUCB), Selfish(BayesUCB), Selfish(BayesUCB)]], 'horizon': 5000}
plots/ is already a directory here...



Considering the list of players :
 [Selfish(UCB($\alpha=1$)), Selfish(UCB($\alpha=1$)), Selfish(UCB($\alpha=1$)), Selfish(UCB($\alpha=1$)), Selfish(UCB($\alpha=1$)), Selfish(UCB($\alpha=1$))]
Number of players in the multi-players game: 6
Time horizon: 5000
Number of repetitions: 4
Sampling rate DELTA_T_SAVE: 1
Using collision model: onlyUniqUserGetsReward
  Detail:  Simple collision model where only the players alone on one arm sample it and receive the reward.

    - This is the default collision model, cf. https://arxiv.org/abs/0910.2065v3 collision model 1.
    - The numpy array 'choices' is increased according to the number of users who collided (it is NOT binary).
    
Creating a new MAB problem ...
  Reading arms of this MAB problem from a dictionnary 'configuration' = {'params': [0.005, 0.01, 0.015, 0.02, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.78, 0.8, 0.82, 0.83, 0.84, 0.85], 'arm_type': <class 'Arms.Bernoulli.Bernoulli'>} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = [0.005, 0.01, 0.015, 0.02, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.78, 0.8, 0.82, 0.83, 0.84, 0.85]
 - with 'arms' = [B(0.005), B(0.01), B(0.015), B(0.02), B(0.3), B(0.35), B(0.4), B(0.45), B(0.5), B(0.55), B(0.6), B(0.78), B(0.8), B(0.82), B(0.83), B(0.84), B(0.85)]
 - with 'nbArms' = 17
 - with 'maxArm' = 0.85
 - with 'minArm' = 0.005

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 66.4 ... 
 - a Optimal Arm Identification factor H_OI(mu) = 62.76% ...
Number of environments to try: 1

Evaluating environment: <MAB{'maxArm': 0.84999999999999998, 'minArm': 0.0050000000000000001, 'arms': [B(0.005), B(0.01), B(0.015), B(0.02), B(0.3), B(0.35), B(0.4), B(0.45), B(0.5), B(0.55), B(0.6), B(0.78), B(0.8), B(0.82), B(0.83), B(0.84), B(0.85)], 'nbArms': 17}>
- Adding player #1 = #1<Selfish(UCB($\alpha=1$))> ...
  Using this already created player 'player' = #1<Selfish(UCB($\alpha=1$))> ...
- Adding player #2 = #2<Selfish(UCB($\alpha=1$))> ...
  Using this already created player 'player' = #2<Selfish(UCB($\alpha=1$))> ...
- Adding player #3 = #3<Selfish(UCB($\alpha=1$))> ...
  Using this already created player 'player' = #3<Selfish(UCB($\alpha=1$))> ...
- Adding player #4 = #4<Selfish(UCB($\alpha=1$))> ...
  Using this already created player 'player' = #4<Selfish(UCB($\alpha=1$))> ...
- Adding player #5 = #5<Selfish(UCB($\alpha=1$))> ...
  Using this already created player 'player' = #5<Selfish(UCB($\alpha=1$))> ...
- Adding player #6 = #6<Selfish(UCB($\alpha=1$))> ...
  Using this already created player 'player' = #6<Selfish(UCB($\alpha=1$))> ...
Giving the final ranks ...

Final ranking for this environment #0 :
- Player #6, '#6<Selfish(UCB($\alpha=1$))>'	was ranked	1 / 6 for this simulation (last rewards = 3983.75).
- Player #2, '#2<Selfish(UCB($\alpha=1$))>'	was ranked	2 / 6 for this simulation (last rewards = 3956.75).
- Player #4, '#4<Selfish(UCB($\alpha=1$))>'	was ranked	3 / 6 for this simulation (last rewards = 3948).
- Player #1, '#1<Selfish(UCB($\alpha=1$))>'	was ranked	4 / 6 for this simulation (last rewards = 3912).
- Player #5, '#5<Selfish(UCB($\alpha=1$))>'	was ranked	5 / 6 for this simulation (last rewards = 3911.25).
- Player #3, '#3<Selfish(UCB($\alpha=1$))>'	was ranked	6 / 6 for this simulation (last rewards = 3895.75).


- Plotting the decentralized rewards


- Plotting the centralized fairness (STD)
  - Amplitude fairness index is = [ 1.          1.          0.71428571 ...,  0.02201869  0.02195183
  0.02194633] ...
  - RajJain fairness index is = [  6.11764706e-01   3.24324324e-01   1.68098160e-01 ...,   7.22475134e-05
   7.16731655e-05   7.20801585e-05] ...
  - Default fairness index is = [ 0.82053376  0.64430165  0.47991685 ...,  0.01247305  0.01243039
  0.01244297] ...
  - STD fairness index is = [ 0.84983659  0.60858062  0.55736666 ...,  0.01532819  0.01526766
  0.01531051] ...
  - Mean fairness index is = [ 0.82053376  0.64430165  0.47991685 ...,  0.01247305  0.01243039
  0.01244297] ...


- Plotting the centralized regret
  - For 6 player, our lower bound gave = 71.75492878990443 ...
  - For 6 player, the initial lower bound in Theorem 6 from [Anandkumar et al., 2010] gave = 54.27851671604706 ...

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 66.4 for 1-player problem ... 
 - a Optimal Arm Identification factor H_OI(mu) = 62.76% ...
 - Our lowerbound = 71.8,
 - anandkumar_lowerbound = 54.3


- Plotting the centralized regret
  - For 6 player, our lower bound gave = 71.75492878990443 ...
  - For 6 player, the initial lower bound in Theorem 6 from [Anandkumar et al., 2010] gave = 54.27851671604706 ...

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 66.4 for 1-player problem ... 
 - a Optimal Arm Identification factor H_OI(mu) = 62.76% ...
 - Our lowerbound = 71.8,
 - anandkumar_lowerbound = 54.3


- Plotting the number of switches


- Plotting the cumulative number of switches
 - Plotting the probability of picking the best arm
 - Plotting the total nb of collision as a function of time
 - Plotting the cumulated total nb of collision as a function of time
 - Plotting the frequency of collision in each arm
  - For #$0$: $B(0.005)$ ($0.0%$$\%$),	frequency of collisions is 0.000216667  ...
  - For #$1$: $B(0.01)$ ($0.0%$$\%$),	frequency of collisions is 0.00015  ...
  - For #$2$: $B(0.015)$ ($0.0%$$\%$),	frequency of collisions is 0.000166667  ...
  - For #$3$: $B(0.02)$ ($0.0%$$\%$),	frequency of collisions is 0.000158333  ...
  - For #$4$: $B(0.3)$ ($0.0%$$\%$),	frequency of collisions is 0.000266667  ...
  - For #$5$: $B(0.35)$ ($0.0%$$\%$),	frequency of collisions is 0.000266667  ...
  - For #$6$: $B(0.4)$ ($0.0%$$\%$),	frequency of collisions is 0.000425  ...
  - For #$7$: $B(0.45)$ ($0.1%$$\%$),	frequency of collisions is 0.000533333  ...
  - For #$8$: $B(0.5)$ ($0.0%$$\%$),	frequency of collisions is 0.000441667  ...
  - For #$9$: $B(0.55)$ ($0.1%$$\%$),	frequency of collisions is 0.000716667  ...
  - For #$10$: $B(0.6)$ ($0.1%$$\%$),	frequency of collisions is 0.000725  ...
  - For #$11$: $B(0.78)$ ($0.3%$$\%$),	frequency of collisions is 0.002825  ...
  - For #$12$: $B(0.8)$ ($0.3%$$\%$),	frequency of collisions is 0.00321667  ...
  - For #$13$: $B(0.82)$ ($0.3%$$\%$),	frequency of collisions is 0.00325833  ...
  - For #$14$: $B(0.83)$ ($0.3%$$\%$),	frequency of collisions is 0.003175  ...
  - For #$15$: $B(0.84)$ ($0.3%$$\%$),	frequency of collisions is 0.00345833  ...
  - For #$16$: $B(0.85)$ ($0.4%$$\%$),	frequency of collisions is 0.00350833  ...


==> To see the figures, do :
eog plots/MP__M6_T5000_N4__6_algos/main*6325159432941142286.png



Considering the list of players :
 [Selfish(UCB($\alpha=0.25$)), Selfish(UCB($\alpha=0.25$)), Selfish(UCB($\alpha=0.25$)), Selfish(UCB($\alpha=0.25$)), Selfish(UCB($\alpha=0.25$)), Selfish(UCB($\alpha=0.25$))]
Number of players in the multi-players game: 6
Time horizon: 5000
Number of repetitions: 4
Sampling rate DELTA_T_SAVE: 1
Using collision model: onlyUniqUserGetsReward
  Detail:  Simple collision model where only the players alone on one arm sample it and receive the reward.

    - This is the default collision model, cf. https://arxiv.org/abs/0910.2065v3 collision model 1.
    - The numpy array 'choices' is increased according to the number of users who collided (it is NOT binary).
    
Creating a new MAB problem ...
  Reading arms of this MAB problem from a dictionnary 'configuration' = {'params': [0.005, 0.01, 0.015, 0.02, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.78, 0.8, 0.82, 0.83, 0.84, 0.85], 'arm_type': <class 'Arms.Bernoulli.Bernoulli'>} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = [0.005, 0.01, 0.015, 0.02, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.78, 0.8, 0.82, 0.83, 0.84, 0.85]
 - with 'arms' = [B(0.005), B(0.01), B(0.015), B(0.02), B(0.3), B(0.35), B(0.4), B(0.45), B(0.5), B(0.55), B(0.6), B(0.78), B(0.8), B(0.82), B(0.83), B(0.84), B(0.85)]
 - with 'nbArms' = 17
 - with 'maxArm' = 0.85
 - with 'minArm' = 0.005

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 66.4 ... 
 - a Optimal Arm Identification factor H_OI(mu) = 62.76% ...
Number of environments to try: 1

Evaluating environment: <MAB{'maxArm': 0.84999999999999998, 'minArm': 0.0050000000000000001, 'arms': [B(0.005), B(0.01), B(0.015), B(0.02), B(0.3), B(0.35), B(0.4), B(0.45), B(0.5), B(0.55), B(0.6), B(0.78), B(0.8), B(0.82), B(0.83), B(0.84), B(0.85)], 'nbArms': 17}>
- Adding player #1 = #1<Selfish(UCB($\alpha=0.25$))> ...
  Using this already created player 'player' = #1<Selfish(UCB($\alpha=0.25$))> ...
- Adding player #2 = #2<Selfish(UCB($\alpha=0.25$))> ...
  Using this already created player 'player' = #2<Selfish(UCB($\alpha=0.25$))> ...
- Adding player #3 = #3<Selfish(UCB($\alpha=0.25$))> ...
  Using this already created player 'player' = #3<Selfish(UCB($\alpha=0.25$))> ...
- Adding player #4 = #4<Selfish(UCB($\alpha=0.25$))> ...
  Using this already created player 'player' = #4<Selfish(UCB($\alpha=0.25$))> ...
- Adding player #5 = #5<Selfish(UCB($\alpha=0.25$))> ...
  Using this already created player 'player' = #5<Selfish(UCB($\alpha=0.25$))> ...
- Adding player #6 = #6<Selfish(UCB($\alpha=0.25$))> ...
  Using this already created player 'player' = #6<Selfish(UCB($\alpha=0.25$))> ...
Giving the final ranks ...

Final ranking for this environment #0 :
- Player #1, '#1<Selfish(UCB($\alpha=0.25$))>'	was ranked	1 / 6 for this simulation (last rewards = 4129).
- Player #5, '#5<Selfish(UCB($\alpha=0.25$))>'	was ranked	2 / 6 for this simulation (last rewards = 4096).
- Player #2, '#2<Selfish(UCB($\alpha=0.25$))>'	was ranked	3 / 6 for this simulation (last rewards = 4084).
- Player #6, '#6<Selfish(UCB($\alpha=0.25$))>'	was ranked	4 / 6 for this simulation (last rewards = 4071.5).
- Player #3, '#3<Selfish(UCB($\alpha=0.25$))>'	was ranked	5 / 6 for this simulation (last rewards = 3990.5).
- Player #4, '#4<Selfish(UCB($\alpha=0.25$))>'	was ranked	6 / 6 for this simulation (last rewards = 3899).


- Plotting the decentralized rewards


- Plotting the centralized fairness (STD)
  - Amplitude fairness index is = [ 1.          0.8         0.88888889 ...,  0.05586153  0.05596902
  0.05590175] ...
  - RajJain fairness index is = [  4.85714286e-01   4.00000000e-01   2.94409938e-01 ...,   4.44050255e-04
   4.45143867e-04   4.44821584e-04] ...
  - Default fairness index is = [ 0.72429952  0.58856181  0.58449225 ...,  0.03133412  0.03138507
  0.03135918] ...
  - STD fairness index is = [ 0.68718427  0.56568542  0.57017794 ...,  0.03769679  0.03774105
  0.03773097] ...
  - Mean fairness index is = [ 0.72429952  0.58856181  0.58449225 ...,  0.03133412  0.03138507
  0.03135918] ...


- Plotting the centralized regret
  - For 6 player, our lower bound gave = 71.75492878990443 ...
  - For 6 player, the initial lower bound in Theorem 6 from [Anandkumar et al., 2010] gave = 54.27851671604706 ...

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 66.4 for 1-player problem ... 
 - a Optimal Arm Identification factor H_OI(mu) = 62.76% ...
 - Our lowerbound = 71.8,
 - anandkumar_lowerbound = 54.3


- Plotting the centralized regret
  - For 6 player, our lower bound gave = 71.75492878990443 ...
  - For 6 player, the initial lower bound in Theorem 6 from [Anandkumar et al., 2010] gave = 54.27851671604706 ...

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 66.4 for 1-player problem ... 
 - a Optimal Arm Identification factor H_OI(mu) = 62.76% ...
 - Our lowerbound = 71.8,
 - anandkumar_lowerbound = 54.3


- Plotting the number of switches


- Plotting the cumulative number of switches
 - Plotting the probability of picking the best arm
 - Plotting the total nb of collision as a function of time
