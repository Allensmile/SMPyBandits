 - Setting dpi of all figures to 110 ...
 - Setting 'figsize' of all figures to (19.8, 10.8) ...
Info: Using the regular tqdm() decorator ...
Info: numba.jit seems to be available.
Info: numba.jit seems to be available.
Loaded experiments configuration from 'configuration.py' :
configuration = {'delta_t_save': 1, 'verbosity': 6, 'finalRanksOnAverage': True, 'averageOn': 0.001, 'environment': [{'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': [0.3, 0.4, 0.5, 0.6, 0.7]}], 'repetitions': 20, 'horizon': 10000, 'n_jobs': -1, 'players': [rhoEst(Thompson), rhoEst(Thompson)], 'successive_players': [[rhoEst(UCB($\alpha=1$)), rhoEst(UCB($\alpha=1$))], [rhoRand(UCB($\alpha=1$)), rhoRand(UCB($\alpha=1$))], [rhoEst(Thompson), rhoEst(Thompson)], [rhoRand(Thompson), rhoRand(Thompson)], [rhoEst(KL-UCB(Bern)), rhoEst(KL-UCB(Bern))], [rhoRand(KL-UCB(Bern)), rhoRand(KL-UCB(Bern))], [rhoEst(BayesUCB), rhoEst(BayesUCB)], [rhoRand(BayesUCB), rhoRand(BayesUCB)]], 'collisionModel': <function onlyUniqUserGetsReward at 0x7f25f89581e0>}
plots/ is already a directory here...



Considering the list of players :
 [rhoEst(UCB($\alpha=1$)), rhoEst(UCB($\alpha=1$))]
Number of players in the multi-players game: 2
Time horizon: 10000
Number of repetitions: 20
Sampling rate DELTA_T_SAVE: 1
Using collision model: onlyUniqUserGetsReward
  Detail:  Simple collision model where only the players alone on one arm sample it and receive the reward.

    - This is the default collision model, cf. https://arxiv.org/abs/0910.2065v3 collision model 1.
    - The numpy array 'choices' is increased according to the number of users who collided (it is NOT binary).
    
Creating a new MAB problem ...
  Reading arms of this MAB problem from a dictionnary 'configuration' = {'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': [0.3, 0.4, 0.5, 0.6, 0.7]} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = [0.3, 0.4, 0.5, 0.6, 0.7]
 - with 'arms' = [B(0.3), B(0.4), B(0.5), B(0.6), B(0.7)]
 - with 'nbArms' = 5
 - with 'maxArm' = 0.7
 - with 'minArm' = 0.3

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 9.46 ... 
 - a Optimal Arm Identification factor H_OI(mu) = 60.00% ...
Number of environments to try: 1

Evaluating environment: <MAB{'minArm': 0.29999999999999999, 'nbArms': 5, 'maxArm': 0.69999999999999996, 'arms': [B(0.3), B(0.4), B(0.5), B(0.6), B(0.7)]}>
- Adding player #1 = #1<$\rho^{\mathrm{Est}}$, UCB($\alpha=1$)> ...
  Using this already created player 'player' = #1<$\rho^{\mathrm{Est}}$, UCB($\alpha=1$)> ...
- Adding player #2 = #2<$\rho^{\mathrm{Est}}$, UCB($\alpha=1$)> ...
  Using this already created player 'player' = #2<$\rho^{\mathrm{Est}}$, UCB($\alpha=1$)> ...

Estimated order by the policy #1<$\rho^{\mathrm{Est}}$, UCB($\alpha=1$)rank:2> after 10000 steps: [0 1 2 3 4] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 100.00% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 98.57% (relative success)...
  ==> Spearman    distance from optimal ordering: 100.00% (relative success)...
  ==> Gestalt     distance from optimal ordering: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 99.64% (relative success)...

Estimated order by the policy #2<$\rho^{\mathrm{Est}}$, UCB($\alpha=1$)rank:1> after 10000 steps: [3 0 1 2 4] ...
  ==> Optimal arm identification: 92.31% (relative success)...
  ==> Manhattan   distance from optimal ordering: 52.00% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 67.28% (relative success)...
  ==> Spearman    distance from optimal ordering: 49.54% (relative success)...
  ==> Gestalt     distance from optimal ordering: 80.00% (relative success)...
  ==> Mean distance from optimal ordering: 62.20% (relative success)...
Giving the final ranks ...

Final ranking for this environment #0 :
- Player #1, '#1<$\rho^{\mathrm{Est}}$, UCB($\alpha=1$)>'	was ranked	1 / 2 for this simulation (last rewards = 6546.5).
- Player #2, '#2<$\rho^{\mathrm{Est}}$, UCB($\alpha=1$)>'	was ranked	2 / 2 for this simulation (last rewards = 6187.05).



Considering the list of players :
 [rhoRand(UCB($\alpha=1$)), rhoRand(UCB($\alpha=1$))]
Number of players in the multi-players game: 2
Time horizon: 10000
Number of repetitions: 20
Sampling rate DELTA_T_SAVE: 1
Using collision model: onlyUniqUserGetsReward
  Detail:  Simple collision model where only the players alone on one arm sample it and receive the reward.

    - This is the default collision model, cf. https://arxiv.org/abs/0910.2065v3 collision model 1.
    - The numpy array 'choices' is increased according to the number of users who collided (it is NOT binary).
    
Creating a new MAB problem ...
  Reading arms of this MAB problem from a dictionnary 'configuration' = {'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': [0.3, 0.4, 0.5, 0.6, 0.7]} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = [0.3, 0.4, 0.5, 0.6, 0.7]
 - with 'arms' = [B(0.3), B(0.4), B(0.5), B(0.6), B(0.7)]
 - with 'nbArms' = 5
 - with 'maxArm' = 0.7
 - with 'minArm' = 0.3

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 9.46 ... 
 - a Optimal Arm Identification factor H_OI(mu) = 60.00% ...
Number of environments to try: 1

Evaluating environment: <MAB{'minArm': 0.29999999999999999, 'nbArms': 5, 'maxArm': 0.69999999999999996, 'arms': [B(0.3), B(0.4), B(0.5), B(0.6), B(0.7)]}>
- Adding player #1 = #1<$\rho^{\mathrm{Rand}}$, UCB($\alpha=1$)> ...
  Using this already created player 'player' = #1<$\rho^{\mathrm{Rand}}$, UCB($\alpha=1$)> ...
- Adding player #2 = #2<$\rho^{\mathrm{Rand}}$, UCB($\alpha=1$)> ...
  Using this already created player 'player' = #2<$\rho^{\mathrm{Rand}}$, UCB($\alpha=1$)> ...

Estimated order by the policy #1<$\rho^{\mathrm{Rand}}$, UCB($\alpha=1$)rank:2> after 10000 steps: [1 0 2 3 4] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 84.00% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 95.00% (relative success)...
  ==> Spearman    distance from optimal ordering: 96.26% (relative success)...
  ==> Gestalt     distance from optimal ordering: 80.00% (relative success)...
  ==> Mean distance from optimal ordering: 88.81% (relative success)...

Estimated order by the policy #2<$\rho^{\mathrm{Rand}}$, UCB($\alpha=1$)rank:2> after 10000 steps: [0 1 2 4 3] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 84.00% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 95.00% (relative success)...
  ==> Spearman    distance from optimal ordering: 96.26% (relative success)...
  ==> Gestalt     distance from optimal ordering: 80.00% (relative success)...
  ==> Mean distance from optimal ordering: 88.81% (relative success)...
Giving the final ranks ...

Final ranking for this environment #0 :
- Player #1, '#1<$\rho^{\mathrm{Rand}}$, UCB($\alpha=1$)>'	was ranked	1 / 2 for this simulation (last rewards = 6475.2).
- Player #2, '#2<$\rho^{\mathrm{Rand}}$, UCB($\alpha=1$)>'	was ranked	2 / 2 for this simulation (last rewards = 6326.6).
plots/MP__M2_T10000_N20__2_algos is already a directory here...



Considering the list of players :
 [rhoEst(Thompson), rhoEst(Thompson)]
Number of players in the multi-players game: 2
Time horizon: 10000
Number of repetitions: 20
Sampling rate DELTA_T_SAVE: 1
Using collision model: onlyUniqUserGetsReward
  Detail:  Simple collision model where only the players alone on one arm sample it and receive the reward.

    - This is the default collision model, cf. https://arxiv.org/abs/0910.2065v3 collision model 1.
    - The numpy array 'choices' is increased according to the number of users who collided (it is NOT binary).
    
Creating a new MAB problem ...
  Reading arms of this MAB problem from a dictionnary 'configuration' = {'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': [0.3, 0.4, 0.5, 0.6, 0.7]} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = [0.3, 0.4, 0.5, 0.6, 0.7]
 - with 'arms' = [B(0.3), B(0.4), B(0.5), B(0.6), B(0.7)]
 - with 'nbArms' = 5
 - with 'maxArm' = 0.7
 - with 'minArm' = 0.3

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 9.46 ... 
 - a Optimal Arm Identification factor H_OI(mu) = 60.00% ...
Number of environments to try: 1

Evaluating environment: <MAB{'minArm': 0.29999999999999999, 'nbArms': 5, 'maxArm': 0.69999999999999996, 'arms': [B(0.3), B(0.4), B(0.5), B(0.6), B(0.7)]}>
- Adding player #1 = #1<$\rho^{\mathrm{Est}}$, Thompson> ...
  Using this already created player 'player' = #1<$\rho^{\mathrm{Est}}$, Thompson> ...
- Adding player #2 = #2<$\rho^{\mathrm{Est}}$, Thompson> ...
  Using this already created player 'player' = #2<$\rho^{\mathrm{Est}}$, Thompson> ...

Estimated order by the policy #1<$\rho^{\mathrm{Est}}$, Thompsonrank:2> after 10000 steps: [0 1 2 3 4] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 100.00% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 98.57% (relative success)...
  ==> Spearman    distance from optimal ordering: 100.00% (relative success)...
  ==> Gestalt     distance from optimal ordering: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 99.64% (relative success)...

Estimated order by the policy #2<$\rho^{\mathrm{Est}}$, Thompsonrank:1> after 10000 steps: [0 1 2 3 4] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 100.00% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 98.57% (relative success)...
  ==> Spearman    distance from optimal ordering: 100.00% (relative success)...
  ==> Gestalt     distance from optimal ordering: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 99.64% (relative success)...
Giving the final ranks ...

Final ranking for this environment #0 :
- Player #2, '#2<$\rho^{\mathrm{Est}}$, Thompson>'	was ranked	1 / 2 for this simulation (last rewards = 6364.35).
- Player #1, '#1<$\rho^{\mathrm{Est}}$, Thompson>'	was ranked	2 / 2 for this simulation (last rewards = 6263.9).
plots/MP__M2_T10000_N20__2_algos is already a directory here...



Considering the list of players :
 [rhoRand(Thompson), rhoRand(Thompson)]
Number of players in the multi-players game: 2
Time horizon: 10000
Number of repetitions: 20
Sampling rate DELTA_T_SAVE: 1
Using collision model: onlyUniqUserGetsReward
  Detail:  Simple collision model where only the players alone on one arm sample it and receive the reward.

    - This is the default collision model, cf. https://arxiv.org/abs/0910.2065v3 collision model 1.
    - The numpy array 'choices' is increased according to the number of users who collided (it is NOT binary).
    
Creating a new MAB problem ...
  Reading arms of this MAB problem from a dictionnary 'configuration' = {'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': [0.3, 0.4, 0.5, 0.6, 0.7]} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = [0.3, 0.4, 0.5, 0.6, 0.7]
 - with 'arms' = [B(0.3), B(0.4), B(0.5), B(0.6), B(0.7)]
 - with 'nbArms' = 5
 - with 'maxArm' = 0.7
 - with 'minArm' = 0.3

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 9.46 ... 
 - a Optimal Arm Identification factor H_OI(mu) = 60.00% ...
Number of environments to try: 1

Evaluating environment: <MAB{'minArm': 0.29999999999999999, 'nbArms': 5, 'maxArm': 0.69999999999999996, 'arms': [B(0.3), B(0.4), B(0.5), B(0.6), B(0.7)]}>
- Adding player #1 = #1<$\rho^{\mathrm{Rand}}$, Thompson> ...
  Using this already created player 'player' = #1<$\rho^{\mathrm{Rand}}$, Thompson> ...
- Adding player #2 = #2<$\rho^{\mathrm{Rand}}$, Thompson> ...
  Using this already created player 'player' = #2<$\rho^{\mathrm{Rand}}$, Thompson> ...

Estimated order by the policy #1<$\rho^{\mathrm{Rand}}$, Thompsonrank:1> after 10000 steps: [1 2 0 3 4] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 68.00% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 85.84% (relative success)...
  ==> Spearman    distance from optimal ordering: 81.19% (relative success)...
  ==> Gestalt     distance from optimal ordering: 80.00% (relative success)...
  ==> Mean distance from optimal ordering: 78.76% (relative success)...

Estimated order by the policy #2<$\rho^{\mathrm{Rand}}$, Thompsonrank:2> after 10000 steps: [0 1 2 3 4] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 100.00% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 98.57% (relative success)...
  ==> Spearman    distance from optimal ordering: 100.00% (relative success)...
  ==> Gestalt     distance from optimal ordering: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 99.64% (relative success)...
Giving the final ranks ...

Final ranking for this environment #0 :
- Player #1, '#1<$\rho^{\mathrm{Rand}}$, Thompson>'	was ranked	1 / 2 for this simulation (last rewards = 6471).
- Player #2, '#2<$\rho^{\mathrm{Rand}}$, Thompson>'	was ranked	2 / 2 for this simulation (last rewards = 6159.45).
plots/MP__M2_T10000_N20__2_algos is already a directory here...



Considering the list of players :
 [rhoEst(KL-UCB(Bern)), rhoEst(KL-UCB(Bern))]
Number of players in the multi-players game: 2
Time horizon: 10000
Number of repetitions: 20
Sampling rate DELTA_T_SAVE: 1
Using collision model: onlyUniqUserGetsReward
  Detail:  Simple collision model where only the players alone on one arm sample it and receive the reward.

    - This is the default collision model, cf. https://arxiv.org/abs/0910.2065v3 collision model 1.
    - The numpy array 'choices' is increased according to the number of users who collided (it is NOT binary).
    
Creating a new MAB problem ...
  Reading arms of this MAB problem from a dictionnary 'configuration' = {'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': [0.3, 0.4, 0.5, 0.6, 0.7]} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = [0.3, 0.4, 0.5, 0.6, 0.7]
 - with 'arms' = [B(0.3), B(0.4), B(0.5), B(0.6), B(0.7)]
 - with 'nbArms' = 5
 - with 'maxArm' = 0.7
 - with 'minArm' = 0.3

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 9.46 ... 
 - a Optimal Arm Identification factor H_OI(mu) = 60.00% ...
Number of environments to try: 1

Evaluating environment: <MAB{'minArm': 0.29999999999999999, 'nbArms': 5, 'maxArm': 0.69999999999999996, 'arms': [B(0.3), B(0.4), B(0.5), B(0.6), B(0.7)]}>
- Adding player #1 = #1<$\rho^{\mathrm{Est}}$, KL-UCB(Bern)> ...
  Using this already created player 'player' = #1<$\rho^{\mathrm{Est}}$, KL-UCB(Bern)> ...
- Adding player #2 = #2<$\rho^{\mathrm{Est}}$, KL-UCB(Bern)> ...
  Using this already created player 'player' = #2<$\rho^{\mathrm{Est}}$, KL-UCB(Bern)> ...

Estimated order by the policy #1<$\rho^{\mathrm{Est}}$, KL-UCB(Bern)rank:2> after 10000 steps: [0 1 2 3 4] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 100.00% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 98.57% (relative success)...
  ==> Spearman    distance from optimal ordering: 100.00% (relative success)...
  ==> Gestalt     distance from optimal ordering: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 99.64% (relative success)...

Estimated order by the policy #2<$\rho^{\mathrm{Est}}$, KL-UCB(Bern)rank:2> after 10000 steps: [0 1 2 4 3] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 84.00% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 95.00% (relative success)...
  ==> Spearman    distance from optimal ordering: 96.26% (relative success)...
  ==> Gestalt     distance from optimal ordering: 80.00% (relative success)...
  ==> Mean distance from optimal ordering: 88.81% (relative success)...
Giving the final ranks ...

Final ranking for this environment #0 :
- Player #2, '#2<$\rho^{\mathrm{Est}}$, KL-UCB(Bern)>'	was ranked	1 / 2 for this simulation (last rewards = 6495.9).
- Player #1, '#1<$\rho^{\mathrm{Est}}$, KL-UCB(Bern)>'	was ranked	2 / 2 for this simulation (last rewards = 6274.75).
plots/MP__M2_T10000_N20__2_algos is already a directory here...



Considering the list of players :
 [rhoRand(KL-UCB(Bern)), rhoRand(KL-UCB(Bern))]
Number of players in the multi-players game: 2
Time horizon: 10000
Number of repetitions: 20
Sampling rate DELTA_T_SAVE: 1
Using collision model: onlyUniqUserGetsReward
  Detail:  Simple collision model where only the players alone on one arm sample it and receive the reward.

    - This is the default collision model, cf. https://arxiv.org/abs/0910.2065v3 collision model 1.
    - The numpy array 'choices' is increased according to the number of users who collided (it is NOT binary).
    
Creating a new MAB problem ...
  Reading arms of this MAB problem from a dictionnary 'configuration' = {'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': [0.3, 0.4, 0.5, 0.6, 0.7]} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = [0.3, 0.4, 0.5, 0.6, 0.7]
 - with 'arms' = [B(0.3), B(0.4), B(0.5), B(0.6), B(0.7)]
 - with 'nbArms' = 5
 - with 'maxArm' = 0.7
 - with 'minArm' = 0.3

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 9.46 ... 
 - a Optimal Arm Identification factor H_OI(mu) = 60.00% ...
Number of environments to try: 1

Evaluating environment: <MAB{'minArm': 0.29999999999999999, 'nbArms': 5, 'maxArm': 0.69999999999999996, 'arms': [B(0.3), B(0.4), B(0.5), B(0.6), B(0.7)]}>
- Adding player #1 = #1<$\rho^{\mathrm{Rand}}$, KL-UCB(Bern)> ...
  Using this already created player 'player' = #1<$\rho^{\mathrm{Rand}}$, KL-UCB(Bern)> ...
- Adding player #2 = #2<$\rho^{\mathrm{Rand}}$, KL-UCB(Bern)> ...
  Using this already created player 'player' = #2<$\rho^{\mathrm{Rand}}$, KL-UCB(Bern)> ...

Estimated order by the policy #1<$\rho^{\mathrm{Rand}}$, KL-UCB(Bern)rank:2> after 10000 steps: [0 2 1 3 4] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 84.00% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 95.00% (relative success)...
  ==> Spearman    distance from optimal ordering: 96.26% (relative success)...
  ==> Gestalt     distance from optimal ordering: 80.00% (relative success)...
  ==> Mean distance from optimal ordering: 88.81% (relative success)...

Estimated order by the policy #2<$\rho^{\mathrm{Rand}}$, KL-UCB(Bern)rank:1> after 10000 steps: [3 2 1 0 4] ...
  ==> Optimal arm identification: 76.92% (relative success)...
  ==> Manhattan   distance from optimal ordering: 36.00% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 37.58% (relative success)...
  ==> Spearman    distance from optimal ordering: 0.00% (relative success)...
  ==> Gestalt     distance from optimal ordering: 40.00% (relative success)...
  ==> Mean distance from optimal ordering: 28.39% (relative success)...
Giving the final ranks ...

Final ranking for this environment #0 :
- Player #1, '#1<$\rho^{\mathrm{Rand}}$, KL-UCB(Bern)>'	was ranked	1 / 2 for this simulation (last rewards = 6495.7).
- Player #2, '#2<$\rho^{\mathrm{Rand}}$, KL-UCB(Bern)>'	was ranked	2 / 2 for this simulation (last rewards = 6309.25).
plots/MP__M2_T10000_N20__2_algos is already a directory here...



Considering the list of players :
 [rhoEst(BayesUCB), rhoEst(BayesUCB)]
Number of players in the multi-players game: 2
Time horizon: 10000
Number of repetitions: 20
Sampling rate DELTA_T_SAVE: 1
Using collision model: onlyUniqUserGetsReward
  Detail:  Simple collision model where only the players alone on one arm sample it and receive the reward.

    - This is the default collision model, cf. https://arxiv.org/abs/0910.2065v3 collision model 1.
    - The numpy array 'choices' is increased according to the number of users who collided (it is NOT binary).
    
Creating a new MAB problem ...
  Reading arms of this MAB problem from a dictionnary 'configuration' = {'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': [0.3, 0.4, 0.5, 0.6, 0.7]} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = [0.3, 0.4, 0.5, 0.6, 0.7]
 - with 'arms' = [B(0.3), B(0.4), B(0.5), B(0.6), B(0.7)]
 - with 'nbArms' = 5
 - with 'maxArm' = 0.7
 - with 'minArm' = 0.3

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 9.46 ... 
 - a Optimal Arm Identification factor H_OI(mu) = 60.00% ...
Number of environments to try: 1

Evaluating environment: <MAB{'minArm': 0.29999999999999999, 'nbArms': 5, 'maxArm': 0.69999999999999996, 'arms': [B(0.3), B(0.4), B(0.5), B(0.6), B(0.7)]}>
- Adding player #1 = #1<$\rho^{\mathrm{Est}}$, BayesUCB> ...
  Using this already created player 'player' = #1<$\rho^{\mathrm{Est}}$, BayesUCB> ...
- Adding player #2 = #2<$\rho^{\mathrm{Est}}$, BayesUCB> ...
  Using this already created player 'player' = #2<$\rho^{\mathrm{Est}}$, BayesUCB> ...

Estimated order by the policy #1<$\rho^{\mathrm{Est}}$, BayesUCBrank:2> after 10000 steps: [0 1 2 3 4] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 100.00% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 98.57% (relative success)...
  ==> Spearman    distance from optimal ordering: 100.00% (relative success)...
  ==> Gestalt     distance from optimal ordering: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 99.64% (relative success)...

Estimated order by the policy #2<$\rho^{\mathrm{Est}}$, BayesUCBrank:1> after 10000 steps: [2 0 3 1 4] ...
  ==> Optimal arm identification: 84.62% (relative success)...
  ==> Manhattan   distance from optimal ordering: 52.00% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 67.28% (relative success)...
  ==> Spearman    distance from optimal ordering: 60.90% (relative success)...
  ==> Gestalt     distance from optimal ordering: 60.00% (relative success)...
  ==> Mean distance from optimal ordering: 60.05% (relative success)...
Giving the final ranks ...

Final ranking for this environment #0 :
- Player #2, '#2<$\rho^{\mathrm{Est}}$, BayesUCB>'	was ranked	1 / 2 for this simulation (last rewards = 6411.9).
- Player #1, '#1<$\rho^{\mathrm{Est}}$, BayesUCB>'	was ranked	2 / 2 for this simulation (last rewards = 6410.85).
plots/MP__M2_T10000_N20__2_algos is already a directory here...



Considering the list of players :
 [rhoRand(BayesUCB), rhoRand(BayesUCB)]
Number of players in the multi-players game: 2
Time horizon: 10000
Number of repetitions: 20
Sampling rate DELTA_T_SAVE: 1
Using collision model: onlyUniqUserGetsReward
  Detail:  Simple collision model where only the players alone on one arm sample it and receive the reward.

    - This is the default collision model, cf. https://arxiv.org/abs/0910.2065v3 collision model 1.
    - The numpy array 'choices' is increased according to the number of users who collided (it is NOT binary).
    
Creating a new MAB problem ...
  Reading arms of this MAB problem from a dictionnary 'configuration' = {'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': [0.3, 0.4, 0.5, 0.6, 0.7]} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = [0.3, 0.4, 0.5, 0.6, 0.7]
 - with 'arms' = [B(0.3), B(0.4), B(0.5), B(0.6), B(0.7)]
 - with 'nbArms' = 5
 - with 'maxArm' = 0.7
 - with 'minArm' = 0.3

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 9.46 ... 
 - a Optimal Arm Identification factor H_OI(mu) = 60.00% ...
Number of environments to try: 1

Evaluating environment: <MAB{'minArm': 0.29999999999999999, 'nbArms': 5, 'maxArm': 0.69999999999999996, 'arms': [B(0.3), B(0.4), B(0.5), B(0.6), B(0.7)]}>
- Adding player #1 = #1<$\rho^{\mathrm{Rand}}$, BayesUCB> ...
  Using this already created player 'player' = #1<$\rho^{\mathrm{Rand}}$, BayesUCB> ...
- Adding player #2 = #2<$\rho^{\mathrm{Rand}}$, BayesUCB> ...
  Using this already created player 'player' = #2<$\rho^{\mathrm{Rand}}$, BayesUCB> ...

Estimated order by the policy #1<$\rho^{\mathrm{Rand}}$, BayesUCBrank:2> after 10000 steps: [0 1 2 3 4] ...
  ==> Optimal arm identification: 100.00% (relative success)...
  ==> Manhattan   distance from optimal ordering: 100.00% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 98.57% (relative success)...
  ==> Spearman    distance from optimal ordering: 100.00% (relative success)...
  ==> Gestalt     distance from optimal ordering: 100.00% (relative success)...
  ==> Mean distance from optimal ordering: 99.64% (relative success)...

Estimated order by the policy #2<$\rho^{\mathrm{Rand}}$, BayesUCBrank:1> after 10000 steps: [1 3 2 0 4] ...
  ==> Optimal arm identification: 76.92% (relative success)...
  ==> Manhattan   distance from optimal ordering: 52.00% (relative success)...
  ==> Kendell Tau distance from optimal ordering: 37.58% (relative success)...
  ==> Spearman    distance from optimal ordering: 37.62% (relative success)...
  ==> Gestalt     distance from optimal ordering: 60.00% (relative success)...
  ==> Mean distance from optimal ordering: 46.80% (relative success)...
Giving the final ranks ...

Final ranking for this environment #0 :
- Player #2, '#2<$\rho^{\mathrm{Rand}}$, BayesUCB>'	was ranked	1 / 2 for this simulation (last rewards = 6494.1).
- Player #1, '#1<$\rho^{\mathrm{Rand}}$, BayesUCB>'	was ranked	2 / 2 for this simulation (last rewards = 6343.8).
plots/MP__M2_T10000_N20__2_algos is already a directory here...


- Plotting the centralized regret for all 'players' values
  and saving the plot to plots/MP__M2_T10000_N20__2_algos/all_RegretCentralized____env1-1_5862843027408232141 ...
  - For 2 player, our lower bound gave = 18 ...
  - For 2 player, the initial lower bound in Theorem 6 from [Anandkumar et al., 2010] gave = 12.1 ...

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 9.46 for 1-player problem ... 
 - a Optimal Arm Identification factor H_OI(mu) = 60.00% ...
 - Our lowerbound = 18,
 - [Anandkumar et al] lowerbound = 12.1
Saving figure with format png, to file 'plots/MP__M2_T10000_N20__2_algos/all_RegretCentralized____env1-1_5862843027408232141.png'...
       Saved! 'plots/MP__M2_T10000_N20__2_algos/all_RegretCentralized____env1-1_5862843027408232141.png' created of size '303575b', at 'Fri Mar 17 15:24:32 2017' ...
Saving figure with format pdf, to file 'plots/MP__M2_T10000_N20__2_algos/all_RegretCentralized____env1-1_5862843027408232141.pdf'...
       Saved! 'plots/MP__M2_T10000_N20__2_algos/all_RegretCentralized____env1-1_5862843027408232141.pdf' created of size '382783b', at 'Fri Mar 17 15:24:32 2017' ...


- Plotting the centralized regret for all 'players' values, in semilogx scale
  and saving the plot to plots/MP__M2_T10000_N20__2_algos/all_RegretCentralized_semilogx____env1-1_5862843027408232141 ...
  - For 2 player, our lower bound gave = 18 ...
  - For 2 player, the initial lower bound in Theorem 6 from [Anandkumar et al., 2010] gave = 12.1 ...

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 9.46 for 1-player problem ... 
 - a Optimal Arm Identification factor H_OI(mu) = 60.00% ...
 - Our lowerbound = 18,
 - [Anandkumar et al] lowerbound = 12.1
Saving figure with format png, to file 'plots/MP__M2_T10000_N20__2_algos/all_RegretCentralized_semilogx____env1-1_5862843027408232141.png'...
       Saved! 'plots/MP__M2_T10000_N20__2_algos/all_RegretCentralized_semilogx____env1-1_5862843027408232141.png' created of size '307453b', at 'Fri Mar 17 15:24:33 2017' ...
Saving figure with format pdf, to file 'plots/MP__M2_T10000_N20__2_algos/all_RegretCentralized_semilogx____env1-1_5862843027408232141.pdf'...
       Saved! 'plots/MP__M2_T10000_N20__2_algos/all_RegretCentralized_semilogx____env1-1_5862843027408232141.pdf' created of size '355305b', at 'Fri Mar 17 15:24:33 2017' ...


- Plotting the centralized regret for all 'players' values, in semilogy scale
  and saving the plot to plots/MP__M2_T10000_N20__2_algos/all_RegretCentralized_semilogy____env1-1_5862843027408232141 ...
  - For 2 player, our lower bound gave = 18 ...
  - For 2 player, the initial lower bound in Theorem 6 from [Anandkumar et al., 2010] gave = 12.1 ...

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 9.46 for 1-player problem ... 
 - a Optimal Arm Identification factor H_OI(mu) = 60.00% ...
 - Our lowerbound = 18,
 - [Anandkumar et al] lowerbound = 12.1
Saving figure with format png, to file 'plots/MP__M2_T10000_N20__2_algos/all_RegretCentralized_semilogy____env1-1_5862843027408232141.png'...
       Saved! 'plots/MP__M2_T10000_N20__2_algos/all_RegretCentralized_semilogy____env1-1_5862843027408232141.png' created of size '220235b', at 'Fri Mar 17 15:24:35 2017' ...
Saving figure with format pdf, to file 'plots/MP__M2_T10000_N20__2_algos/all_RegretCentralized_semilogy____env1-1_5862843027408232141.pdf'...
       Saved! 'plots/MP__M2_T10000_N20__2_algos/all_RegretCentralized_semilogy____env1-1_5862843027408232141.pdf' created of size '200844b', at 'Fri Mar 17 15:24:35 2017' ...


- Plotting the centralized regret for all 'players' values, in loglog scale
  and saving the plot to plots/MP__M2_T10000_N20__2_algos/all_RegretCentralized_loglog____env1-1_5862843027408232141 ...
  - For 2 player, our lower bound gave = 18 ...
  - For 2 player, the initial lower bound in Theorem 6 from [Anandkumar et al., 2010] gave = 12.1 ...

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 9.46 for 1-player problem ... 
 - a Optimal Arm Identification factor H_OI(mu) = 60.00% ...
 - Our lowerbound = 18,
 - [Anandkumar et al] lowerbound = 12.1
Saving figure with format png, to file 'plots/MP__M2_T10000_N20__2_algos/all_RegretCentralized_loglog____env1-1_5862843027408232141.png'...
       Saved! 'plots/MP__M2_T10000_N20__2_algos/all_RegretCentralized_loglog____env1-1_5862843027408232141.png' created of size '311452b', at 'Fri Mar 17 15:24:36 2017' ...
Saving figure with format pdf, to file 'plots/MP__M2_T10000_N20__2_algos/all_RegretCentralized_loglog____env1-1_5862843027408232141.pdf'...
       Saved! 'plots/MP__M2_T10000_N20__2_algos/all_RegretCentralized_loglog____env1-1_5862843027408232141.pdf' created of size '344357b', at 'Fri Mar 17 15:24:37 2017' ...


- Plotting the centralized fairness (RajJain)
  and saving the plot to plots/MP__M2_T10000_N20__2_algos/all____env1-1_5862843027408232141 ...
Saving figure with format png, to file 'plots/MP__M2_T10000_N20__2_algos/all____env1-1_5862843027408232141.png'...
       Saved! 'plots/MP__M2_T10000_N20__2_algos/all____env1-1_5862843027408232141.png' created of size '153527b', at 'Fri Mar 17 15:24:38 2017' ...
Saving figure with format pdf, to file 'plots/MP__M2_T10000_N20__2_algos/all____env1-1_5862843027408232141.pdf'...
       Saved! 'plots/MP__M2_T10000_N20__2_algos/all____env1-1_5862843027408232141.pdf' created of size '91195b', at 'Fri Mar 17 15:24:38 2017' ...
 - Plotting the total nb of collision as a function of time for all 'players' values
  and saving the plot to plots/MP__M2_T10000_N20__2_algos/all_NbCollisions____env1-1_5862843027408232141 ...
Saving figure with format png, to file 'plots/MP__M2_T10000_N20__2_algos/all_NbCollisions____env1-1_5862843027408232141.png'...
       Saved! 'plots/MP__M2_T10000_N20__2_algos/all_NbCollisions____env1-1_5862843027408232141.png' created of size '154481b', at 'Fri Mar 17 15:24:39 2017' ...
Saving figure with format pdf, to file 'plots/MP__M2_T10000_N20__2_algos/all_NbCollisions____env1-1_5862843027408232141.pdf'...
       Saved! 'plots/MP__M2_T10000_N20__2_algos/all_NbCollisions____env1-1_5862843027408232141.pdf' created of size '62757b', at 'Fri Mar 17 15:24:39 2017' ...
 - Plotting the cumulated total nb of collision as a function of time for all 'players' values
  and saving the plot to plots/MP__M2_T10000_N20__2_algos/all_CumNbCollisions____env1-1_5862843027408232141 ...
Saving figure with format png, to file 'plots/MP__M2_T10000_N20__2_algos/all_CumNbCollisions____env1-1_5862843027408232141.png'...
       Saved! 'plots/MP__M2_T10000_N20__2_algos/all_CumNbCollisions____env1-1_5862843027408232141.png' created of size '180068b', at 'Fri Mar 17 15:24:42 2017' ...
Saving figure with format pdf, to file 'plots/MP__M2_T10000_N20__2_algos/all_CumNbCollisions____env1-1_5862843027408232141.pdf'...
       Saved! 'plots/MP__M2_T10000_N20__2_algos/all_CumNbCollisions____env1-1_5862843027408232141.pdf' created of size '83814b', at 'Fri Mar 17 15:24:42 2017' ...


- Plotting the number of switches as a function of time for all 'players' values
  and saving the plot to plots/MP__M2_T10000_N20__2_algos/all_CumNbSwitchs____env1-1_5862843027408232141 ...
Saving figure with format png, to file 'plots/MP__M2_T10000_N20__2_algos/all_CumNbSwitchs____env1-1_5862843027408232141.png'...
       Saved! 'plots/MP__M2_T10000_N20__2_algos/all_CumNbSwitchs____env1-1_5862843027408232141.png' created of size '223758b', at 'Fri Mar 17 15:24:43 2017' ...
Saving figure with format pdf, to file 'plots/MP__M2_T10000_N20__2_algos/all_CumNbSwitchs____env1-1_5862843027408232141.pdf'...
       Saved! 'plots/MP__M2_T10000_N20__2_algos/all_CumNbSwitchs____env1-1_5862843027408232141.pdf' created of size '108162b', at 'Fri Mar 17 15:24:44 2017' ...


==> To see the figures, do :
eog plots/MP__M2_T10000_N20__2_algos/all*5862843027408232141.png
Done for simulations main_multiplayers.py ...
