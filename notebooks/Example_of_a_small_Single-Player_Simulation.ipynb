{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "# An example of a small Single-Player simulation\n",
    "\n",
    "First, be sure to be in the main folder, and import `Evaluator` from `Environment` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sys import path\n",
    "path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Setting dpi of all figures to 110 ...\n",
      " - Setting 'figsize' of all figures to (19.8, 10.8) ...\n",
      "Info: Using the Jupyter notebook version of the tqdm() decorator, tqdm_notebook() ...\n"
     ]
    }
   ],
   "source": [
    "# Local imports\n",
    "from Environment import Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We also need arms, for instance `Bernoulli`-distributed arm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: numba.jit seems to be available.\n"
     ]
    }
   ],
   "source": [
    "# Import arms\n",
    "from Arms import makeMeans, Bernoulli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "And finally we need some single-player Reinforcement Learning algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: numba.jit seems to be available.\n"
     ]
    }
   ],
   "source": [
    "# Import algorithms\n",
    "from Policies import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For instance, this imported the `UCB` algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class UCB in module Policies.UCB:\n",
      "\n",
      "class UCB(Policies.IndexPolicy.IndexPolicy)\n",
      " |  The UCB policy for bounded bandits.\n",
      " |  Reference: [Lai & Robbins, 1985].\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      UCB\n",
      " |      Policies.IndexPolicy.IndexPolicy\n",
      " |      Policies.BasePolicy.BasePolicy\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  computeIndex(self, arm)\n",
      " |      Compute the current index of arm 'arm'.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from Policies.IndexPolicy.IndexPolicy:\n",
      " |  \n",
      " |  __init__(self, nbArms, lower=0.0, amplitude=1.0)\n",
      " |      New generic index policy.\n",
      " |      \n",
      " |      - nbArms: the number of arms,\n",
      " |      - lower, amplitude: lower value and known amplitude of the rewards.\n",
      " |  \n",
      " |  choice(self)\n",
      " |      In an index policy, choose an arm with maximal index (uniformly at random).\n",
      " |  \n",
      " |  choiceFromSubSet(self, availableArms='all')\n",
      " |      In an index policy, choose the best arm from sub-set availableArms (uniformly at random).\n",
      " |  \n",
      " |  choiceIMP(self, nb=1, startWithChoiceMultiple=True)\n",
      " |      In an index policy, the IMP strategy is hybrid: choose nb-1 arms with maximal empirical averages, then 1 arm with maximal index. Cf. algorithm IMP-TS [Komiyama, Honda, Nakagawa, 2016, arXiv 1506.00779].\n",
      " |  \n",
      " |  choiceMultiple(self, nb=1)\n",
      " |      In an index policy, choose nb arms with maximal indexes (uniformly at random).\n",
      " |  \n",
      " |  choiceWithRank(self, rank=1)\n",
      " |      In an index policy, choose an arm with index is the (1+rank)-th best (uniformly at random).\n",
      " |      \n",
      " |      - For instance, if rank is 1, the best arm is chosen (the 1-st best).\n",
      " |      - If rank is 4, the 4-th best arm is chosen.\n",
      " |      \n",
      " |      - Note: this method is *required* for the rhoRand policy.\n",
      " |  \n",
      " |  estimatedOrder(self)\n",
      " |      Return the estimate order of the arms, as a permutation on [0..K-1] that would order the arms by increasing means.\n",
      " |  \n",
      " |  startGame(self)\n",
      " |      Initialize the policy for a new game.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from Policies.BasePolicy.BasePolicy:\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  getReward(self, arm, reward)\n",
      " |      # def getReward(self, arm, reward, checkBounds=False):  # XXX useless checkBounds feature\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from Policies.BasePolicy.BasePolicy:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(UCB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "## Creating the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Parameters for the simulation\n",
    "- $T = 10000$ is the time horizon,\n",
    "- $N = 100$ is the number of repetitions,\n",
    "- `N_JOBS = 4` is the number of cores used to parallelize the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "HORIZON = 10000\n",
    "REPETITIONS = 100\n",
    "N_JOBS = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Some MAB problem with Bernoulli arms\n",
    "We consider in this example $3$ problems, with `Bernoulli` arms, of different means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ENVIRONMENTS = [  # 1)  Bernoulli arms\n",
    "        {   # A very easy problem, but it is used in a lot of articles\n",
    "            \"arm_type\": Bernoulli,\n",
    "            \"params\": [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "        },\n",
    "        {   # An other problem, best arm = last, with three groups: very bad arms (0.01, 0.02), middle arms (0.3 - 0.6) and very good arms (0.78, 0.8, 0.82)\n",
    "            \"arm_type\": Bernoulli,\n",
    "            \"params\": [0.01, 0.02, 0.3, 0.4, 0.5, 0.6, 0.795, 0.8, 0.805]\n",
    "        },\n",
    "        {   # A very hard problem, as used in [Capp√© et al, 2012]\n",
    "            \"arm_type\": Bernoulli,\n",
    "            \"params\": [0.01, 0.01, 0.01, 0.02, 0.02, 0.02, 0.05, 0.05, 0.1]\n",
    "        },\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Some RL algorithms\n",
    "We compare Thompson Sampling against $\\mathrm{UCB}_1$, and $\\mathrm{kl}-\\mathrm{UCB}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "POLICIES = [\n",
    "        # --- UCB1 algorithm\n",
    "        {\n",
    "            \"archtype\": UCB,\n",
    "            \"params\": {}\n",
    "        },\n",
    "        # --- Thompson algorithm\n",
    "        {\n",
    "            \"archtype\": Thompson,\n",
    "            \"params\": {}\n",
    "        },\n",
    "        # --- KL algorithms, here only klUCB\n",
    "        {\n",
    "            \"archtype\": klUCB,\n",
    "            \"params\": {}\n",
    "        },\n",
    "        # --- BayesUCB algorithm\n",
    "        {\n",
    "            \"archtype\": BayesUCB,\n",
    "            \"params\": {}\n",
    "        },\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Complete configuration for the problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'environment': [{'arm_type': Arms.Bernoulli.Bernoulli,\n",
       "   'params': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]},\n",
       "  {'arm_type': Arms.Bernoulli.Bernoulli,\n",
       "   'params': [0.01, 0.02, 0.3, 0.4, 0.5, 0.6, 0.795, 0.8, 0.805]},\n",
       "  {'arm_type': Arms.Bernoulli.Bernoulli,\n",
       "   'params': [0.01, 0.01, 0.01, 0.02, 0.02, 0.02, 0.05, 0.05, 0.1]}],\n",
       " 'horizon': 10000,\n",
       " 'n_jobs': 4,\n",
       " 'policies': [{'archtype': Policies.UCB.UCB, 'params': {}},\n",
       "  {'archtype': Policies.Thompson.Thompson, 'params': {}},\n",
       "  {'archtype': Policies.klUCB.klUCB, 'params': {}},\n",
       "  {'archtype': Policies.BayesUCB.BayesUCB, 'params': {}}],\n",
       " 'repetitions': 100,\n",
       " 'verbosity': 6}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configuration = {\n",
    "    # --- Duration of the experiment\n",
    "    \"horizon\": HORIZON,\n",
    "    # --- Number of repetition of the experiment (to have an average)\n",
    "    \"repetitions\": REPETITIONS,\n",
    "    # --- Parameters for the use of joblib.Parallel\n",
    "    \"n_jobs\": N_JOBS,    # = nb of CPU cores\n",
    "    \"verbosity\": 6,      # Max joblib verbosity\n",
    "    # --- Arms\n",
    "    \"environment\": ENVIRONMENTS,\n",
    "    # --- Algorithms\n",
    "    \"policies\": POLICIES,\n",
    "}\n",
    "configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "## Solving the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of policies in this comparison: 4\n",
      "Time horizon: 10000\n",
      "Number of repetitions: 100\n",
      "Sampling rate DELTA_T_SAVE: 1\n",
      "Creating a new MAB problem ...\n",
      "  Reading arms of this MAB problem from a dictionnary 'configuration' = {'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]} ...\n",
      " - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>\n",
      " - with 'params' = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
      " - with 'arms' = [B(0.1), B(0.2), B(0.3), B(0.4), B(0.5), B(0.6), B(0.7), B(0.8), B(0.9)]\n",
      " - with 'nbArms' = 9\n",
      " - with 'maxArm' = 0.9\n",
      " - with 'minArm' = 0.1\n",
      "\n",
      "This MAB problem has: \n",
      " - a [Lai & Robbins] complexity constant C(mu) = 7.52 ... \n",
      " - a Optimal Arm Identification factor H_OI(mu) = 48.89% ...\n",
      "Creating a new MAB problem ...\n",
      "  Reading arms of this MAB problem from a dictionnary 'configuration' = {'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': [0.01, 0.02, 0.3, 0.4, 0.5, 0.6, 0.795, 0.8, 0.805]} ...\n",
      " - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>\n",
      " - with 'params' = [0.01, 0.02, 0.3, 0.4, 0.5, 0.6, 0.795, 0.8, 0.805]\n",
      " - with 'arms' = [B(0.01), B(0.02), B(0.3), B(0.4), B(0.5), B(0.6), B(0.795), B(0.8), B(0.805)]\n",
      " - with 'nbArms' = 9\n",
      " - with 'maxArm' = 0.805\n",
      " - with 'minArm' = 0.01\n",
      "\n",
      "This MAB problem has: \n",
      " - a [Lai & Robbins] complexity constant C(mu) = 101 ... \n",
      " - a Optimal Arm Identification factor H_OI(mu) = 55.39% ...\n",
      "Creating a new MAB problem ...\n",
      "  Reading arms of this MAB problem from a dictionnary 'configuration' = {'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': [0.01, 0.01, 0.01, 0.02, 0.02, 0.02, 0.05, 0.05, 0.1]} ...\n",
      " - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>\n",
      " - with 'params' = [0.01, 0.01, 0.01, 0.02, 0.02, 0.02, 0.05, 0.05, 0.1]\n",
      " - with 'arms' = [B(0.01), B(0.01), B(0.01), B(0.02), B(0.02), B(0.02), B(0.05), B(0.05), B(0.1)]\n",
      " - with 'nbArms' = 9\n",
      " - with 'maxArm' = 0.1\n",
      " - with 'minArm' = 0.01\n",
      "\n",
      "This MAB problem has: \n",
      " - a [Lai & Robbins] complexity constant C(mu) = 14.5 ... \n",
      " - a Optimal Arm Identification factor H_OI(mu) = 82.11% ...\n",
      "Number of environments to try: 3\n"
     ]
    }
   ],
   "source": [
    "evaluation = Evaluator(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating environment: <MAB{'nbArms': 9, 'maxArm': 0.90000000000000002, 'minArm': 0.10000000000000001, 'arms': [B(0.1), B(0.2), B(0.3), B(0.4), B(0.5), B(0.6), B(0.7), B(0.8), B(0.9)]}>\n",
      "- Adding policy #1 = {'params': {}, 'archtype': <class 'Policies.UCB.UCB'>} ...\n",
      "  Creating this policy from a dictionnary 'self.cfg['policies'][0]' = {'params': {}, 'archtype': <class 'Policies.UCB.UCB'>} ...\n",
      "- Adding policy #2 = {'params': {}, 'archtype': <class 'Policies.Thompson.Thompson'>} ...\n",
      "  Creating this policy from a dictionnary 'self.cfg['policies'][1]' = {'params': {}, 'archtype': <class 'Policies.Thompson.Thompson'>} ...\n",
      "- Adding policy #3 = {'params': {}, 'archtype': <class 'Policies.klUCB.klUCB'>} ...\n",
      "  Creating this policy from a dictionnary 'self.cfg['policies'][2]' = {'params': {}, 'archtype': <class 'Policies.klUCB.klUCB'>} ...\n",
      "- Adding policy #4 = {'params': {}, 'archtype': <class 'Policies.BayesUCB.BayesUCB'>} ...\n",
      "  Creating this policy from a dictionnary 'self.cfg['policies'][3]' = {'params': {}, 'archtype': <class 'Policies.BayesUCB.BayesUCB'>} ...\n",
      "\n",
      "- Evaluating policy #1/4: UCB ...\n",
      "\n",
      "Estimated order by the policy UCB after 10000 steps: [1 0 3 5 4 6 7 2 8] ...\n",
      "  ==> Optimal arm identification: 100.00% (relative success)...\n",
      "  ==> Manhattan   distance from optimal ordering: 70.37% (relative success)...\n",
      "  ==> Kendell Tau distance from optimal ordering: 97.82% (relative success)...\n",
      "  ==> Spearman    distance from optimal ordering: 97.02% (relative success)...\n",
      "  ==> Gestalt     distance from optimal ordering: 66.67% (relative success)...\n",
      "  ==> Mean distance from optimal ordering: 82.97% (relative success)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:   10.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "- Evaluating policy #2/4: Thompson ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:   23.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Estimated order by the policy Thompson after 10000 steps: [0 3 5 2 1 4 6 7 8] ...\n",
      "  ==> Optimal arm identification: 100.00% (relative success)...\n",
      "  ==> Manhattan   distance from optimal ordering: 75.31% (relative success)...\n",
      "  ==> Kendell Tau distance from optimal ordering: 98.77% (relative success)...\n",
      "  ==> Spearman    distance from optimal ordering: 99.04% (relative success)...\n",
      "  ==> Gestalt     distance from optimal ordering: 66.67% (relative success)...\n",
      "  ==> Mean distance from optimal ordering: 84.94% (relative success)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    6.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "- Evaluating policy #3/4: KL-UCB(Bern) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:   16.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Estimated order by the policy KL-UCB(Bern) after 10000 steps: [2 1 0 6 4 5 3 7 8] ...\n",
      "  ==> Optimal arm identification: 100.00% (relative success)...\n",
      "  ==> Manhattan   distance from optimal ordering: 75.31% (relative success)...\n",
      "  ==> Kendell Tau distance from optimal ordering: 96.29% (relative success)...\n",
      "  ==> Spearman    distance from optimal ordering: 98.75% (relative success)...\n",
      "  ==> Gestalt     distance from optimal ordering: 55.56% (relative success)...\n",
      "  ==> Mean distance from optimal ordering: 81.48% (relative success)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:   14.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:   33.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- Evaluating policy #4/4: BayesUCB ...\n",
      "\n",
      "Estimated order by the policy BayesUCB after 10000 steps: [2 4 0 7 6 5 1 3 8] ...\n",
      "  ==> Optimal arm identification: 100.00% (relative success)...\n",
      "  ==> Manhattan   distance from optimal ordering: 45.68% (relative success)...\n",
      "  ==> Kendell Tau distance from optimal ordering: 59.58% (relative success)...\n",
      "  ==> Spearman    distance from optimal ordering: 64.42% (relative success)...\n",
      "  ==> Gestalt     distance from optimal ordering: 44.44% (relative success)...\n",
      "  ==> Mean distance from optimal ordering: 53.53% (relative success)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:    3.7s\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:   18.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Evaluating environment: <MAB{'nbArms': 9, 'maxArm': 0.80500000000000005, 'minArm': 0.01, 'arms': [B(0.01), B(0.02), B(0.3), B(0.4), B(0.5), B(0.6), B(0.795), B(0.8), B(0.805)]}>\n",
      "- Adding policy #1 = {'params': {}, 'archtype': <class 'Policies.UCB.UCB'>} ...\n",
      "  Creating this policy from a dictionnary 'self.cfg['policies'][0]' = {'params': {}, 'archtype': <class 'Policies.UCB.UCB'>} ...\n",
      "- Adding policy #2 = {'params': {}, 'archtype': <class 'Policies.Thompson.Thompson'>} ...\n",
      "  Creating this policy from a dictionnary 'self.cfg['policies'][1]' = {'params': {}, 'archtype': <class 'Policies.Thompson.Thompson'>} ...\n",
      "- Adding policy #3 = {'params': {}, 'archtype': <class 'Policies.klUCB.klUCB'>} ...\n",
      "  Creating this policy from a dictionnary 'self.cfg['policies'][2]' = {'params': {}, 'archtype': <class 'Policies.klUCB.klUCB'>} ...\n",
      "- Adding policy #4 = {'params': {}, 'archtype': <class 'Policies.BayesUCB.BayesUCB'>} ...\n",
      "  Creating this policy from a dictionnary 'self.cfg['policies'][3]' = {'params': {}, 'archtype': <class 'Policies.BayesUCB.BayesUCB'>} ...\n",
      "\n",
      "- Evaluating policy #1/4: UCB ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:   37.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Estimated order by the policy UCB after 10000 steps: [0 1 3 2 5 4 7 6 8] ...\n",
      "  ==> Optimal arm identification: 100.00% (relative success)...\n",
      "  ==> Manhattan   distance from optimal ordering: 85.19% (relative success)...\n",
      "  ==> Kendell Tau distance from optimal ordering: 99.82% (relative success)...\n",
      "  ==> Spearman    distance from optimal ordering: 99.99% (relative success)...\n",
      "  ==> Gestalt     distance from optimal ordering: 66.67% (relative success)...\n",
      "  ==> Mean distance from optimal ordering: 87.92% (relative success)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:   12.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "- Evaluating policy #2/4: Thompson ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:   28.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Estimated order by the policy Thompson after 10000 steps: [0 4 3 2 1 5 6 7 8] ...\n",
      "  ==> Optimal arm identification: 100.00% (relative success)...\n",
      "  ==> Manhattan   distance from optimal ordering: 80.25% (relative success)...\n",
      "  ==> Kendell Tau distance from optimal ordering: 98.77% (relative success)...\n",
      "  ==> Spearman    distance from optimal ordering: 99.47% (relative success)...\n",
      "  ==> Gestalt     distance from optimal ordering: 66.67% (relative success)...\n",
      "  ==> Mean distance from optimal ordering: 86.29% (relative success)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    6.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "- Evaluating policy #3/4: KL-UCB(Bern) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:   15.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Estimated order by the policy KL-UCB(Bern) after 10000 steps: [0 3 1 2 4 5 6 8 7] ...\n",
      "  ==> Optimal arm identification: 99.38% (relative success)...\n",
      "  ==> Manhattan   distance from optimal ordering: 85.19% (relative success)...\n",
      "  ==> Kendell Tau distance from optimal ordering: 99.82% (relative success)...\n",
      "  ==> Spearman    distance from optimal ordering: 99.98% (relative success)...\n",
      "  ==> Gestalt     distance from optimal ordering: 77.78% (relative success)...\n",
      "  ==> Mean distance from optimal ordering: 90.69% (relative success)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:   12.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "- Evaluating policy #4/4: BayesUCB ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:   27.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Estimated order by the policy BayesUCB after 10000 steps: [0 1 3 2 4 5 7 6 8] ...\n",
      "  ==> Optimal arm identification: 100.00% (relative success)...\n",
      "  ==> Manhattan   distance from optimal ordering: 90.12% (relative success)...\n",
      "  ==> Kendell Tau distance from optimal ordering: 99.92% (relative success)...\n",
      "  ==> Spearman    distance from optimal ordering: 100.00% (relative success)...\n",
      "  ==> Gestalt     distance from optimal ordering: 77.78% (relative success)...\n",
      "  ==> Mean distance from optimal ordering: 91.95% (relative success)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:   14.8s\n"
     ]
    }
   ],
   "source": [
    "for envId, env in enumerate(evaluation.envs):\n",
    "    # Evaluate just that env\n",
    "    evaluation.startOneEnv(envId, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final ranking for this environment #0 :\n",
      "- Policy 'BayesUCB'\twas ranked\t1 / 4 for this simulation (last regret = 41.29).\n",
      "- Policy 'Thompson'\twas ranked\t2 / 4 for this simulation (last regret = 44.4).\n",
      "- Policy 'KL-UCB(Bern)'\twas ranked\t3 / 4 for this simulation (last regret = 57.43).\n",
      "- Policy 'UCB'\twas ranked\t4 / 4 for this simulation (last regret = 327.56).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 327.56,   44.4 ,   57.43,   41.29]), array([3, 1, 2, 0]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This MAB problem has: \n",
      " - a [Lai & Robbins] complexity constant C(mu) = 7.52 for 1-player problem... \n",
      " - a Optimal Arm Identification factor H_OI(mu) = 48.89% ...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'showplot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-8a86d51ec5f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mevaluation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintFinalRanking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvId\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mevaluation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplotRegrets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvId\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msemilogx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplotSTD\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mevaluation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplotRegrets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvId\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msemilogx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplotSTD\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lilian/ownCloud/cloud.openmailbox.org/Th√®se_2016-17/src/AlgoBandits.git/Environment/Evaluator.py\u001b[0m in \u001b[0;36mplotRegrets\u001b[0;34m(self, envId, savefig, meanRegret, plotSTD, semilogx, normalizedRegret, drawUpperBound)\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Saving to\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msavefig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox_inches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBBOX_INCHES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'showplot'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'showplot'"
     ]
    }
   ],
   "source": [
    "for envId, env in enumerate(evaluation.envs):\n",
    "    evaluation.printFinalRanking(envId)\n",
    "    \n",
    "    evaluation.plotRegrets(envId, semilogx=False, plotSTD=False)\n",
    "    evaluation.plotRegrets(envId, semilogx=True, plotSTD=False)\n",
    "    \n",
    "    evaluation.plotRegrets(envId, semilogx=semilogx, meanRegret=True, plotSTD=False)\n",
    "    \n",
    "    evaluation.plotBestArmPulls(envId)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  },
  "notify_time": "10",
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "150px",
    "width": "251px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "widgets": {
   "state": {
    "2dd56681ef5445e9bc368fbb699e7e93": {
     "views": [
      {
       "cell_index": 20
      }
     ]
    },
    "50af079deda44cc9bb4c20b0ecce512d": {
     "views": [
      {
       "cell_index": 20
      }
     ]
    },
    "959c1a37c65742e3bd47005b20524d60": {
     "views": [
      {
       "cell_index": 20
      }
     ]
    },
    "97dc8e24b1df4646b0f554a0ce129eff": {
     "views": [
      {
       "cell_index": 20
      }
     ]
    },
    "9e3bc8460b3142e991152c92c52e7ea4": {
     "views": [
      {
       "cell_index": 20
      }
     ]
    },
    "ad8eabd926464a84a75364817d70e61d": {
     "views": [
      {
       "cell_index": 20
      }
     ]
    },
    "bef19880986c44a98d56b18ec874edd1": {
     "views": [
      {
       "cell_index": 20
      }
     ]
    },
    "dfc61d267fc64123b6286edd340f9177": {
     "views": [
      {
       "cell_index": 20
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
