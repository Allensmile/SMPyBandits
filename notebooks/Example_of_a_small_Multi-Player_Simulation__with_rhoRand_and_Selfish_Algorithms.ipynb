{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#An-example-of-a-small-Multi-Player-simulation,-with-rhoRand-and-Selfish,-for-different-algorithms\" data-toc-modified-id=\"An-example-of-a-small-Multi-Player-simulation,-with-rhoRand-and-Selfish,-for-different-algorithms-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>An example of a small Multi-Player simulation, with rhoRand and Selfish, for different algorithms</a></div><div class=\"lev2 toc-item\"><a href=\"#Creating-the-problem\" data-toc-modified-id=\"Creating-the-problem-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Creating the problem</a></div><div class=\"lev3 toc-item\"><a href=\"#Parameters-for-the-simulation\" data-toc-modified-id=\"Parameters-for-the-simulation-111\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Parameters for the simulation</a></div><div class=\"lev3 toc-item\"><a href=\"#Three-MAB-problems-with-Bernoulli-arms\" data-toc-modified-id=\"Three-MAB-problems-with-Bernoulli-arms-112\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Three MAB problems with Bernoulli arms</a></div><div class=\"lev3 toc-item\"><a href=\"#Some-RL-algorithms\" data-toc-modified-id=\"Some-RL-algorithms-113\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>Some RL algorithms</a></div><div class=\"lev2 toc-item\"><a href=\"#Creating-the-EvaluatorMultiPlayers-objects\" data-toc-modified-id=\"Creating-the-EvaluatorMultiPlayers-objects-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Creating the <code>EvaluatorMultiPlayers</code> objects</a></div><div class=\"lev2 toc-item\"><a href=\"#Solving-the-problem\" data-toc-modified-id=\"Solving-the-problem-13\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Solving the problem</a></div><div class=\"lev2 toc-item\"><a href=\"#Plotting-the-results\" data-toc-modified-id=\"Plotting-the-results-14\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Plotting the results</a></div><div class=\"lev3 toc-item\"><a href=\"#First-problem\" data-toc-modified-id=\"First-problem-141\"><span class=\"toc-item-num\">1.4.1&nbsp;&nbsp;</span>First problem</a></div><div class=\"lev3 toc-item\"><a href=\"#Second-problem\" data-toc-modified-id=\"Second-problem-142\"><span class=\"toc-item-num\">1.4.2&nbsp;&nbsp;</span>Second problem</a></div><div class=\"lev3 toc-item\"><a href=\"#Comparing-their-performances\" data-toc-modified-id=\"Comparing-their-performances-143\"><span class=\"toc-item-num\">1.4.3&nbsp;&nbsp;</span>Comparing their performances</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "# An example of a small Multi-Player simulation, with rhoRand and Selfish, for different algorithms\n",
    "\n",
    "First, be sure to be in the main folder, and import `EvaluatorMultiPlayers` from `Environment` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sys import path\n",
    "path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Setting dpi of all figures to 110 ...\n",
      " - Setting 'figsize' of all figures to (19.8, 10.8) ...\n",
      "Info: Using the Jupyter notebook version of the tqdm() decorator, tqdm_notebook() ...\n"
     ]
    }
   ],
   "source": [
    "# Local imports\n",
    "from Environment import EvaluatorMultiPlayers, tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We also need arms, for instance `Bernoulli`-distributed arm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: numba.jit seems to be available.\n"
     ]
    }
   ],
   "source": [
    "# Import arms\n",
    "from Arms import makeMeans, Bernoulli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "And finally we need some single-player and multi-player Reinforcement Learning algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: numba.jit seems to be available.\n"
     ]
    }
   ],
   "source": [
    "# Import algorithms\n",
    "from Policies import *\n",
    "from PoliciesMultiPlayers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Just improving the ?? in Jupyter. Thanks to https://nbviewer.jupyter.org/gist/minrk/7715212\n",
    "from __future__ import print_function\n",
    "from IPython.core import page\n",
    "def myprint(s):\n",
    "    try:\n",
    "        print(s['text/plain'])\n",
    "    except (KeyError, TypeError):\n",
    "        print(s)\n",
    "page.page = myprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For instance, this imported the `Thompson` algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m \u001b[0mThompson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbArms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposterior\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;32mclass\u001b[0m \u001b[0;34m'Policies.Beta.Beta'\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamplitude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "The Thompson (Bayesian) index policy. By default, it uses a Beta posterior.\n",
      "Reference: [Thompson - Biometrika, 1933].\n",
      "\u001b[0;31mInit docstring:\u001b[0m\n",
      "New generic index policy.\n",
      "\n",
      "- nbArms: the number of arms,\n",
      "- lower, amplitude: lower value and known amplitude of the rewards.\n",
      "\u001b[0;31mFile:\u001b[0m           ~/ownCloud/cloud.openmailbox.org/Thèse_2016-17/src/AlgoBandits.git/Policies/Thompson.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Thompson?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As well as the `rhoRand` and `Selfish` multi-player policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m \u001b[0mrhoRand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbPlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayerAlgo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbArms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamplitude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "rhoRand: implementation of the multi-player policy from [Distributed Algorithms for Learning..., Anandkumar et al., 2010](http://ieeexplore.ieee.org/document/5462144/).\n",
      "    \n",
      "\u001b[0;31mInit docstring:\u001b[0m\n",
      "- nbPlayers: number of players to create (in self._players).\n",
      "- playerAlgo: class to use for every players.\n",
      "- nbArms: number of arms, given as first argument to playerAlgo.\n",
      "- `*args`, `**kwargs`: arguments, named arguments, given to playerAlgo.\n",
      "\n",
      "Examples:\n",
      "\n",
      ">>> s = rhoRand(nbPlayers, Thompson, nbArms)\n",
      "\n",
      "- To get a list of usable players, use s.children.\n",
      "- Warning: s._players is for internal use ONLY!\n",
      "\u001b[0;31mFile:\u001b[0m           ~/ownCloud/cloud.openmailbox.org/Thèse_2016-17/src/AlgoBandits.git/PoliciesMultiPlayers/rhoRand.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rhoRand?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m \u001b[0mSelfish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbPlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayerAlgo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbArms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Selfish: a multi-player policy where every player is selfish, playing on their side.\n",
      "\n",
      "- without nowing how many players there is, and\n",
      "- not even knowing that they should try to avoid collisions. When a collision happens, the algorithm simply receives a 0 reward for the chosen arm (can be changed with penalty= argument).\n",
      "\u001b[0;31mInit docstring:\u001b[0m\n",
      "- nbPlayers: number of players to create (in self._players).\n",
      "- playerAlgo: class to use for every players.\n",
      "- nbArms: number of arms, given as first argument to playerAlgo.\n",
      "- `*args`, `**kwargs`: arguments, named arguments, given to playerAlgo.\n",
      "\n",
      "Examples:\n",
      "\n",
      ">>> s = Selfish(10, TakeFixedArm, 14)\n",
      ">>> s = Selfish(NB_PLAYERS, Softmax, nbArms, temperature=TEMPERATURE)\n",
      "\n",
      "- To get a list of usable players, use s.children.\n",
      "- Warning: s._players is for internal use ONLY!\n",
      "\u001b[0;31mFile:\u001b[0m           ~/ownCloud/cloud.openmailbox.org/Thèse_2016-17/src/AlgoBandits.git/PoliciesMultiPlayers/Selfish.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Selfish?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We also need a collision model. The usual ones are defined in the `CollisionModels` package, and the only one we need is the classical one, where two or more colliding users don't receive any rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0monlyUniqUserGetsReward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchoices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpulls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollisions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Simple collision model where only the players alone on one arm sample it and receive the reward.\n",
      "\n",
      "- This is the default collision model, cf. https://arxiv.org/abs/0910.2065v3 collision model 1.\n",
      "- The numpy array 'choices' is increased according to the number of users who collided (it is NOT binary).\n",
      "\u001b[0;31mFile:\u001b[0m      ~/ownCloud/cloud.openmailbox.org/Thèse_2016-17/src/AlgoBandits.git/Environment/CollisionModels.py\n",
      "\u001b[0;31mType:\u001b[0m      function\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Collision Models\n",
    "from Environment.CollisionModels import onlyUniqUserGetsReward\n",
    "onlyUniqUserGetsReward?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "## Creating the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Parameters for the simulation\n",
    "- $T = 10000$ is the time horizon,\n",
    "- $N = 10$ is the number of repetitions (should be larger to have consistent results),\n",
    "- $M = 2$ is the number of players,\n",
    "- `N_JOBS = 4` is the number of cores used to parallelize the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "HORIZON = 10000\n",
    "REPETITIONS = 10\n",
    "NB_PLAYERS = 2\n",
    "N_JOBS = 4\n",
    "collisionModel = onlyUniqUserGetsReward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Three MAB problems with Bernoulli arms\n",
    "We consider in this example $3$ problems, with `Bernoulli` arms, of different means.\n",
    "\n",
    "1. The first problem is very easy, with two good arms and three arms, with a fixed gap $\\Delta = \\max_{\\mu_i \\neq \\mu_j}(\\mu_{i} - \\mu{j}) = 0.1$.\n",
    "2. The second problem is as easier, with a larger gap.\n",
    "3. Third problem is harder, with a smaller gap, and a very large difference between the two optimal arms and the suboptimal arms.\n",
    "\n",
    "> Note: right now, the multi-environments evaluator does not work well for MP policies, if there is a number different of arms in the scenarios. So I use the same number of arms in all the problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ENVIRONMENTS = [  # 1)  Bernoulli arms\n",
    "        {   # Scenario 1 from [Komiyama, Honda, Nakagawa, 2016, arXiv 1506.00779]\n",
    "            \"arm_type\": Bernoulli,\n",
    "            \"params\": [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "        },\n",
    "        {   # Classical scenario\n",
    "             \"arm_type\": Bernoulli,\n",
    "             \"params\": [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "        },\n",
    "        {   # Harder scenario\n",
    "             \"arm_type\": Bernoulli,\n",
    "             \"params\": [0.005, 0.01, 0.015, 0.84, 0.85]\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Some RL algorithms\n",
    "We will use Thompson Sampling, $\\mathrm{UCB}_1$ and $\\mathrm{kl}-\\mathrm{UCB}$, using two different decentralized policy:\n",
    "\n",
    "1. `rhoRand`: each player starts with a rank $1$, and on collision it selects a new rank $r \\sim U([1,M])$. Instead of aiming at the best arm, each player aims at the $r$-th best arm (as given by its UCB index, or posterior sampling if Thompson sampling).\n",
    "\n",
    "2. `Selfish`: each player runs a classical RL algorithm, on the joint reward $\\tilde{r}$:\n",
    "   $$ \\tilde{r}_j(t) = r_j(t) \\times (1 - \\eta_j(t)) $$\n",
    "   where $r_j(t)$ is the reward from the sensing of the $j$-th arm at time $t \\geq 1$ and $\\eta_j(t)$ is a boolean indicator saying that there were a collision on that arm at time $t$.\n",
    "   In other words, if a player chose arm $j$ and was alone on it, it receives $r_j(t)$, and if it was not alone, all players who chose arm $j$ receive $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[[rhoRand(UCB($\\alpha=1$)), rhoRand(UCB($\\alpha=1$))],\n",
       " [Selfish(UCB($\\alpha=1$)), Selfish(UCB($\\alpha=1$))],\n",
       " [rhoRand(Thompson), rhoRand(Thompson)],\n",
       " [Selfish(Thompson), Selfish(Thompson)],\n",
       " [rhoRand(KL-UCB(Bern)), rhoRand(KL-UCB(Bern))],\n",
       " [Selfish(KL-UCB(Bern)), Selfish(KL-UCB(Bern))]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbArms = len(ENVIRONMENTS[0]['params'])\n",
    "nbArms\n",
    "\n",
    "SUCCESSIVE_PLAYERS = [\n",
    "    # UCB alpha=1\n",
    "    rhoRand(NB_PLAYERS, UCBalpha, nbArms, alpha=1).children,\n",
    "    Selfish(NB_PLAYERS, UCBalpha, nbArms, alpha=1).children,\n",
    "    # Thompson Sampling\n",
    "    rhoRand(NB_PLAYERS, Thompson, nbArms).children,\n",
    "    Selfish(NB_PLAYERS, Thompson, nbArms).children,\n",
    "    # Thompson Sampling\n",
    "    rhoRand(NB_PLAYERS, klUCB, nbArms).children,\n",
    "    Selfish(NB_PLAYERS, klUCB, nbArms).children,\n",
    "]\n",
    "SUCCESSIVE_PLAYERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mother class in this case does not do any centralized learning, as `Selfish` and `rhoRand` are fully decentralized scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PoliciesMultiPlayers.rhoRand.rhoRand at 0x7f5059139a58>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "AttributeError",
     "evalue": "'oneRhoRand' object has no attribute 'nbArms'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-a98a76d8a51f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mOnePlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSUCCESSIVE_PLAYERS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mOnePlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnbArms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'oneRhoRand' object has no attribute 'nbArms'"
     ]
    }
   ],
   "source": [
    "OneMother = SUCCESSIVE_PLAYERS[0][0].mother\n",
    "OneMother\n",
    "OneMother.nbArms\n",
    "\n",
    "OnePlayer = SUCCESSIVE_PLAYERS[0][0]\n",
    "OnePlayer.nbArms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Complete configuration for the problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "configuration = {\n",
    "    # --- Duration of the experiment\n",
    "    \"horizon\": HORIZON,\n",
    "    # --- Number of repetition of the experiment (to have an average)\n",
    "    \"repetitions\": REPETITIONS,\n",
    "    # --- Parameters for the use of joblib.Parallel\n",
    "    \"n_jobs\": N_JOBS,    # = nb of CPU cores\n",
    "    \"verbosity\": 6,      # Max joblib verbosity\n",
    "    # --- Collision model\n",
    "    \"collisionModel\": onlyUniqUserGetsReward,\n",
    "    # --- Arms\n",
    "    \"environment\": ENVIRONMENTS,\n",
    "    # --- Algorithms\n",
    "    \"successive_players\": SUCCESSIVE_PLAYERS,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "## Creating the `EvaluatorMultiPlayers` objects\n",
    "We will need to create several objects, as the simulation first runs one policy against each environment, and then aggregate them to compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N_players = len(configuration[\"successive_players\"])\n",
    "\n",
    "# List to keep all the EvaluatorMultiPlayers objects\n",
    "evs = [None] * N_players\n",
    "evaluators = [[None] * N_players] * len(configuration[\"environment\"])\n",
    "\n",
    "for playersId, players in tqdm(enumerate(configuration[\"successive_players\"]), desc=\"Creating\"):\n",
    "    print(\"\\n\\nConsidering the list of players :\\n\", players)\n",
    "    conf = configuration.copy()\n",
    "    conf['players'] = players\n",
    "    evs[playersId] = EvaluatorMultiPlayers(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##  Solving the problem\n",
    "Now we can simulate the $2$ environments, for the successive policies. That part can take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for playersId, evaluation in tqdm(enumerate(evs), desc=\"Policies\"):\n",
    "    for envId, env in tqdm(enumerate(evaluation.envs), desc=\"Problems\"):\n",
    "        # Evaluate just that env\n",
    "        evaluation.startOneEnv(envId, env)\n",
    "        # Storing it after simulation is done\n",
    "        evaluators[envId][playersId] = evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Plotting the results\n",
    "And finally, visualize them, with the plotting method of a `EvaluatorMultiPlayers` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plotAll(evaluation, envId):\n",
    "    evaluation.printFinalRanking(envId)\n",
    "    # Rewards\n",
    "    evaluation.plotRewards(envId)\n",
    "    # Fairness\n",
    "    evaluation.plotFairness(envId, fairness=\"STD\")\n",
    "    # Centralized regret\n",
    "    evaluation.plotRegretCentralized(envId, subTerms=True)\n",
    "    #evaluation.plotRegretCentralized(envId, semilogx=True, subTerms=True)\n",
    "    # Number of switches\n",
    "    #evaluation.plotNbSwitchs(envId, cumulated=False)\n",
    "    evaluation.plotNbSwitchs(envId, cumulated=True)\n",
    "    # Frequency of selection of the best arms\n",
    "    evaluation.plotBestArmPulls(envId)\n",
    "    # Number of collisions\n",
    "    evaluation.plotNbCollisions(envId, cumulated=False)\n",
    "    evaluation.plotNbCollisions(envId, cumulated=True)\n",
    "    # Frequency of collision in each arm\n",
    "    evaluation.plotFrequencyCollisions(envId, piechart=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### First problem\n",
    "$\\mu = [0.3, 0.4, 0.5, 0.6, 0.7]$ was an easy Bernoulli problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for playersId in tqdm(range(len(evs)), desc=\"Policies\"):\n",
    "    evaluation = evaluators[0][playersId]\n",
    "    plotAll(evaluation, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Second problem\n",
    "$\\mu = [0.03, \\dots, 0.03, 0.05, \\dots, 0.05, 0.1, 0.12, 0.15]$ is an harder Bernoulli problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for playersId in tqdm(range(len(evs)), desc=\"Policies\"):\n",
    "    evaluation = evaluators[1][playersId]\n",
    "    plotAll(evaluation, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "### Comparing their performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def plotCombined(e0, eothers, envId):\n",
    "    # Centralized regret\n",
    "    e0.plotRegretCentralized(envId, evaluators=eothers)\n",
    "    # Fairness\n",
    "    e0.plotFairness(envId, fairness=\"STD\", evaluators=eothers)\n",
    "    # Number of switches\n",
    "    e0.plotNbSwitchsCentralized(envId, cumulated=True, evaluators=eothers)\n",
    "    # Number of collisions - not for Centralized* policies\n",
    "    e0.plotNbCollisions(envId, cumulated=True, evaluators=eothers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    " N = len(configuration[\"environment\"])\n",
    "for envId, env in enumerate(configuration[\"environment\"]):\n",
    "    e0, eothers = evaluators[envId][0], evaluators[envId][1:]\n",
    "    plotCombined(e0, eothers, envId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "> That's it for this demo!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  },
  "notify_time": "30",
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "150px",
    "width": "251px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "widgets": {
   "state": {
    "07bf6f872d2a4a90a571ed90f0257d50": {
     "views": [
      {
       "cell_index": 28
      }
     ]
    },
    "0bd745ec347f4b3c9a32a53f0534b94a": {
     "views": [
      {
       "cell_index": 28
      }
     ]
    },
    "0e15ca8bdd0a422e9dc257aa1788bf66": {
     "views": [
      {
       "cell_index": 28
      }
     ]
    },
    "10c04a06c50e4c5fa80773c388774478": {
     "views": [
      {
       "cell_index": 28
      }
     ]
    },
    "202dbb789be74a519e423e20b0d00d2a": {
     "views": [
      {
       "cell_index": 28
      }
     ]
    },
    "222f5621736446f3a46ad73be27f4c19": {
     "views": [
      {
       "cell_index": 28
      }
     ]
    },
    "2cdad853f2bc4c9fa3f20f790d15bedc": {
     "views": [
      {
       "cell_index": 28
      }
     ]
    },
    "31b0fb9d6aae4f5e96739ae0461ff660": {
     "views": [
      {
       "cell_index": 26
      }
     ]
    },
    "34f78916cdab4529b415c2eeb9b84cda": {
     "views": [
      {
       "cell_index": 28
      }
     ]
    },
    "36c4ac86160346ca9015dde670b2da9e": {
     "views": [
      {
       "cell_index": 28
      }
     ]
    },
    "3763681e58ef4413bbf695f059b6aa51": {
     "views": [
      {
       "cell_index": 28
      }
     ]
    },
    "3ab6d4a637c943c2af0dae6f577b0eb0": {
     "views": [
      {
       "cell_index": 28
      }
     ]
    },
    "4c246d34e0984db5aee26badaaa9ffda": {
     "views": [
      {
       "cell_index": 32
      }
     ]
    },
    "5e901982d1474a8d96e786374db171c2": {
     "views": [
      {
       "cell_index": 28
      }
     ]
    },
    "662d8bd49ffa4fdab6ec43806d2c5053": {
     "views": [
      {
       "cell_index": 28
      }
     ]
    },
    "7729c0f84e6b4220ba6af95b522372df": {
     "views": [
      {
       "cell_index": 28
      }
     ]
    },
    "7a1837f988754d3a9f07711bc25b45a7": {
     "views": [
      {
       "cell_index": 28
      }
     ]
    },
    "7ac8bf7c5fa24909a03e17696d5cdf21": {
     "views": [
      {
       "cell_index": 28
      }
     ]
    },
    "7f34bacfd3804342ae010abbef305b01": {
     "views": [
      {
       "cell_index": 28
      }
     ]
    },
    "852ab9fa8c7b49c5a2f97a0e43ff65fd": {
     "views": [
      {
       "cell_index": 28
      }
     ]
    },
    "86251cae4dd144f48a091303da9a819b": {
     "views": [
      {
       "cell_index": 28
      }
     ]
    },
    "8bee27888b114d5bbbb331105afce668": {
     "views": [
      {
       "cell_index": 28
      }
     ]
    },
    "b331240d611b411f8838082d13c84796": {
     "views": [
      {
       "cell_index": 28
      }
     ]
    },
    "b9f0122804364d8eb3d49b358c955bc1": {
     "views": [
      {
       "cell_index": 28
      }
     ]
    },
    "c3a85bc33e184c7f9b83a1cc4d965a37": {
     "views": [
      {
       "cell_index": 28
      }
     ]
    },
    "cbbbc4f22d934f28943b8342d114de28": {
     "views": [
      {
       "cell_index": 28
      }
     ]
    },
    "d2300ff8339b43abb146e011dbb6bb23": {
     "views": [
      {
       "cell_index": 28
      }
     ]
    },
    "e784b86d259540d78f20fc82c5d16c3a": {
     "views": [
      {
       "cell_index": 28
      }
     ]
    },
    "ead0837ca3454fcb9238b73bb300efe3": {
     "views": [
      {
       "cell_index": 28
      }
     ]
    },
    "eb35abd684b74e309e1ca7700ce8422b": {
     "views": [
      {
       "cell_index": 28
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
